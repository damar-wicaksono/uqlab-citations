% Encoding: UTF-8

@Article{Abdallah2016,
  author   = {Imad Abdallah and Anand Natarajan and Jon D. Sørensen},
  title    = {Influence of the control system on wind turbine loads during power production in extreme turbulence: {S}tructural reliability},
  journal  = {Renewable Energy},
  year     = {2016},
  volume   = {87},
  pages    = {464--477},
  abstract = {The wind energy industry is continuously researching better computational models of wind inflow and turbulence to predict extreme loading (the nature of randomness) and their corresponding probability of occurrence. Sophisticated load alleviation control systems are increasingly being designed and deployed to specifically reduce the adverse effects of extreme load events resulting in lighter structures. The main objective herein is to show that despite large uncertainty in the extreme turbulence models, advanced load alleviation control systems yield both a reduction in magnitude and scatter of the extreme loads which in turn translates in a change in the shape of the annual maximum load distribution function resulting in improved structural reliability. Using a probabilistic loads extrapolation approach and the first order reliability method, a large multi-megawatt wind turbine blade and tower structural reliability are assessed when the extreme turbulence model is uncertain. The structural reliability is assessed for the wind turbine when three configurations of an industrial grade load alleviation control system of increasing complexity and performance are used. The load alleviation features include a cyclic pitch, individual pitch, static thrust limiter, condition based thrust limiter and an active tower vibration damper. We show that large uncertainties in the extreme turbulence model can be mitigated and significantly reduced while maintaining an acceptable structural reliability level when advanced load alleviation control systems are used. We end by providing a rational comparison between the long term loads extrapolation method and the environmental contour method for the three control configurations.},
  date     = {2015-11-10},
  doi      = {10.1016/j.renene.2015.10.044},
  file     = {:Abdallah2016 - Influence of the control system on wind turbine loads during power production in extreme turbulence_ Structural reliability.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {84946196677},
  url      = {http://dx.doi.org/10.1016/j.renene.2015.10.044},
}

@InProceedings{Abraham2016,
  author       = {Simon Abraham and Ghader Ghorbaniasl and Chris Lacor},
  title        = {A statistical approach for building sparse polynomial chaos expansions},
  booktitle    = {Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS Congress 2016)},
  year         = {2016},
  pages        = {6307--6315},
  address      = {Crete Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  abstract     = {Over the last years, a lot of effort has been made to make existing uncertainty quantification techniques more efficient in high dimensions. An important class of methods relies on the assumption that the polynomial chaos representation of the model response is sparse. This paper contributes to the validation and assessment of an innovative basis selection technique for building sparse polynomial chaos expansions. A regression approach is used for computing the polynomial chaos coefficients. The technique is based on statistical inference theory which provides information about the true regression model from an estimated regression model based on samples. The latter information is used to build iteratively the sparse polynomial chaos expansion. Using the developed methodology, a more robust and efficient basis selection technique is obtained. For validation purpose, the methodology is applied to high dimensional analytical test cases, including the Oakley & O'Hagan function (d=15) and the Morris function (d=20). The results are compared with those obtained from two state-of-the-art techniques, namely the LARS-based algorithm and compressive sampling. As compared to previous work, more comparisons with the LARS-based method are provided, through the use of UQLab, a MATLAB-based uncertainty quantification framework developed by Sudret and Marelli [1]. It is shown that, with equal settings, the developed methodology results in a more accurate polynomial chaos expansion compared to the aforementioned technique. In addition, a new criterion for building an optimal polynomial chaos expansion is further investigated. The conclusions are in-line with previous findings, i.e. the present criterion always builds a sparser polynomial chaos expansion which is, in addition, at least as accurate as compared to the optimal polynomial chaos expansion obtained from the classical cross validation technique.},
  doi          = {10.7712/100016.2259.7412},
  eventdate    = {2016-06-05/2016-06-10},
  file         = {:Abraham2016 - A statistical approach for building sparse polynomial chaos expansions.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {84995530441},
  review       = {Used for benchmarking purpose.},
  url          = {http://dx.doi.org/10.7712/100016.2259.7412},
}

@Article{Abraham2017,
  author   = {Simon Abraham and Mehrdad Raisee and Ghader Ghorbaniasl and Francesco Contino and Chris Lacor},
  title    = {A robust and efficient stepwise regression method for building sparse polynomial chaos expansions},
  journal  = {Journal of Computational Physics},
  year     = {2017},
  volume   = {332},
  pages    = {461--474},
  abstract = {Polynomial Chaos (PC) expansions are widely used in various engineering fields for quantifying uncertainties arising from uncertain parameters. The computational cost of classical PC solution schemes is unaffordable as the number of deterministic simulations to be calculated grows dramatically with the number of stochastic dimension. This considerably restricts the practical use of PC at the industrial level. A common approach to address such problems is to make use of sparse PC expansions. This paper presents a non-intrusive regression-based method for building sparse PC expansions. The most important PC contributions are detected sequentially through an automatic search procedure. The variable selection criterion is based on efficient tools relevant to probabilistic method. Two benchmark analytical functions are used to validate the proposed algorithm. The computational efficiency of the method is then illustrated by a more realistic CFD application, consisting of the non-deterministic flow around a transonic airfoil subject to geometrical uncertainties. To assess the performance of the developed methodology, a detailed comparison is made with the well established LAR-based selection technique. The results show that the developed sparse regression technique is able to identify the most significant PC contributions describing the problem. Moreover, the most important stochastic features are captured at a reduced computational cost compared to the LAR method. The results also demonstrate the superior robustness of the method by repeating the analyses using random experimental designs.},
  date     = {2016-12-15},
  doi      = {10.1016/j.jcp.2016.12.015},
  file     = {:Abraham2017 - A robust and efficient stepwise regression method for building sparse polynomial chaos expansions.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85006944223},
  url      = {http://dx.doi.org/10.1016/j.jcp.2016.12.015},
}

@InProceedings{Acikgoz2016,
  author       = {Hulusi Acikgoz and Ravi Kumar Arya and Raj Mittra},
  title        = {Statistical analysis of {3D}-printed flat {GRIN} lenses},
  booktitle    = {2016 IEEE Antennas and Propagation Society International Symposium (APSURSI 2016)},
  year         = {2016},
  pages        = {473--474},
  address      = {Fajardo, Puerto Rico},
  organization = {IEEE},
  abstract     = {This paper presents the statistical analysis of a 3D printed flat lens by using the Polynomial Chaos Expansion (PCE) analysis technique. The flat lens is fabricated using the 3D printing technology and is based on the grading-index (GRIN) approach. It is composed of several concentric rings with graded relative permittivity, made of a single material with different air holes-host material volume ratio. We show that the hole size can have significant effect on the performance of the lens, especially on the focal distance. PCE analysis enables us also to determine the impact of each individual ring on the performance of the lens.},
  doi          = {10.1109/APS.2016.7695945},
  eventdate    = {2016-06-26/2016-07-01},
  file         = {:Acikgoz2016 - Statistical analysis of 3D-printed flat GRIN lenses.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {84997525285},
  url          = {http://dx.doi.org/10.1109/APS.2016.7695945},
}

@Article{Bdour2016,
  author   = {Tarek Bdour and Alain Reineix},
  title    = {Global sensitivity analysis and uncertainty quantification of radiated susceptibility in {PCB} using nonintrusive {P}olynomial {C}haos {E}xpansions},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  year     = {2016},
  pages    = {939--942},
  abstract = {In this paper, we introduce a workflow that uses uncertainty quantification (UQ) and sensitivity analysis (SA) in conjunction with a metamodeling technique called polynomial chaos expansion (PCE) to investigate the stochastic terminal response of a printed circuit board (PCB) due to an external plane wave excitation. The objective of PCE use is to reduce the computational burden of commonly used Monte Carlo (MC) approach for calculating statistical moments and Sobol' sensitivity indices in the presence of uncertain geometrical and electrical parameters of the investigated PCB. The statistical results computed by PCE has been shown to be very accurate compared to MC simulation results.},
  date     = {2016-03-08},
  doi      = {10.1109/TEMC.2016.2535266},
  file     = {:Bdour2016 - Global Sensitivity Analysis and Uncertainty Quantification of Radiated Susceptibility in PCB Using Nonintrusive Polynomial Chaos Expansions.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {84979468030},
  url      = {http://dx.doi.org/10.1109/TEMC.2016.2535266},
}

@InProceedings{Bilicz2016,
  author       = {Sándor Bilicz and Szabolcs Gyimóthy and József Pávó and Péter Horváth and Károly Marák},
  title        = {Uncertainty quantification of wireless power transfer systems},
  booktitle    = {2016 IEEE Wireless Power Transfer Conference (WPTC 2016)},
  year         = {2016},
  address      = {Aveiro, Portugal},
  organization = {IEEE},
  abstract     = {In this paper, the uncertainty of electric properties of wireless power transfer systems due to uncertain geometric design parameters is analyzed. An electromagnetic simulation tool, based on an integral formulation, is coupled with a stochastic method that quantitatively determines the contribution of each uncertain design variable to the output uncertainty in terms of Sobol' indices. A generalized polynomial chaos expansion - being a state-of-art surrogate modeling method - is used to reduce the computational cost involved by these stochastic simulations. The performance of the proposed method is illustrated via the analysis of a resonant wireless power transfer chain.},
  doi          = {10.1109/WPT.2016.7498861},
  eventdate    = {2016-05-05/2016-05-06},
  file         = {:Bilicz2016 - Uncertainty quantification of wireless power transfer systems.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {84979574638},
  url          = {http://dx.doi.org/10.1109/WPT.2016.7498861},
}

@InProceedings{Capellari2016,
  author       = {Giovanni Capellari and Eleni Chatzi and Stefano Mariani},
  title        = {An optimal sensor placement method for {SHM} based on {B}ayesian experimental design and {P}olynomial {C}haos {E}xpansion},
  booktitle    = {Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS Congress 2016)},
  year         = {2016},
  pages        = {6272--6282},
  address      = {Crete Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  abstract     = {We present an optimal sensor placement methodology for structural health monitoring (SHM) purposes, relying on a Bayesian experimental design approach. The unknown structural properties, e.g. the residual strength and stiffness, are inferred from data collected through a network of sensors, whose architecture, i.e., type and position may largely affect the accuracy of the monitoring system. In tackling this issue, an optimal network configuration is herein sought by maximizing the expected information gain between prior and posterior probability distributions of the parameters to be estimated. Since the objective function linked to the network topology cannot be analytically computed, a numerical approximation is provided by means of a Monte Carlo analysis, wherein each realization is obtained via finite element modeling. Since the computational burden linked to this procedure often grows infeasible, a Polynomial Chaos Expansion (PCE) approach is adopted for accelerating the computation of the forward problem. The analysis expands over joint samples covering both structural state and design variables, i.e., sensor locations. Via increase of the number of deployed sensors in the network, the optimization procedure soon turns computationally costly due to the curse of dimensionality. To this end, a stochastic optimization method is adopted for accelerating the convergence of the optimization process and thereby the damage detection capability of the SHM system. The proposed method is applied to thin flexible structures, and the resulting optimal sensor configuration is shown. The effects of the number of training samples, the polynomial degree of the approximation expansion and the optimization settings are also discussed.},
  doi          = {10.7712/100016.2257.6762},
  eventdate    = {2016-06-05/2016-06-10},
  file         = {:Capellari2016 - An optimal sensor placement method for SHM based on Bayesian experimental design and Polynomial Chaos Expansion.pdf:PDF},
  keywords     = {monitoring and remote sensing, used UQLab},
  piis         = {84995471069},
  url          = {http://dx.doi.org/10.7712/100016.2257.6762},
}

@Article{Chen2017,
  author   = {Cheng Chen and Weijie Xu and Tong Guo and Kai Chen},
  title    = {Analysis of actuator delay and its effect on uncertainty quantification for real-time hybrid simulation},
  journal  = {Earthquake Engineering and Engineering Vibration},
  year     = {2017},
  volume   = {16},
  number   = {4},
  pages    = {713--725},
  abstract = {Uncertainties in structure properties can result in different responses in hybrid simulations. Quantification of the effect of these uncertainties would enable researchers to estimate the variances of structural responses observed from experiments. This poses challenges for real-time hybrid simulation (RTHS) due to the existence of actuator delay. Polynomial chaos expansion (PCE) projects the model outputs on a basis of orthogonal stochastic polynomials to account for influences of model uncertainties. In this paper, PCE is utilized to evaluate effect of actuator delay on the maximum displacement from real-time hybrid simulation of a single degree of freedom (SDOF) structure when accounting for uncertainties in structural properties. The PCE is first applied for RTHS without delay to determine the order of PCE, the number of sample points as well as the method for coefficients calculation. The PCE is then applied to RTHS with actuator delay. The mean, variance and Sobol indices are compared and discussed to evaluate the effects of actuator delay on uncertainty quantification for RTHS. Results show that the mean and the variance of the maximum displacement increase linearly and exponentially with respect to actuator delay, respectively. Sensitivity analysis through Sobol indices also indicates the influence of the single random variable decreases while the coupling effect increases with the increase of actuator delay.},
  date     = {2017-11-07},
  doi      = {10.1007/s11803-017-0409-6},
  file     = {:Chen2017 - Analysis of actuator delay and its effect on uncertainty quantification for real-time hybrid simulation.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85033482190},
  url      = {http://dx.doi.org/10.1007/s11803-017-0409-6},
}

@Article{Cheng2018,
  author   = {Kai Cheng and Zhenzhou Lu},
  title    = {Sparse polynomial chaos expansion based on {D-MORPH} regression},
  journal  = {Applied Mathematics and Computation},
  year     = {2018},
  volume   = {323},
  pages    = {17--30},
  abstract = {Polynomial chaos expansion (PCE) is widely used by engineers and modelers in various engineering fields for uncertainty analysis. The computational cost of full PCE is unaffordable for the "curse of dimensionality" of the expansion coefficients. In this paper, a new method for developing sparse PCE is proposed based on the diffeomorphic modulation under observable response preserving homotopy (D-MORPH) algorithm. D-MORPH is a regression technique, it can construct the full PCE models with model evaluations much less than the unknown coefficients. This technique determines the unknown coefficients by minimizing the least-squared error and an objective function. For the purpose of developing sparse PCE, an iterative reweighted algorithm is proposed to construct the objective function. As a result, the objective in D-MORPH regression is converted to minimize the ℓ1 norm of PCE coefficients, and the sparse PCE is established after the proposed algorithm converges to the optimal value. To validate the performance of the developed methodology, several benchmark examples are investigated. The accuracy and efficiency are compared to the well-established least angle regression (LAR) sparse PCE, and results show that the developed method is superior to the LAR-based sparse PCE in terms of efficiency and accuracy.},
  date     = {2017-12-09},
  doi      = {10.1016/j.amc.2017.11.044},
  file     = {:Cheng2018 - Sparse polynomial chaos expansion based on D-MORPH regression.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85037524488},
  url      = {http://dx.doi.org/10.1016/j.amc.2017.11.044},
}

@Article{Chiaramello2017,
  author   = {Emma Chiaramello and Serena Fiocchi and Paolo Ravazzani and Marta Parazzini},
  title    = {Stochastic dosimetry for the assessment of children exposure to uniform 50 {Hz} magnetic field with uncertain orientation},
  journal  = {BioMed Research International},
  year     = {2017},
  abstract = {This study focused on the evaluation of the exposure of children aging from five to fourteen years to 50 Hz homogenous magnetic field uncertain orientation using stochastic dosimetry. Surrogate models allowed assessing how the variation of the orientation of the magnetic field influenced the induced electric field in each tissue of the central nervous system (CNS) and in the peripheral nervous system (PNS) of children. Results showed that the electric field induced in CNS and PNS tissues of children were within the ICNIRP basic restrictions for general public and that no significant difference was found in the level of exposure of children of different ages when considering 10000 possible orientations of the magnetic field. A "mean stochastic model," useful to estimate the level of exposure in each tissue of a representative child in the range of age from five to fourteen years, was developed. In conclusion, this study was useful to deepen knowledge about the ELF-MF exposure, including the evaluation of variable and uncertain conditions, thus representing a step towards a more realistic characterization of the exposure to EMF.},
  date     = {2017-10-31},
  doi      = {10.1155/2017/4672124},
  file     = {:Chiaramello2017 - Stochastic Dosimetry for the Assessment of Children Exposure to Uniform 50 Hz Magnetic Field with Uncertain Orientation.pdf:PDF},
  keywords = {biomedical science, used UQLab},
  piis     = {85042085356},
  url      = {http://dx.doi.org/10.1155/2017/4672124},
}

@Article{Colone2018,
  author   = {Lorenzo Colone and Anand Natarajan and Nikolay Dimitrov},
  title    = {Impact of turbulence induced loads and wave kinematic models on fatigue reliability estimates of offshore wind turbine monopiles},
  journal  = {Ocean Engineering},
  year     = {2018},
  volume   = {155},
  pages    = {295--309},
  abstract = {The cost of offshore wind turbine substructures has a significant impact on competitiveness of the wind energy market and is affected by conservative safety margins adopted in the design phase. This implies that an accurate design load prediction, especially of those resulting in fatigue damage accumulation, may help achieve more cost-effective solutions. In this article, the impact of turbulence and wave loads on fatigue reliability of pile foundations is investigated for a 5-MW offshore wind turbine. Loads obtained by varying turbulence percentiles are compared with those obtained from the full joint probability distribution of wind speed and turbulence through Monte Carlo (MC) simulations, and from the equivalent turbulence level currently adopted by IEC standards. The analyses demonstrate that a lower equivalent turbulence percentile leads to a more realistic and less conservative estimation of fatigue loads. Subsequently, the research focuses on studying the effects of uncertain marine environments on the fatigue load distribution, showing that the latter is insensitive to the random variability of the hydrodynamic coefficients. With respect to the wave kinematic model, a comparison between nonlinear and linear waves clearly suggests that hydrodynamic forces depend significantly on the kinematic model adopted and the operational conditions of the turbine. Furthermore, a term is derived to correct the error introduced by Wheeler stretching at finite water depths. The respective model uncertainties that originate from the nonlinear irregular wave model and Wheeler correction are quantified and employed in a reliability analysis. In a case study, the results are finally compared in terms of estimated probability of failure, with the aim to quantify the influence of environmental models on monopile reliability.},
  date     = {2018-03-23},
  doi      = {10.1016/j.oceaneng.2018.02.045},
  file     = {:Colone2018 - Impact of turbulence induced loads and wave kinematic models on fatigue reliability estimates of offshore wind turbine monopiles.pdf:PDF},
  keywords = {ocean engineering, used UQLab},
  piis     = {85042905073},
  url      = {http://dx.doi.org/10.1016/j.oceaneng.2018.02.045},
}

@InProceedings{Du2017,
  author       = {Jinxin Du and Christophe Roblin},
  title        = {Statistical modeling of the reflection coefficient of deformable antennas},
  booktitle    = {2017 11th European Conference on Antennas and Propagation (EUCAP 2017)},
  year         = {2017},
  pages        = {1928--1932},
  address      = {Paris, France},
  organization = {IEEE},
  abstract     = {A modeling methodology is proposed for characterizing the reflection coefficient S11(f) of narrow band antennas undergoing random disturbances. Firstly, identification techniques are used to get a parsimonious representation of the S11; then the Polynomial Chaos Expansion (PCE) method is used to characterize quantitatively the influence of random disturbances on the compressed S11. The derived S11 model can be used as efficient surrogate for statistical analysis of antennas' frequency behavior. We have applied the proposed methodology to two narrow band antennas - a deformable dipole and a textile patch - in order to demonstrate its performance. Models with good accuracy have been derived for both cases.},
  doi          = {10.23919/EuCAP.2017.7928373},
  eventdate    = {2017-03-19/2017-03-24},
  file         = {:Du2017 - Statistical modeling of the reflection coefficient of deformable antennas.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85020204287},
  url          = {http://dx.doi.org/10.23919/EuCAP.2017.7928373},
}

@Article{Du2017a,
  author   = {Jinxin Du and Christophe Roblin},
  title    = {Statistical modeling of disturbed antennas based on the polynomial chaos expansion},
  journal  = {IEEE Antennas and Wireless Propagation Letters},
  year     = {2017},
  volume   = {16},
  pages    = {1843--1846},
  abstract = {A new methodology of statistical modeling of the far field (FF) radiated by antennas undergoing random disturbances is presented. First, the radiated FF is transformed into a parsimonious form using the spherical modes expansion method (SMEM); then, a surrogate model relating the parsimonious field with the input random parameters is constructed using the polynomial chaos expansion method (PCEM). The combination of the SMEM and PCEM allows developing a compact and precise model with a minimized experimental design cost. The obtained model is computationally costless for generating statistical samples of disturbed antennas easily usable as surrogate models in various types of analyses. In order to demonstrate its performance, the proposed methodology is validated with a deformable canonical antenna-A dipole undergoing three independent random deformations (stretching, bending, and torsion), deriving a compact and precise surrogate model.},
  date     = {2016-09-14},
  doi      = {10.1109/LAWP.2016.2609739},
  file     = {:Du2017a - Statistical Modeling of Disturbed Antennas Based on the Polynomial Chaos Expansion.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85024483492},
  url      = {http://dx.doi.org/10.1109/LAWP.2016.2609739},
}

@Article{Dutta2018,
  author   = {Subhrajit Dutta and Siddhartha Ghosh and Mandar M. Inamdar},
  title    = {Optimisation of tensile membrane structures under uncertain wind loads using {PCE} and {K}riging based metamodels},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  volume   = {57},
  number   = {3},
  pages    = {1149--1161},
  abstract = {Tensile membrane structures (TMS) are light-weight flexible structures that are designed to span long distances with structural efficiency. The stability of a TMS is jeopardised under heavy wind forces due to its inherent flexibility and inability to carry out-of-plane moment and shear. A stable TMS under uncertain wind loads (without any tearing failure) can only be achieved by a proper choice of the initial prestress. In this work, a double-loop reliability-based design optimisation (RBDO) of TMS under uncertain wind load is proposed. Using a sequential polynomial chaos expansion (PCE) and kriging based metamodel, this RBDO reduces the cost of inner-loop reliability analysis involving an intensive finite element solver. The proposed general approach is applied to the RBDO of two benchmark TMS and its computational efficiency is demonstrated through these case studies. The method developed here is suggested for RBDO of large and complex engineering systems requiring costly numerical solution.},
  date     = {2017-09-08},
  doi      = {10.1007/s00158-017-1802-5},
  file     = {:Dutta2018 - Optimisation of tensile membrane structures under uncertain wind loads using PCE and kriging based metamodels.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85029009303},
  url      = {http://dx.doi.org/10.1007/s00158-017-1802-5},
}

@Article{Erten2016,
  author   = {Esra Erten and Juan M. Lopez-Sanchez and Onur Yuzugullu and Irena Hajnsek},
  title    = {Retrieval of agricultural crop height from space: {A} comparison of {SAR} techniques},
  journal  = {Remote Sensing of Environment},
  year     = {2016},
  volume   = {187},
  pages    = {130--144},
  abstract = {This paper deals with the retrieval of agricultural crop height from space by using multipolarization Synthetic Aperture Radar (SAR) images. Coherent and incoherent crop height estimation methods are discussed for the first time with a unique TanDEM-X dataset acquired over rice cultivation areas. Indeed, with its polarimetric and interferometric capabilities, the TanDEM-X mission enables the tracking of crop height through interferometric SAR (InSAR), polarimetric interferometric SAR (PolInSAR) and the inversion of radiative transfer-based backscattering model. The paper evaluates the three aforementioned techniques simultaneously with a data set acquired in September 2014 and 2015 over rice fields in Turkey during their reproductive stage. The assessment of the absolute height accuracy and the limitations of the approaches are provided. In-situ measurements conducted in the same cultivation periods are used for validation purposes. The PolInSAR and morphological backscattering model results showed better performance with low RMSEs (12 and 13 cm) compared to the differential InSAR result having RMSE of 18 cm. The spatial baseline, i.e. the distance between satellites, is a key parameter for coherent methods such as InSAR and PolInSAR. Its effect on the absolute height accuracy is discussed using TanDEM-X pairs separated by a baseline of 101.7m and 932m. Although the InSAR based approach is demonstrated to provide sufficient crop height accuracy, the availability of a precise vegetation-free digital elevation model and a structurally dense crop are basic requirements for achieving high accuracy. The PolInSAR approach provides reliable crop height estimation if the spatial baseline is large enough for the inversion. The impact of increasing spatial baseline on the absolute accuracy of the crop height estimation is evident for both methods. However, PolInSAR is more cost-efficient, e.g. there is no need for phase unwrapping and any external vegetation free surface elevation data. Instead, the usage of radiative transfer based backscattering models provides not only crop height but also other biophysical properties of the crops with consistent accuracy. The efficient retrieval of crop height with backscattering model is achieved by metamodelling, which makes the computational cost of backscattering inversion comparable to the ones of the coherent methods. However, effectiveness depends on not only the backscattering model, but also the integration of agronomic crop growth rules. Motivated by these results, a combination of backscattering and PolInSAR inversion models would provide a successful method of future precision farming studies.},
  date     = {2016-10-14},
  doi      = {10.1016/j.rse.2016.10.007},
  file     = {:Erten2016 - Retrieval of agricultural crop height from space_ A comparison of SAR techniques.pdf:PDF},
  keywords = {sensors engineering, used UQLab},
  piis     = {84991593765},
  url      = {http://dx.doi.org/10.1016/j.rse.2016.10.007},
}

@InProceedings{Erten2016a,
  author       = {Esra Erten and Onur Yuzugullu and Juan M. Lopez-Sanchez and Irena Hajnsek},
  title        = {{SAR} algorithms for crop height estimation: {T}he paddy-rice case study},
  booktitle    = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  year         = {2016},
  pages        = {7117--7120},
  address      = {Beijing, China},
  organization = {IEEE},
  abstract     = {This paper presents a study of the sensibility of the incoherent (electromagnetic backscattering model) and coherent (DInSAR and PolInSAR inversion) crop height estimation methods of SAR imaging. The methods were compared for paddy-rice crop height monitoring with a TanDEM-X dataset. For this, rice-cultivated agricultural fields located in Northern Turkey were selected. Intensive ground data collection during the cultivation period in 2015 was carried out. The accuracy analysis showed that the requirement of external (vegetation-free) DEM in DInSAR-based crop height estimation decreases its performance compared to the PolInSAR and backscattering inversion methods.},
  doi          = {10.1109/IGARSS.2016.7730857},
  eventdate    = {2016-07-10/2016-07-15},
  file         = {:Erten2016a - SAR algorithms for crop height estimation_ The paddy-rice case study.pdf:PDF},
  keywords     = {monitoring and remote sensing, used UQLab},
  piis         = {85007425666},
  url          = {http://dx.doi.org/10.1109/IGARSS.2016.7730857},
}

@Article{Tan2018,
  author   = {Fengjie Tan and Tom Lahmer},
  title    = {Shape optimization based design of arch-type dams under uncertainties},
  journal  = {Engineering Optimization},
  year     = {2018},
  volume   = {50},
  number   = {9},
  pages    = {1470--1482},
  abstract = {Comparing existing design methodologies for arch-type dams, model-based shape optimization can effectively reduce construction costs and leverage the properties of construction materials. To apply means of shape optimization, suitable variables need to be chosen to formulate the objective function, which is here the volume of the arch dam. A genetic algorithm is adopted as the optimization method, which allows a global search. The reliability index is considered as the main constraint. Its computation is realized by adaptive Kriging Monte Carlo simulation, which visibly increases the analysis efficiency compared with traditional Monte Carlo simulations. Constraints, such as the reliability index and further with respect to the geometry, are taken into consideration by a penalty formulation. By means of this approach, a reliability-based design can be found which ensures both the safety and serviceability of a newly designed arch-type dam.},
  date     = {2017-12-18},
  doi      = {10.1080/0305215X.2017.1409348},
  file     = {:Tan2018 - Shape optimization based design of arch-type dams under uncertainties.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85038396163},
  url      = {http://dx.doi.org/10.1080/0305215X.2017.1409348},
}

@InCollection{Gaspar2016,
  author    = {Bruno Gaspar and Carlos Guedes Soares and Ehsan Bahmyari and Mohammad Reza Khedmati},
  title     = {Application of polynomial chaos expansions in stochastic analysis of plate elements under lateral pressure},
  booktitle = {Maritime Technology and Engineering 3},
  publisher = {Taylor \& Francis Group},
  year      = {2016},
  pages     = {471--480},
  isbn      = {978-1-138-03000-8},
  abstract  = {An application of the polynomial chaos expansion technique in the stochastic analysis of plate elements under lateral pressure is presented. An adaptive non-intrusive technique based on regression is adopted to define sparse polynomial chaos expansion representations. The analysis is performed considering as response quantity of interest the maximum deflection of a steel rectangular plate element under lateral pressure representative of a ship bottom plate element. The material modulus of elasticity is considered to be a spatially varying property represented by a random field. The plate numerical model is based on the classical theory of thin plates, which is solved numerically using the element free Galerkin method. The accuracy and efficiency of the adaptive non-intrusive sparse polynomial chaos expansion technique adopted is demonstrated in the paper for different correlation lengths of the random field.},
  file      = {:Gaspar2016 - Application of polynomial chaos expansions in stochastic analysis of plate elements under lateral pressure.pdf:PDF},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016740809},
  url       = {https://www.researchgate.net/publication/304944256_Application_of_polynomial_chaos_expansions_in_stochastic_analysis_of_plate_elements_under_lateral_pressure},
}

@InCollection{Gaspar2016a,
  author    = {Bruno Gaspar and A. P. Teixeira and Carlos Guedes Soares},
  title     = {Sensitivity analysis of the {IACS-CSR} buckling strength requirements for stiffened panels},
  booktitle = {Maritime Technology and Engineering 3},
  publisher = {Taylor \& Francis Group},
  year      = {2016},
  pages     = {459--470},
  abstract  = {A sensitivity analysis of the IACS-CSR buckling strength requirements for stiffened panels under uniaxial compression is presented. The buckling strength requirements considered are defined on the basis of semi-empirical design formulations that account explicitly for three failure modes of the stiffened panel: uniaxial buckling of the plating between stiffeners, column buckling of stiffeners with attached plating and lateral-torsional buckling or tripping of stiffeners. A sample of deck stiffened panels obtained from a representative sample of double hull oil tanker designs is considered in the sensitivity analysis. Different local and global sensitivity analysis methods are used to compute sensitivity indices for the different stiffened panel designs and failure modes. The most important variables with respect to the contribution to the critical buckling stress variability are identified and the different local and global sensitivity indices compared.},
  file      = {:Gaspar2016a - Sensitivity analysis of the IACS-CSR buckling strength requirements for stiffened panels.pdf:PDF},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016821176},
  url       = {https://www.researchgate.net/publication/305872936_Sensitivity_analysis_of_the_IACS-CSR_buckling_strength_requirements_for_stiffened_panels_Proceedings_of_the_3rd_International_Conference_on_Maritime_Technology_and_Engineering_MARTECH_2016_Lisbon_Port},
}

@Article{Hamdi2017,
  author   = {Hamidreza Hamdi and Ivo Couckuyt and Mario Costa Sousa and Tom Dhaene},
  title    = {{G}aussian {P}rocesses for history-matching: application to an unconventional gas reservoir},
  journal  = {Computational Geosciences},
  year     = {2017},
  volume   = {21},
  number   = {2},
  pages    = {267--287},
  abstract = {The process of reservoir history-matching is a costly task. Many available history-matching algorithms either fail to perform such a task or they require a large number of simulation runs. To overcome such struggles, we apply the Gaussian Process (GP) modeling technique to approximate the costly objective functions and to expedite finding the global optima. A GP model is a proxy, which is employed to model the input-output relationships by assuming a multi-Gaussian distribution on the output values. An infill criterion is used in conjunction with a GP model to help sequentially add the samples with potentially lower outputs. The IC fault model is used to compare the efficiency of GP-based optimization method with other typical optimization methods for minimizing the objective function. In this paper, we present the applicability of using a GP modeling approach for reservoir history-matching problems, which is exemplified by numerical analysis of production data from a horizontal multi-stage fractured tight gas condensate well. The results for the case that is studied here show a quick convergence to the lowest objective values in less than 100 simulations for this 20-dimensional problem. This amounts to an almost 10 times faster performance compared to the Differential Evolution (DE) algorithm that is also known to be a powerful optimization technique. The sensitivities are conducted to explain the performance of the GP-based optimization technique with various correlation functions.},
  date     = {2017-01-21},
  doi      = {10.1007/s10596-016-9611-2},
  file     = {:Hamdi2017 - Gaussian Processes for history-matching_ application to an unconventional gas reservoir.pdf:PDF},
  keywords = {geoscience, used UQLab},
  piis     = {85009932344},
  url      = {http://dx.doi.org/10.1007/s10596-016-9611-2},
}

@Article{Hariri-Ardebili2018,
  author   = {Mohammad Amin Hariri-Ardebili and Farhad Pourkamali-Anaraki},
  title    = {Simplified reliability analysis of multi hazard risk in gravity dams via machine learning techniques},
  journal  = {Archives of Civil and Mechanical Engineering},
  year     = {2018},
  volume   = {18},
  number   = {2},
  pages    = {592--610},
  abstract = {Deterministic analysis does not provide a comprehensive model for concrete dam response under multi-hazard risk. Thus, the use of probabilistic approach is usually recommended which is problematic due to high computational demand. This paper presents a simplified reliability analysis framework for gravity dams subjected to flooding, earthquakes, and aging. A group of time-variant degradation models are proposed for different random variables. Response of the dam is presented by explicit limit state functions. The probability of failure is directly computed by either classical Monte Carlo simulation or the refined importance sampling technique. Next, three machine learning techniques (i.e., K-nearest neighbor, support vector machine, and naive Bayes classifier) are adopted for binary classification of the structural results. These methods are then demonstrated in terms of accuracy, applicability and computational time for prediction of the failure probability. Results are then generalized for different dam classes (based on the height-to-width ratio), various water levels, earthquake intensity, degradation rate, and cross-correlation between the random variables. Finally, a sigmoid-type function is proposed for analytical calculation of the failure probability for different classes of gravity dams. This function is then specialized for the hydrological hazard and the failure surface is presented as a direct function of the dam's height and width.},
  date     = {2017-11-09},
  doi      = {10.1016/j.acme.2017.09.003},
  file     = {:Hariri-Ardebili2018 - Simplified reliability analysis of multi hazard risk in gravity dams via machine learning techniques.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85033218669},
  url      = {http://dx.doi.org/10.1016/j.acme.2017.09.003},
}

@InProceedings{Hashemian2017,
  author       = {Raoufehsadat Hashemian and Niklas Carlsson and Diwakar Krishnamurthy and Martin Arlitt},
  title        = {Iris: {I}te{R}ative and {I}ntelligent {E}xperiment {S}election},
  booktitle    = {Proceedings of the 2017 ACM/SPEC International Conference on Performance Engineering (ICPE 2017)},
  year         = {2017},
  pages        = {143--154},
  address      = {L'Aquila, Italy},
  organization = {Association for Computing Machinery (ACM)},
  abstract     = {Benchmarking is a widely-used technique to quantify the performance of software systems. However, the design and implementation of a benchmarking study can face several challenges. In particular, the time required to perform a benchmarking study can quickly spiral out of control, owing to the number of distinct variables to systematically examine. In this paper, we propose IRIS, an IteRative and Intelligent Experiment Selection methodology, to maximize the information gain while minimizing the duration of the benchmarking process. IRIS selects the region to place the next experiment point based on the variability of both dependent, i.e., response, and independent variables in that region. It aims to identify a performance function that minimizes the response variable prediction error for a constant and limited experimentation budget. We evaluate IRIS for a wide selection of experimental, simulated and synthetic systems with one, two and three independent variables. Considering a limited experimentation budget, the results show IRIS is able to reduce the performance function prediction error up to 4:3 times compared to equal distance experiment point selection. Moreover, we show that the error reduction can further improve through system-specific parameter tuning. Analysis of the error distributions obtained with IRIS reveals that the technique is particularly effective in regions where the response variable is sensitive to changes in the independent variables.},
  doi          = {10.1145/3030207.3030225},
  eventdate    = {2017-04-22/2017-04-26},
  file         = {:Hashemian2017 - Iris_ IteRative and Intelligent Experiment Selection.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85019034357},
  review       = {Used Kriging module of UQLab},
  url          = {http://dx.doi.org/10.1145/3030207.3030225},
}

@Article{Kaintura2017,
  author   = {A. Kaintura and D. Spina and I. Couckuyt and L. Knockaert and W. Bogaerts and T. Dhaene},
  title    = {A {K}riging and {S}tochastic {C}ollocation ensemble for uncertainty quantification in engineering applications},
  journal  = {Engineering with Computers},
  year     = {2017},
  volume   = {33},
  number   = {4},
  pages    = {935--949},
  abstract = {We propose a new surrogate modeling approach by combining two non-intrusive techniques: Kriging and Stochastic Collocation. The proposed method relies on building a sufficiently accurate Stochastic Collocation model which acts as a basis to construct a Kriging model on the residuals, to combine the accuracy and efficiency of Stochastic Collocation methods in describing stochastic quantities with the flexibility and modeling power of Kriging-based approaches. We investigate and compare performance of the proposed approach with state-of-art techniques over benchmark problems and practical engineering examples on various experimental designs.},
  date     = {2017-03-17},
  doi      = {10.1007/s00366-017-0507-0},
  file     = {:Kaintura2017 - A Kriging and Stochastic Collocation ensemble for uncertainty quantification in engineering applications.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85015702883},
  url      = {http://dx.doi.org/10.1007/s00366-017-0507-0},
}

@InProceedings{Larbi2017,
  author       = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Crosstalk analysis of printed circuits with many uncertain parameters using sparse polynomial chaos metamodels},
  booktitle    = {2017 International Symposium on Electromagnetic Compatibility (EMC EUROPE 2017)},
  year         = {2017},
  address      = {Angers, France},
  organization = {IEEE},
  abstract     = {This paper presents a metamodel based on the sparse polynomial chaos approach, well adapted to high-dimensional uncertainty quantification problems, applied for the analysis of crosstalk in printed circuit board microstrip traces. It enables to estimate, with a low computational cost compared to Monte Carlo (MC) simulation, statistical quantities and provides a sensitivity analysis of the crosstalk effects considering numerous uncertain variables. The approach is validated against MC simulation and shows a good efficiency and accuracy.},
  doi          = {10.1109/EMCEurope.2017.8094623},
  eventdate    = {2017-09-04/2017-09-07},
  file         = {:Larbi2017 - Crosstalk analysis of printed circuits with many uncertain parameters using sparse polynomial chaos metamodels.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85040624369},
  url          = {http://dx.doi.org/10.1109/EMCEurope.2017.8094623},
}

@InProceedings{Larbi2017a,
  author       = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Analysis of a printed circuit board with many uncertain variables by sparse polynomial chaos},
  booktitle    = {2017 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization for RF, Microwave, and Terahertz Applications (NEMO 2017)},
  year         = {2017},
  pages        = {323--325},
  address      = {Seville, Spain},
  organization = {IEEE},
  abstract     = {This communication deals with the uncertainty quantification in high dimensional problems. It introduces a metamodel based on the sparse polynomial chaos for the analysis of a printed circuit board, depending on many uncertain variables. This metamodel allows to estimate statistical quantities of an output with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained have been validated by comparison with MC simulation.},
  doi          = {10.1109/NEMO.2017.7964274},
  eventdate    = {2017-05-17/2017-05-19},
  file         = {:Larbi2017a - Analysis of a printed circuit board with many uncertain variables by sparse polynomial chaos.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85028511104},
  url          = {https://dx.doi.org/10.1109/NEMO.2017.7964274},
}

@Article{Larbi2018,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {Variability impact of many design parameters: {T}he case of a realistic electronic link},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  year     = {2018},
  volume   = {60},
  number   = {1},
  pages    = {34--41},
  abstract = {In this paper, we adopt the so-called sparse polynomial chaos metamodel for the uncertainty quantification in the framework of high-dimensional problems. This metamodel is used to model a realistic electronic bus structure with a large number of uncertain input parameters such as those related to microstrip line geometries. It aims at estimating quantities of interest, such as statistical moments, probability density functions, and provides sensitivity analysis of a response. It drastically reduces the model computational cost with regard to brute force Monte Carlo (MC) simulation. The method presents a good performance and is validated in comparison with MC simulation.},
  date     = {2017-07-25},
  doi      = {10.1109/TEMC.2017.2727961},
  file     = {:Larbi2018 - Variability Impact of Many Design Parameters_ The Case of a Realistic Electronic Link.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85028942396},
  url      = {http://dx.doi.org/10.1109/TEMC.2017.2727961},
}

@Article{Larbi2018a,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {Identification of main factors of uncertainty in a microstrip line network},
  journal  = {Progress in Electromagnetics Research},
  year     = {2018},
  volume   = {162},
  pages    = {61--72},
  abstract = {This paper deals with uncertainty propagation applied to the analysis of crosstalk in printed circuit board microstrip traces. Complex interconnection networks generally are affected by many uncertain parameters and their point-to-point transfer functions are computationally expensive, thus making Monte-Carlo analyses rather inefficient. To overcome this situation, a metamodel is highly desirable. This paper presents a sparse and accelerated polynomial chaos approach, which proves to be well adapted for high-dimensional uncertainty quantification and well suited for the sensitivity analysis of crosstalk effects. We highlight the significant advantage of the advocated approach for the design of microstrip line networks of complex topology. In fact, we demonstrate how a small number of system simulations can help to quantify the statistics of the output variability and identify a reduced set of high-impact parameters.},
  date     = {2018-06-13},
  doi      = {10.2528/PIER18040607},
  file     = {:Larbi2018a - Identification of main factors of uncertainty in a microstrip line network.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85049190395},
  url      = {http://dx.doi.org/10.2528/PIER18040607},
}

@InProceedings{Li2017,
  author       = {Meng Li and Yinghong Wen and Jinbao Zhang and Dan Zhang},
  title        = {An {EMC} safety assessment model to analyze complex system in high speed railways},
  booktitle    = {Proceedings of the 2017 19th International Conference on Electromagnetics in Advanced Applications (ICEAA 2017)},
  year         = {2017},
  pages        = {626--629},
  address      = {Verona, Italy},
  organization = {IEEE},
  abstract     = {This paper presents a safety assessment model with regard to electromagnetic compatibility (EMC) fault suitable for estimating the EMC failure probability (EMC FP) of the complex system facing uncertain electromagnetic environment. First, the model is successfully applied to decompose the complex system into different independent subsystem and equipment. Then the practical EMI scenario is modeled as different typical EMC problem, such as the Plane Wave Coupling and Crosstalk respectively. After that the fault tree analysis (FTA) is proposed to use in combination with electromagnetic topology (EMT) calculating the EMC FP of the system. A focus is put on the practical case of the Desktop Management Interface (DMI) system. In this context, a system-level EMC safety assessment model has been applied to solve EMC FP by uncertainty analysis in high speed railways with the complex electromagnetic environment.},
  doi          = {10.1109/ICEAA.2017.8065324},
  eventdate    = {2017-09-11/2017-09-15},
  file         = {:Li2017 - An EMC safety assessment model to analyze complex system in high speed railways.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85035133738},
  url          = {http://dx.doi.org/10.1109/ICEAA.2017.8065324},
}

@Article{Mentani2016,
  author   = {Alessio Mentani and Laura Govoni and Guido Gottardi and Stéphane Lambert and Franck Bourrier and David Toe},
  title    = {A new approach to evaluate the effectiveness of rockfall barriers},
  journal  = {Procedia Engineering},
  year     = {2016},
  volume   = {158},
  pages    = {398--403},
  abstract = {The paper addresses the response of a semi-rigid rockfall protection barrier using numerical models. The study show a large dependence of the barrier response to the impact conditions. The block size and impact position, rather than the velocity direction and magnitude induce different modes of failure of the fence, which in turn result in different values of failure energy. As a result, the barrier capacity cannot be established in a deterministic way. The effectiveness of structures as such can be more successfully evaluated through a reliability probabilistic approach. Results can be used to create a meta-model of the barrier response which can be incorporated into rockfall simulation models, enabling a reliable and comprehensive design of rockfall mitigation interventions performed with this type of structure.},
  address  = {Bologna, Italy},
  doi      = {10.1016/j.proeng.2016.08.462},
  file     = {:Mentani2016 - A New Approach to Evaluate the Effectiveness of Rockfall Barriers.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {84988353699},
  review   = {Used PCE in UQLab.},
  url      = {http://dx.doi.org/10.1016/j.proeng.2016.08.462},
}

@InProceedings{Mylonas2017,
  author       = {Charilaos Mylonas and Bemetz Valentin and Eleni Chatzi},
  title        = {Multiscale surrogate modeling and uncertainty quantification for periodic composite structures},
  booktitle    = {Proceedings of the 2nd International Conference on Uncertainty Quantification in Computational Sciences and Engineering (UNCECOMP 2017)},
  year         = {2017},
  pages        = {406--418},
  address      = {Rhodes Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  abstract     = {Computational modeling of the structural behavior of continuous fiber composite materials often takes into account the periodicity of the underlying micro-structure. A well established method dealing with the structural behavior of periodic micro-structures is the socalled Asymptotic Expansion Homogenization (AEH). By considering a periodic perturbation of the material displacement, scale bridging functions, also referred to as elastic correctors, can be derived in order to connect the strains at the level of the macro-structure with microstructural strains. For complicated inhomogeneous micro-structures, the derivation of such functions is usually performed by the numerical solution of a PDE problem -Typically with the Finite Element Method. Moreover, when dealing with uncertain micro-structural geometry and material parameters, there is considerable uncertainty introduced in the actual stresses experienced by the materials. Due to the high computational cost of computing the elastic correctors, the choice of a pure Monte-Carlo approach for dealing with the inevitable material and geometric uncertainties is clearly computationally intractable. This problem is even more pronounced when the effect of damage in the micro-scale is considered, where re-evaluation of the micro-structural representative volume element is necessary for every occurring damage. The novelty in this paper is that a non-intrusive surrogate modeling approach is employed with the purpose of directly bridging the macro-scale behavior of the structure with the material behavior in the micro-scale, therefore reducing the number of costly evaluations of corrector functions, allowing for future developments on the incorporation of fatigue or static damage in the analysis of composite structural components.},
  doi          = {10.7712/120217.5379.16904},
  eventdate    = {2017-06-15/2017-06-17},
  file         = {:Mylonas2017 - Multiscale surrogate modeling and uncertainty quantification for periodic composite structures.pdf:PDF},
  keywords     = {civil engineering, used UQLab},
  piis         = {85043484343},
  url          = {http://dx.doi.org/10.7712/120217.5379.16904},
}

@InProceedings{Mylonas2017a,
  author       = {Charilaos Mylonas and Imad Abdallah and Eleni Chatzi},
  title        = {Surrogate modelling for fatigue damage of wind-turbine blades using polynomial chaos expansions and non-negative matrix factorization},
  booktitle    = {IABSE Symposium Vancouver 2017: Engineering the Future},
  year         = {2017},
  pages        = {809--816},
  address      = {Vancouver, British Columbia, Canada},
  organization = {International Association for Bridge and Structural Engineering (IABSE)},
  abstract     = {A computational approach for the estimation of fatigue degradation of composite wind turbine blades by means of time domain aero-servo-elastic simulations is proposed. Wind turbine blades are subjected throughout their lifetime to highly stochastic loading. Fatigue damage of the composite reinforcement of the wind turbine blades has been identified early on in the wind turbine design practice as a factor driving design. A simple fatigue accumulation model is utilized for the spar cap reinforcement of a wind-turbine blade. Non-Negative Matrix Factorization (NMF) for the damage accumulation random field is used for dimensionality reduction. An approximate computationally efficient model, relying on Polynomial Chaos Expansion (PCE) of the damage state with respect to probabilistically modelled mean wind and turbulence intensity is derived. The framework is exemplified in a case-study of a 1.5MW wind turbine.},
  eventdate    = {2017-09-19/2017-09-19},
  keywords     = {civil engineering, used UQLab},
  piis         = {85050026817},
  url          = {https://www.ingentaconnect.com/contentone/iabse/report/2017/00000109/00000058/art00001},
}

@Article{Ni2016,
  author   = {Fei Ni and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {Basis-adaptive sparse polynomial chaos expansion for probabilistic power flow},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2016},
  volume   = {32},
  number   = {1},
  pages    = {694--704},
  abstract = {This paper introduces the basis-adaptive sparse polynomial chaos (BASPC) expansion to perform the probabilistic power flow (PPF) analysis in power systems. The proposed method takes advantage of three state-of-the-art uncertainty quantification methodologies reasonably: the hyperbolic scheme to truncate the infinite polynomial chaos (PC) series; the least angle regression (LARS) technique to select the optimal degree of each univariate PC series; and the Copula to deal with nonlinear correlations among random input variables. Consequently, the proposed method brings appealing features to PPF, including the ability to handle the large-scale uncertainty sources; to tackle the nonlinear correlation among the random inputs; to analytically calculate representative statistics of the desired outputs; and to dramatically alleviate the computational burden as of traditional methods. The accuracy and efficiency of the proposed method are verified through either quantitative indicators or graphical results of PPF on both the IEEE European Low Voltage Test Feeder and the IEEE 123 Node Test Feeder, in the presence of more than 100 correlated uncertain input variables.},
  date     = {2016-04-27},
  doi      = {10.1109/TPWRS.2016.2558622},
  file     = {:Ni2016 - Basis-Adaptive Sparse Polynomial Chaos Expansion for Probabilistic Power Flow.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85008659770},
  url      = {http://dx.doi.org/10.1109/TPWRS.2016.2558622},
}

@Article{Ni2017,
  author   = {Fei Ni and Michiel Nijhuis and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {Variance-based global sensitivity analysis for power systems},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2017},
  volume   = {33},
  number   = {2},
  pages    = {1670--1682},
  abstract = {Knowledge of the impact of uncertain inputs is valuable, especially in power systems with large amounts of stochastic renewable generations. A global sensitivity analysis (GSA) can determine the impact of input uncertainties on the output quantity of interest in a certain physical or mathematical model. The GSA has not been widely employed in power systems due to the prohibitively computational burden. In this paper, it is demonstrated that, via the implementation of a basis-Adaptive sparse polynomial chaos expansion, a GSA can be applied to the power system with numerous uncertain inputs. The performance of the proposed method is tested on both the IEEE 13-bus test feeder and the IEEE 123-node test system, in presence of a large amount of independent or correlated uncertain inputs. The possible application of a GSA on the basis of the basis-Adaptive sparse polynomial chaos expansion in power systems are discussed in terms of various sensitivities. The findings cannot only be used to rank the most influential input uncertainties with respect to a specific output, such as variances of the nodal power, but also to identify the most sensitive or robust electrical variables such as the bus voltage with respect to input uncertainties.},
  date     = {2017-06-27},
  doi      = {10.1109/TPWRS.2017.2719046},
  file     = {:Ni2017a - Variance-Based Global Sensitivity Analysis for Power Systems.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85023758823},
  url      = {http://dx.doi.org/10.1109/TPWRS.2017.2719046},
}

@InProceedings{Palar2017,
  author       = {Pramudita Satria Palar and Koji Shimoyama},
  title        = {Polynomial-chaos-kriging-assisted efficient global optimization},
  booktitle    = {2017 IEEE Symposium Series on Computational Intelligence (SSCI 2017)},
  year         = {2017},
  pages        = {1--8},
  address      = {Honolulu, Hawaii, USA},
  organization = {IEEE},
  abstract     = {In this paper, we explore the use of the recently proposed polynomial chaos-Kriging (PCK) surrogate model to assist a single-objective efficient global optimization (EGO) framework in order to solve expensive optimization problems. PCK is a form of universal Kriging (UK) that employs orthogonal polynomials and least-angle-regression (LARS) algorithm to select the proper set of polynomial basis. The use of LARS within the PCK algorithm eliminates the need for the manual selection of UK's trend function. Investigation on the capability of PCK-EGO is performed on five synthetic and one aerodynamic test problems. In light of the results, we observe that PCK-EGO performs in a similar way to standard EGO in cases with no clear polynomial-like trend. However, PCK-EGO shows a notable faster convergence in problems where the objective function exhibits a landscape trend that can be captured by polynomials. Application to the subsonic wing problem further demonstrates that PCK-EGO is more efficient than EGO in a real-world aerodynamic optimization problem.},
  doi          = {10.1109/SSCI.2017.8280831},
  eventdate    = {2017-11-27/2017-12-01},
  file         = {:Palar2017 - Polynomial-chaos-kriging-assisted efficient global optimization.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85046164050},
  url          = {http://dx.doi.org/10.1109/SSCI.2017.8280831},
}

@InProceedings{Palar2018,
  author       = {Pramudita Satria Palar and Koji Shimoyama},
  title        = {Ensemble of kriging with multiple kernel functions for engineering design optimization},
  booktitle    = {International Conference on Bioinspired Methods and Their Applications (BIOMA 2018)},
  year         = {2018},
  pages        = {211--222},
  address      = {Paris, France},
  organization = {University of Lille and Jožef Stefan Institute},
  abstract     = {We introduce the ensemble of Kriging with multiple kernel functions guided by cross-validation error for creating a robust and accurate surrogate model to handle engineering design problems. By using the ensemble of Kriging models, the resulting ensemble model preserves the uncertainty structure of Kriging, thus, can be further exploited for Bayesian optimization. The objective of this paper is to develop a Kriging methodology that eliminates the needs for manual kernel selection which might not be optimal for a specific application. Kriging models with three kernel functions, that is, Gaussian, Matérn-3/2, and Matérn-5/2 are combined through a global and a local ensemble technique where their approximation quality are investigated on a set of aerodynamic problems. Results show that the ensemble approaches are more robust in terms of accuracy and able to perform similarly to the best performing individual kernel function or avoiding misspecification of kernel.},
  doi          = {10.1007/978-3-319-91641-5_18},
  eventdate    = {2018-05-16/2018-05-18},
  file         = {:Palar2018 - Ensemble of kriging with multiple kernel functions for engineering design optimization.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85047468928},
  url          = {http://dx.doi.org/10.1007/978-3-319-91641-5_18},
}

@Article{Perko2016,
  author   = {Zoltán Perkó and Sebastian R. Van Der Voort and Steven Van De Water and Charlotte M. H. Hartman and Mischa Hoogeman and Danny Lathouwers},
  title    = {Fast and accurate sensitivity analysis of {IMPT} treatment plans using {P}olynomial {C}haos {E}xpansion},
  journal  = {Physics in Medicine and Biology},
  year     = {2016},
  volume   = {61},
  pages    = {4646--4664},
  abstract = {The highly conformal planned dose distribution achievable in intensity modulated proton therapy (IMPT) can severely be compromised by uncertainties in patient setup and proton range. While several robust optimization approaches have been presented to address this issue, appropriate methods to accurately estimate the robustness of treatment plans are still lacking. To fill this gap we present Polynomial Chaos Expansion (PCE) techniques which are easily applicable and create a meta-model of the dose engine by approximating the dose in every voxel with multidimensional polynomials. This Polynomial Chaos (PC) model can be built in an automated fashion relatively cheaply and subsequently it can be used to perform comprehensive robustness analysis. We adapted PC to provide among others the expected dose, the dose variance, accurate probability distribution of dose-volume histogram (DVH) metrics (e.g. minimum tumor or maximum organ dose), exact bandwidths of DVHs, and to separate the effects of random and systematic errors. We present the outcome of our verification experiments based on 6 head-and-neck (HN) patients, and exemplify the usefulness of PCE by comparing a robust and a non-robust treatment plan for a selected HN case. The results suggest that PCE is highly valuable for both research and clinical applications.},
  date     = {2016-05-26},
  doi      = {10.1088/0031-9155/61/12/4646},
  file     = {:Perko2016 - Fast and accurate sensitivity analysis of IMPT treatment plans using Polynomial Chaos Expansion.pdf:PDF},
  keywords = {biomedical science, used UQLab},
  piis     = {84975041431},
  url      = {http://dx.doi.org/10.1088/0031-9155/61/12/4646},
}

@Article{Sanctis2016,
  author   = {Gianluca De Sanctis and Mario Fontana},
  title    = {Risk-based optimisation of fire safety egress provisions based on the {LQI} acceptance criterion},
  journal  = {Reliability Engineering and System Safety},
  year     = {2016},
  volume   = {152},
  pages    = {339--350},
  abstract = {Life safety is the primary objective of fire safety design and is mainly ensured by an appropriate design of the means of egress, which reduce the risk to life. An unlimited reduction of this risk is not desirable since it would lead to an immense investment of resources. The Life Quality Index (LQI) acceptance criterion is a societal indicator that is used to judge the efficiency of safety measures to reduce the risk to life and can be used to optimise the investments into life saving measures. The risk is assessed by a probabilistic engineering approach that considers the fire development and the individual-based evacuation process as well as the associated uncertainties. Advanced uncertainty propagation methods are applied in order to face the difficulties by using computational expensive models and by using individual-based models. Sensitivity measures are applied to reveal the contribution of the uncertainties on the estimation of the risk. The approach is applied for a risk-based optimisation of the minimal required door width for retail buildings.},
  date     = {2016-04-09},
  doi      = {10.1016/j.ress.2016.04.001},
  file     = {:Sanctis2016 - Risk-based optimisation of fire safety egress provisions based on the LQI acceptance criterion.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {84964397298},
  url      = {http://dx.doi.org/10.1016/j.ress.2016.04.001},
}

@InCollection{Schenkendorf2017,
  author    = {René Schenkendorf and Xiangzhong Xie and Ulrike Krewer},
  title     = {An efficient polynomial chaos expansion strategy for active fault identification of chemical processes},
  booktitle = {Computer Aided Chemical Engineering},
  publisher = {Elsevier},
  year      = {2017},
  pages     = {1675--1680},
  abstract  = {To gain profit from complex chemical processes, it is essential to ensure its proper operation, i.e. to avoid costly unexpected downtimes of underlying processing units. This paper explores a highly efficient active fault detection and isolation (FDI) framework, which facilitates the discriminability of a set of analysed model candidates including the reference model (nominal behaviour) as well as pre-defined failure models (faulty behaviour). Practically, an auxiliary, model-discriminating input is derived by solving a dynamic optimization problem. While using a model-based approach, the active FDI implementation has to be robustified against the inherent model parameter uncertainties. To this end, a non-intrusive polynomial chaos expansion (PCE) is used to address these uncertainties. To guarantee a computationally feasible performance, the original PCE setting has been considerably improved. Here, the basic idea is to render the design variables (auxiliary inputs) into random variables as well. Thus, the derived PCE results are not only sensitive to the model parameters but also to the design variables. To lower the computational burden further, a least angle regression strategy is applied utilizing the sparsity property of the PCE approach. The overall effectiveness of this One-Short Sparse Polynomial Chaos Expansion (OS2-PCE) concept for FDI is illustrated conceptually by analysing a tubular plug flow reactor.},
  date      = {2017-10-09},
  doi       = {10.1016/B978-0-444-63965-3.50281-6},
  file      = {:Schenkendorf2017 - An Efficient Polynomial Chaos Expansion Strategy for Active Fault Identification of Chemical Processes.pdf:PDF},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041401594},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50281-6},
}

@Article{Toe2017,
  author   = {David Toe and Franck Bourrier and Ignacio Olmedo and Jean-Matthieu Monnet and Frédéric Berger},
  title    = {Analysis of the effect of trees on block propagation using a {DEM} model: implications for rockfall modelling},
  journal  = {Landslides},
  year     = {2017},
  volume   = {14},
  number   = {5},
  pages    = {1603--1614},
  abstract = {The objective of this research was to use numerical models based on mechanical approaches to improve the integration of the protective role of forests against rockfall into block propagation models. A model based on the discrete element method (DEM) was developed to take into account the complex mechanical processes involved during the impact of a block on a tree. This modelling approach requires the definition of many input parameters and cannot be directly integrated into block propagation models. A global sensitivity analysis identified the leading parameters of the block kinematics after impact (i.e. block energy reduction, trajectory changes, and rotational velocity): the impact velocity, the tree diameter, and the impact point horizontal location (i.e. eccentricity). Comparisons with the previous experimental and numerical studies of block impacts on trees demonstrated the applicability of the DEM model and showed some of the limitations of earlier approaches. Our sensitivity analysis highlights the significant influence of the impact velocity on the reduction of the block’s kinetic energy. Previous approaches usually also focus on parameters such as impact height, impact vertical incidence, and tree species, whose importance is only minor according to the present results. This suggests that the integration of forest effects into block propagation models could be both improved and simplified. The DEM model can also be used as an alternative to classical approaches for the integration of forest effects by directly coupling it with block propagation models. This direct coupling only requires the additional definition of the location and the diameter of each tree. Indeed, the input parameters related to the mechanical properties of the stem and the block/stem interaction in the DEM model can be set to average values because they are not leading parameters. The other input parameters are already defined or calculated in the block propagation model.},
  date     = {2017-03-18},
  doi      = {10.1007/s10346-017-0799-6},
  file     = {:Toe2017 - Analysis of the effect of trees on block propagation using a DEM model_ implications for rockfall modelling.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {85015610979},
  url      = {http://dx.doi.org/10.1007/s10346-017-0799-6},
}

@Article{Toe2018,
  author   = {David Toe and Alessio Mentani and Laura Govoni and Franck Bourrier and Guido Gottardi and Stéphane Lambert},
  title    = {Introducing meta-models for a more efficient hazard mitigation strategy with rockfall protection barriers},
  journal  = {Rock Mechanics and Rock Engineering},
  year     = {2018},
  volume   = {51},
  number   = {4},
  pages    = {1097--1109},
  abstract = {The paper presents a new approach to assess the effecctiveness of rockfall protection barriers, accounting for the wide variety of impact conditions observed on natural sites. This approach makes use of meta-models, considering a widely used rockfall barrier type and was developed from on FE simulation results. Six input parameters relevant to the block impact conditions have been considered. Two meta-models were developed concerning the barrier capability either of stopping the block or in reducing its kinetic energy. The outcome of the parameters range on the meta-model accuracy has been also investigated. The results of the study reveal that the meta-models are effective in reproducing with accuracy the response of the barrier to any impact conditions, providing a formidable tool to support the design of these structures. Furthermore, allowing to accommodate the effects of the impact conditions on the prediction of the block–barrier interaction, the approach can be successfully used in combination with rockfall trajectory simulation tools to improve rockfall quantitative hazard assessment and optimise rockfall mitigation strategies.},
  date     = {2018-01-10},
  doi      = {10.1007/s00603-017-1394-9},
  file     = {:Toe2018 - Introducing Meta-models for a More Efficient Hazard Mitigation Strategy with Rockfall Protection Barriers.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {85040370552},
  url      = {http://dx.doi.org/10.1007/s00603-017-1394-9},
}

@Article{Turati2018,
  author   = {Pietro Turati and Antonio Cammi and Stefano Lorenzi and Nicola Pedroni and Enrico Zio},
  title    = {Adaptive simulation for failure identification in the {A}dvanced {L}ead {F}ast {R}eactor {E}uropean {D}emonstrator},
  journal  = {Progress in Nuclear Energy},
  year     = {2018},
  volume   = {103},
  pages    = {176--190},
  abstract = {The identification undesired or abnormal states of a nuclear power plant is of primary importance for defining accident prevention and mitigation actions. To this aim, computational models and simulators are frequently employed, as they allow to study the system response to different operational conditions. For complex systems like the nuclear power plants, this is in general challenging because the simulation tools are i) high-dimensional; ii) black-box; iii) dynamic and iv) computationally demanding. In this paper, an adaptive simulation framework recently proposed by some of the authors is tailored for the analysis of accident scenarios involving the control system of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED). The results confirm that the adaptive simulation framework proposed is effective in identifying critical regions of operation with a limited number of calls to the computationally expensive model. The time of occurrence and magnitude of the failures of the components of the control system are identified as key factors to characterize the critical regions. In particular, it is shown that the order of occurrence of the components’ failures strongly affects the evolution of the accident scenarios.},
  date     = {2017-12-22},
  doi      = {10.1016/j.pnucene.2017.11.013},
  file     = {:Turati2018 - Adaptive simulation for failure identification in the Advanced Lead Fast Reactor European Demonstrator.pdf:PDF},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85037379518},
  url      = {http://dx.doi.org/10.1016/j.pnucene.2017.11.013},
}

@Article{Vohra2018,
  author   = {Manav Vohra and Ali Yousefzadi Nobakht and Seungha Shin and Sankaran Mahadevan},
  title    = {Uncertainty quantification in non-equilibrium molecular dynamics simulations of thermal transport},
  journal  = {International Journal of Heat and Mass Transfer},
  year     = {2018},
  volume   = {127},
  pages    = {297--307},
  abstract = {Bulk thermal conductivity estimates based on predictions from non-equilibrium molecular dynamics (NEMD) using the so-called direct method are known to be severely under-predicted since finite simulation length-scales are unable to mimic bulk transport. Moreover, subjecting the system to a temperature gradient by means of thermostatting tends to impact phonon transport adversely. Additionally, NEMD predictions are tightly coupled with the choice of the inter-atomic potential and the underlying values associated with its parameters. In the case of silicon (Si), nominal estimates of the Stillinger-Weber (SW) potential parameters are largely based on a constrained regression approach aimed at agreement with experimental data while ensuring structural stability. However, this approach has its shortcomings and it may not be ideal to use the same set of parameters to study a wide variety of Si-based systems subjected to different thermodynamic conditions. In this study, NEMD simulations are performed on a Si bar to investigate the impact of bar-length, and the applied temperature gradient on the discrepancy between predictions and the available measurement for bulk thermal conductivity at 300 K by constructing statistical response surfaces at different temperatures. The approach helps quantify the discrepancy, observed to be largely dependent on the system-size, with minimal computational effort. A computationally efficient approach based on derivative-based sensitivity measures to construct a reduced-order polynomial chaos surrogate for NEMD predictions is also presented. The surrogate is used to perform parametric sensitivity analysis, forward propagation of the uncertainty, and calibration of the important SW potential parameters in a Bayesian setting. It is found that only two (out of seven) parameters contribute significantly to the uncertainty in bulk thermal conductivity estimates for Si.},
  date     = {2018-07-29},
  doi      = {10.1016/j.ijheatmasstransfer.2018.07.073},
  file     = {:Vohra2018 - Uncertainty quantification in non-equilibrium molecular dynamics simulations of thermal transport.pdf:PDF},
  keywords = {mechanical engineering, used UQLab},
  piis     = {85050528394},
  url      = {http://dx.doi.org/10.1016/j.ijheatmasstransfer.2018.07.073},
}

@Article{Wu2018,
  author   = {Xu Wu and Tomasz Kozlowski and Hadi Meidani},
  title    = {Kriging-based inverse uncertainty quantification of nuclear fuel performance code {BISON} fission gas release model using time series measurement data},
  journal  = {Reliability Engineering and System Safety},
  year     = {2018},
  volume   = {169},
  pages    = {422--436},
  abstract = {In nuclear reactor fuel performance simulation, fission gas release (FGR) and swelling involve treatment of several complicated and interrelated physical processes, which inevitably depend on uncertain input parameters. However, the uncertainties associated with these input parameters are only known by “expert judgment”. In this paper, inverse Uncertainty Quantification (UQ) under the Bayesian framework is applied to BISON code FGR model based on Risø-AN3 time series experimental data. Inverse UQ seeks statistical descriptions of the uncertain input parameters that are consistent with the available measurement data. It always captures the uncertainties in its estimates rather than merely determining the best-fit values. Kriging metamodel is applied to greatly reduce the computational cost during Markov Chain Monte Carlo sampling. We performed a dimension reduction for the FGR time series data using Principal Component Analysis. We also projected the original FGR time series measurement data onto the PC subspace as “transformed experiment data”. A forward uncertainty propagation based on the posterior distributions shows that the agreement between BISON simulation and Ris{\n}-AN3 time series measurement data is greatly improved. The posterior distributions for the uncertain input factors can be used to replace the expert specifications for future uncertainty/sensitivity analysis.},
  date     = {2017-10-03},
  doi      = {10.1016/j.ress.2017.09.029},
  file     = {:Wu2018a - Kriging-based inverse uncertainty quantification of nuclear fuel performance code BISON fission gas release model using time series measurement data.pdf:PDF},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85030669828},
  url      = {http://dx.doi.org/10.1016/j.ress.2017.09.029},
}

@InCollection{Xie2017,
  author    = {Xiangzhong Xie and René Schenkendorf and Ulrike Krewer},
  title     = {Robust design of chemical processes based on a one-shot sparse polynomial chaos expansion concept},
  booktitle = {Computer Aided Chemical Engineering},
  publisher = {Elsevier},
  year      = {2017},
  pages     = {613--618},
  abstract  = {The application of robust model-based design concepts for complex chemical processes is limited due to the repeated cpu-intensive uncertainty quantification step for any new tested process design configuration. Therefore, an efficient One-Shot Sparse Polynomial Chaos Expansion (OS2-PCE) based process design framework is introduced in this work. The key idea is to define the process design variables as uncertain quantities as well and, in consequence, they become an integral part of the robust optimization routine. Moreover, by utilizing the sparsity feature of the PCE approach, the implementation of a least angle regression (LAR) concept leads to a significant reduction in computational costs. The overall performance of the novel OS2-PCE approach is illustrated by a robust process design study of a jacketed tubular reactor. In comparison to state-of-the-art concepts, the proposed framework shows promising results in terms of efficiency and robustness.},
  date      = {2017-10-01},
  doi       = {10.1016/B978-0-444-63965-3.50104-5},
  file      = {:Xie2017 - Robust Design of Chemical Processes Based on a One-Shot Sparse Polynomial Chaos Expansion Concept.pdf:PDF},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041412536},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50104-5},
}

@Article{Xie2018,
  author   = {Xiangzhong Xie and Rüdiger Ohs and Antje Spieß and Ulrike Krewer and René Schenkendorf},
  title    = {Moment-independent sensitivity analysis of enzyme-catalyzed reactions with correlated model parameters},
  journal  = {IFAC-PapersOnLine},
  year     = {2018},
  volume   = {51},
  number   = {2},
  pages    = {753--758},
  abstract = {The dynamic models used for biological and chemical process analysis and design usually include a significant number of uncertain model parameters. Sensitivity analysis is frequently applied to provide quantitative information regarding the influence of the parameters, as well as their uncertainties, on the model output. Various techniques are available in the literature to calculate parameter sensitivities based on local derivatives or changes in dedicated statistical moments of the model output. However, these methods may lead to an inevitable loss of information for a proper sensitivity analysis and are not directly available for problems with correlated model parameters. In this work, we demonstrate the use of a moment-independent sensitivity analysis concept in the presence and absence of parameter correlations and investigate the correlation effect in more detail. Moment-independent sensitivity analysis calculates parameter sensitivities based on changes in the entire probability density distribution of the model output and is formulated independently of whether the parameters are correlated or not. Technically, a single-loop Monte Carlo simulation method in combination with polynomial chaos expansion is implemented to reduce the computational cost significantly. A sampling procedure derived from Gaussian copula formalism is used to generate sample points for arbitrarily correlated uncertain parameters. The proposed concept is demonstrated with a case study of an enzyme-catalyzed reaction network. We observe evident differences in the parameter sensitivities for cases with independent and correlated model parameters.},
  date     = {2018-05-03},
  doi      = {10.1016/j.ifacol.2018.04.004},
  file     = {:Xie2018 - Moment-Independent Sensitivity Analysis of Enzyme-Catalyzed Reactions with Correlated Model Parameters.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  piis     = {85046655059},
  url      = {http://dx.doi.org/10.1016/j.ifacol.2018.04.004},
}

@Article{Yuzugullu2017,
  author   = {Onur Yuzugullu and Esra Erten and Irena Hajnsek},
  title    = {Estimation of rice crop height from {X-and C-Band PolSAR} by metamodel-based optimization},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year     = {2017},
  volume   = {10},
  number   = {1},
  pages    = {194--204},
  abstract = {Rice crops are important in global food economy and are monitored by precise agricultural methods, in which crop morphology in high spatial resolution becomes the point of interest. Synthetic aperture radar (SAR) technology is being used for such agricultural purposes. Using polarimetric SAR (PolSAR) data, plant morphology dependent electromagnetic scattering models can be used to approximate the backscattering behaviors of the crops. However, the inversion of such models for the morphology estimation is complex, ill-posed, and computationally expensive. Here, a metamodel-based probabilistic inversion algorithm is proposed to invert the morphology-based scattering model for the crop biophysical parameter mainly focusing on the crop height estimation. The accuracy of the proposed approach is tested with ground measured biophysical parameters on rice fields in two different bands (X and C) and several channel combinations. Results show that in C-band the combination of the HH and VV channels has the highest overall accuracy through the crop growth cycle. Finally, the proposed metamodel-based probabilistic biophysical parameter retrieval algorithm allows estimation of rice crop height using PolSAR data with high accuracy and low computation cost. This research provides a new perspective on the use of PolSAR data in modern precise agriculture studies.},
  date     = {2016-06-29},
  doi      = {10.1109/JSTARS.2016.2575362},
  file     = {:Yuzugullu2017 - Estimation of Rice Crop Height from X-and C-Band PolSAR by Metamodel-Based Optimization.pdf:PDF},
  keywords = {sensors engineering, used UQLab},
  piis     = {84976511966},
  url      = {http://dx.doi.org/10.1109/JSTARS.2016.2575362},
}

@Article{Xu2018,
  author   = {Xiaoyuan Xu and Zheng Yan and Mohammad Shahidehpour and Sijie Chen and Han Wang and Quan Zhou},
  title    = {Maximum loadability of islanded microgrids with renewable energy generation},
  journal  = {IEEE Transactions on Smart Grid},
  year     = {2018},
  volume   = {early access},
  abstract = {A microgrid (MG) is a small-scale power system which is fed by constrained distributed generation (DG) units and its continuous operation is affected by the variability of available generation resources. In this paper a global sensitivity analysis (GSA) method is proposed to evaluate the impact of variable energy resources on the maximum loadability of islanded MGs (IMGs). First, a probabilistic optimization problem is formulated to calculate the IMG load margin considering the droop characteristics of DG units and uncertainties of renewable generation, loads and distribution feeder parameters. Then, the global sensitivity (GS) is introduced that can identify the impact of independent and correlated variables on the IMG loadability. Next, the sparse polynomial chaos expansion (SPCE) method is used to obtain the probabilistic models for IMG load margins, and an efficient GSA method is proposed to calculate the GS of IMG loadability to prevailing variables. The probabilistic models are considered for IMG input variables and the impact of variable correlations on IMG loadability is analyzed. The proposed method for calculating the maximum loadability is tested using a 33-bus IMG and the results are compared with those of other GSA and local sensitivity analysis (LSA) methods.},
  date     = {2018-06-19},
  doi      = {10.1109/TSG.2018.2848958},
  file     = {:Xu2018 - Maximum loadability of islanded microgrids with renewable energy generation.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  url      = {https://dx.doi.org/10.1109/TSG.2018.2848958},
}

@Article{Yuzugullu2018,
  author   = {Onur Yuzugullu and Esra Erten and Irena Hajnsek},
  title    = {Assessment of paddy rice height: sequential inversion of coherent and incoherent models},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year     = {2018},
  volume   = {11},
  number   = {9},
  pages    = {3001--3013},
  abstract = {This paper investigates the evolution of canopy height of rice fields for a complete growth cycle. For this purpose, copolar interferometric Synthetic Aperture Radar (Pol-InSAR) time series data were acquired during the large across-track baseline (>1 km) science phase of the TanDEM-X mission. The height of rice canopies is estimated by three different model-based approaches. The first approach evaluates the inversion of the Random Volume over Ground (RVoG) model. The second approach evaluates the inversion of a metamodel-driven electromagnetic backscattering model by including a priori morphological information. The third approach combines the previous two processes. The validation analysis was carried out using the Pol-InSAR and ground measurement data acquired between May and September in 2015 over rice fields located in Ipsala district of Edirne, Turkey. The results of presented height estimation algorithms demonstrated the advantage of Pol-InSAR data. The combined RvoG model and EM metamodel height estimation approach provided rice canopy heights with errors less than 20 cm for the complete growth cycle.},
  date     = {2018-06-26},
  doi      = {10.1109/JSTARS.2018.2844798},
  file     = {:Yuzugullu2018 - Assessment of paddy rice height_ sequential inversion of coherent and incoherent models.pdf:PDF},
  keywords = {monitoring and remote sensing, used UQLab},
  url      = {https://dx.doi.org/10.1109/JSTARS.2018.2844798},
}

@InProceedings{Larbi2018b,
  author       = {Mourad Larbi and H. M. Torun and M. Swaminathan and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Uncertainty quantification of {SiP} based integrated voltage regulator},
  booktitle    = {2018 IEEE 22nd Workshop on Signal and Power Integrity (SPI)},
  year         = {2018},
  address      = {Brest, France},
  organization = {IEEE},
  abstract     = {This paper deals with the uncertainty quantification applied to the analysis of Integrated Voltage Regulator (IVR) efficiency. It presents a meta-model based on a sparse polynomial chaos technique, aiming at estimating statistical quantities of a response with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained are validated against MC simulation.},
  date         = {2018-07-02},
  doi          = {10.1109/SaPIW.2018.8401677},
  eventdate    = {2018-05-22/2018-05-25},
  file         = {:Larbi2018b - Uncertainty quantification of SiP based integrated voltage regulator.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  url          = {https://dx.doi.org/10.1109/SaPIW.2018.8401677},
}

@InProceedings{Zhang2018,
  author       = {Jiangjiang Zhang and Laosheng Wu and Lingzao Zeng},
  title        = {Surrogate-based {Bayesian} inverse modeling of the hydrological system: an adaptive approach considering surrogate structural error},
  booktitle    = {The AGU 2018 Fall Meeting},
  year         = {2018},
  address      = {Washington, D.C., USA},
  organization = {American Geophysical Union (AGU)},
  abstract     = {Inverse modeling is vital for an improved hydrological prediction. However, this process can be computationally demanding as it usually requires a large number of model evaluations. To address this issue, one can take advantage of surrogate modeling techniques, e.g., the one based on sparse polynomial chaos expansion (PCE). Nevertheless, when structural error of the surrogate model is neglected in inverse modeling, the inversion results will be biased. In this paper, we develop a surrogate-based Bayesian inversion framework that rigorously quantifies and gradually eliminates the structural error of the surrogate. Specifically, two strategies are proposed and compared. The first strategy works by obtaining an ensemble of sparse PCE surrogates with Markov chain Monte Carlo sampling, while the second one uses Gaussian process (GP) to simulate the structural error of a single sparse PCE surrogate. With an active learning process, the surrogate structural error can be gradually reduced to a negligible level in the posterior region, where the original input-output relationship can be much more easily captured by PCE than in the prior. Demonstrated by one numerical case of groundwater contaminant source identification with 28 unknown input variables, it is found that both strategies can efficiently reduce the bias introduced by surrogate modeling, while the second strategy has a better performance as it integrates two methods (i.e., PCE and GP) that complement each other.},
  date         = {2018-12-14},
  eventdate    = {2018-12-10/2018-12-14},
  file         = {:Zhang2018 - Surrogate-based Bayesian inverse modeling of the hydrological system_ an adaptive approach considering surrogate structural error.pdf:PDF},
  keywords     = {geoscience, used UQLab},
  url          = {https://arxiv.org/abs/1807.05187},
}

@InProceedings{Miers2018,
  author       = {Collier Miers and Amy Marconnet},
  title        = {Uncertainty quantification for a high temperature {Z-Meter} characterization system},
  booktitle    = {2018 17th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm)},
  year         = {2018},
  pages        = {572--581},
  address      = {San Diego, California, USA},
  organization = {IEEE},
  note         = {Date is the "date added to IEEE Xplore"},
  abstract     = {Few systems have been developed for simultaneous measurement of thermal, electrical, and thermoelectric properties at high temperatures (> 600K) relevant to high temperature waste heat recovery applications. The Z-Meter approach enables simultaneous measurements of the three thermoelectric properties from which the figure of merit, ZT, is calculated. The method combines an ASTM D5470 reference bar measurement for thermal conductivity (often used for thermal interface materials) with the added functionality of electrical measurements for electrical conductivity and Seebeck Coefficient. Traditionally, this technique has been employed utilizing a very small temperature difference across the sample (ΔT ≈ 1 - 2K) and test temperatures between approximately 300 - 600K, but thermoelectric materials must generally have sizable temperature gradients across them to achieve desirable performance as increased operating temperatures generally allow access to higher grade waste heat, and the relevant properties are also significantly temperature dependent. Here, we design a Z-meter system to evaluate the performance of materials under large temperature differences relevant to such applications. The design of the instrument is driven by uncertainty quantification to minimize measurement error. A detailed design of experiments model enables informed decisions regarding the component and system designs (e.g. placement of the temperature sensors). This design of experiments model includes effects of radiative losses in the meter bars as well as the losses due to the measurement probes installed along the bars. The system as designed is capable of hot side temperatures of 1350K with the thermoelectric material to temperature gradients on the order of 500K. The elevated temperatures are necessary to fill a gap in characterization equipment for very high temperature thermoelectric applications. The measurement system requires vacuum pressure of 1 μTorr for suppression of convection losses and to prevent of oxidation of the hot system components. The system design employs a loading platform that allows the interfacial sample/bar loading to be changed in situ without breaking system vacuum allowing for high measurement throughput. Additionally, a triad of integrated load cells ensure repeatable loading conditions for the sample interface. The combination of simultaneous thermal, electrical, and thermoelectric properties at high temperatures with controlled mechanical loading opens the doors to new understanding of thermoelectrics for high temperature waste heat recovery. Ultimately, this uncertainty analysis coupled with the design process demonstrates the limits of using this system and guides operating conditions for given expected sampleproperties.},
  date         = {2018-07-26},
  doi          = {10.1109/ITHERM.2018.8419511},
  eventdate    = {2018-05-29/2018-06-01},
  file         = {:Miers2018 - Uncertainty quantification for a high temperature Z-Meter characterization system.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  url          = {https://dx.doi.org/10.1109/ITHERM.2018.8419511},
}

@Article{Palar2018a,
  author   = {Pramudita Satria Palar and Koji Shimoyama},
  title    = {Efficient global optimization with ensemble and selection of kernel functions for engineering design},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  pages    = {1--24},
  abstract = {In this paper, we investigate the use of multiple kernel functions for assisting single-objective Kriging-based efficient global optimization (EGO). The primary objective is to improve the robustness of EGO in terms of the choice of kernel function for solving a variety of black-box optimization problems in engineering design. Specifically, three widely used kernel functions are studied, that is, Gaussian, Matérn-3/2, and Matérn-5/2 function. We investigate both model selection and ensemble techniques based on Akaike information criterion (AIC) and cross-validation error on a set of synthetic (noiseless and noisy) and non-algebraic (aerodynamic and parameter tuning) optimization problems; in addition, the use of cross-validation-based local (i.e., pointwise) ensemble is also studied. Since all the constituent surrogate models in the ensemble scheme are Kriging models, it is possible to perform EGO since the Kriging uncertainty structure is still preserved. Through analyses of empirical experiments, it is revealed that the ensemble techniques improve the robustness and performance of EGO. It is also revealed that the use of Matérn-kernels yields better results than those of the Gaussian kernel when EGO with a single kernel is considered. Furthermore, we observe that model selection methods do not yield any substantial improvement over single kernel EGO. When averaged across all types of problem (i.e., noise level, dimensionality, and synthetic/non-algebraic), the local ensemble technique achieves the best performance.},
  date     = {2018-07-28},
  doi      = {10.1007/s00158-018-2053-9},
  file     = {:Palar2018a - Efficient global optimization with ensemble and selection of kernel functions for engineering design.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1007/s00158-018-2053-9},
}

@InProceedings{Prakash2018,
  author       = {Chandra Prakash and I. Emre Gunduz and Vikas Tomar},
  title        = {Uncertainty quantification in nanoscale impact experiment in energetic materials},
  booktitle    = {Proceedings of the 36th IMAC, a Conference and Exposition on Structural Dynamics},
  year         = {2018},
  pages        = {257--263},
  address      = {Orlando, Florida, USA},
  organization = {Society for Experimental Mechanics},
  abstract     = {Finite element method is extensively used for the analysis of impact response in complex materials. The prediction from finite element model may exhibit significant difference from that of experiments due to uncertainties in model, experimental measurements, and parameters that are derived based on experiments for model development. The quantification of parametric uncertainties, such as parameters in constitutive relation, associated with the numerical model is an important aspect that needs to be investigated for a credible computational prediction. This work considers uncertainty quantification in finite element modeling of nanoscale dynamic impact problems. A viscoplastic power law constitutive model is obtained from nanoscale impact experiments on Hydroxyl-terminated polybutadiene (HTPB)-Ammonium Perchlorate (AP) samples. The constitutive model is used in a finite element model to simulate impact experiments. The measured response from impact experiment and FEM simulation is used to quantify the parametric uncertainties in the constitutive model for the analyzed HTPB-AP sample.},
  date         = {2018-07-31},
  doi          = {10.1007/978-3-319-74793-4_30},
  eventdate    = {2018-02-12/2018-02-15},
  file         = {:Prakash2018 - Uncertainty quantification in nanoscale impact experiment in energetic materials.pdf:PDF},
  keywords     = {mechanical engineering, used UQLab},
  url          = {https://doi.org/10.1007/978-3-319-74793-4_30},
}

@InProceedings{Ligeikis2018,
  author       = {Connor Ligeikis and Alex Freeman and Richard Christenson},
  title        = {Assessing structural reliability at the component test stage using real-time hybrid substructuring},
  booktitle    = {Proceedings of the 36th IMAC, a Conference and Exposition on Structural Dynamics},
  year         = {2018},
  pages        = {75--78},
  address      = {Orlando, Florida, USA},
  organization = {Society for Experimental Mechanics},
  abstract     = {The propagation of uncertainties through complex systems is a challenging endeavor. While numerical simulations can be used to accurately predict the dynamic performance of structural systems, there are some instances where the dynamics and uncertainties of specific components may be less understood or difficult to accurately model. This paper will implement a structural reliability assessment employing the cyber-physical real-time hybrid substructuring (RTHS) method to combine a numerical model of a larger structural system, incorporating uncertainty in specific parameters, with a physical test specimen of a component of the system while fully incorporating the system-level dynamic interactions and uncertainty propagation. This RTHS approach will allow for uncertainty and reliability to be addressed in the early stage of the design process as components become available and the remainder of the system remains numerically modeled. A small-scale RTHS experiment will be used to demonstrate the probability of failure of a spring-mass-damper system with a relatively small number of component tests by employing the previously proposed Adaptive Kriging-Hybrid Simulation (AK-HS) reliability method.},
  date         = {2018-07-31},
  doi          = {10.1007/978-3-319-74793-4_11},
  eventdate    = {2018-02-12/2018-02-15},
  file         = {:Ligeikis2018 - Assessing structural reliability at the component test stage using real-time hybrid substructuring.pdf:PDF},
  keywords     = {civil engineering, used UQLab},
  url          = {https://doi.org/10.1007/978-3-319-74793-4_11},
}

@Article{Aleksankina2018,
  author   = {Ksenia Aleksankina and Stefan Reis and Massimo Vieno and Mathew R. Heal},
  title    = {Advanced methods for uncertainty assessment and global sensitivity analysis of a {E}ulerian atmospheric chemistry transport model},
  journal  = {Atmospheric Chemistry and Physics Discussion},
  year     = {2018},
  volume   = {in review},
  abstract = {Atmospheric chemistry transport models (ACTMs) are extensively used to provide scientific support for the development of policies to mitigate against the detrimental effects of air pollution on human health and ecosystems. Therefore, it is essential to quantitatively assess the level of model uncertainty and to identify the model input parameters that contribute the most to the uncertainty. For complex process-based models, such as ACTMs, uncertainty and global sensitivity analyses are still challenging and are often limited by computational constraints due to the requirement of a large number of model runs. In this work, we demonstrate an emulator-based approach to uncertainty quantification and variance-based sensitivity analysis for the EMEP4UK model (regional application of the European Monitoring and Evaluation Programme Meteorological Synthesizing Centre-West). A separate Gaussian process emulator was used to estimate model predictions at unsampled points in the space of the uncertain model inputs for every modelled grid cell. The training points for the emulator were chosen using an optimised Latin hypercube sampling design. The uncertainties in surface concentrations of O3, NO2, and PM2.5 were propagated from the uncertainties in the anthropogenic emissions of NOx, SO2, NH3, VOC, and primary PM2.5 reported by the UK National Atmospheric Emissions Inventory. The results of the EMEP4UK uncertainty analysis for the annually averaged model predictions indicate that modelled surface concentrations of O3, NO2, and PM2.5 have the highest level of uncertainty in the grid cells comprising urban areas (up to \pm 7\%, \pm 9\%, and \pm 9\% respectively). The uncertainty in the surface concentrations of O3 and NO2 were dominated by uncertainties in NOx emissions combined from non-dominant sectors (i.e. all sectors excluding energy production and road transport) and shipping emissions. Additionally, uncertainty in OO3 was driven by uncertainty VOC emissions combined from sectors excluding solvent use. Uncertainties in the modelled PM2.5 concentrations were mainly driven by uncertainties in primary PM2.5 emissions and NH3 emissions from the agricultural sector. Uncertainty and sensitivity analyses were also performed for five selected grid sells for monthly averaged model predictions to illustrate the seasonal change in the magnitude of uncertainty and change in the contribution of different model inputs to the overall uncertainty. Our study demonstrates the viability of a Gaussian process emulator-based approach for uncertainty and global sensitivity analyses, which can be applied to other ACTMs. Conducting these analyses helps to increase the confidence in model predictions. Additionally, the emulators created for these analyses can be used to predict the ACTM response for any other combination of perturbed input emissions within the ranges set for the original Latin hypercube sampling design without the need to re-run the ACTM, thus allowing fast exploratory assessments at significantly reduced computational costs.},
  date     = {2018-07-26},
  doi      = {10.5194/acp-2018-690},
  file     = {:Aleksankina2018 - Advanced methods for uncertainty assessment and global sensitivity analysis of a Eulerian atmospheric chemistry transport model.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.5194/acp-2018-690},
}

@InProceedings{Errante2018,
  author       = {Paolo Errante and Christophe Corre},
  title        = {Uncertainty quantification for the {E}ulerian-{L}agrangian simulation of evaporating sprays},
  booktitle    = {the 14th Triennial International Conference on Liquid Atomization and Spray Systems (ICLASS 2018)},
  year         = {2018},
  address      = {Chicago, Illinois, USA},
  organization = {Institute for Liquid Atomization and Spray Systems (ILASS)},
  abstract     = {Evaporating sprays can be almost routinely simulated using an Eulerian-Lagrangian approach which relies on a Reynolds Averaged Navier Stokes (RANS) modeling of the continuous phase and a Lagrangian description of the discrete phase, including a turbulent dispersion model to express the effect of turbulent fluctuations within the carrier phase on the spray particles and an evaporation model for the spray droplets. Both descriptions are coupled through a two-way approach which accounts for the effects of the continuous phase on the droplets and the retroaction of the droplets on the carrier phase. Several experiments available in the literature have been used to calibrate the physical models involved in the numerical prediction of evaporating sprays (see [6][7]). The present study focuses on the simulation of evaporative sprays taking into account existing experimental and modeling uncertainties. Due to the significant cost of the numerical prediction, a surrogate model is used to quantify the effect of these uncertainties on quantities of interest. The non-intrusive Polynomial Chaos Expansion (PCE) technique available in the uncertainty quantification framework software UQLab is coupled with the deterministic Computational Fluid Dynamics (CFD) solver to yield statistical outputs (typically mean value and variance) of key quantities, such as the liquid mass flow rate for instance. The study analyzes the sensitivity of these outputs to experimental uncertainties on the continuous and discrete phase inputs (gas inlet velocity distribution, inlet liquid mass flow rate) and to modeling uncertainties in the turbulent dispersion model and evaporation model.},
  date         = {2018-07-26},
  eventdate    = {2018-07-22/2018-07-26},
  file         = {:Errante2018 - Uncertainty quantification for the Eulerian-Lagrangian simulation of evaporating sprays.pdf:PDF},
  keywords     = {mechanical engineering, used UQLab},
  url          = {https://www.researchgate.net/publication/326741120_Uncertainty_Quantification_for_the_Eulerian-Lagrangian_simulation_of_evaporating_sprays},
}

@Article{Naik2018,
  author   = {Pratik Naik and Soroush Aramideh and Arezoo M. Ardekani},
  title    = {History matching of surfactant-polymer flooding using polynomial chaos expansion},
  journal  = {Journal of Petroleum Science and Engineering},
  year     = {2018},
  volume   = {173},
  pages    = {1438--1452},
  note     = {Date is "available online"},
  abstract = {This paper proposes a robust framework for history matching which employs a sequential execution of sensitivity analysis, proxy modeling and inverse optimization to determine the optimized parameter space of model parameters. A mechanistic surfactant-polymer (SP) flood model is considered for history matching with an ultimate goal of accurately calibrating models that describe physical subprocesses of surfactant flooding, polymer flooding and displacement process. The employed model calibration algorithm starts with Sobol sensitivity analysis which reduces the large uncertain space of model parameters to determine the most important stochastic variables. The resulting low-dimensional parameter space is then represented via appropriate orthonormal basis of polynomial chaos expansion (PCE-proxy). An inverse optimization problem is then posed that minimizes the miss-fit between PCE-proxy response and experimental observations by employing a Genetic Algorithm. Finally, the epistemic uncertainty in PCE-proxy is quantified by combining it with a Gaussian regression process called Kriging.

We use this framework to calibrate the SP flood model by history matching a single coreflood experiment for quantities of interest such as pressure drop profile and cumulative oil recovery curve. We then show that the calibrated model is successfully able to predict all our quantities of interest for two other coreflood experiments without any ad-hoc tuning of parameters. The proposed proxy-accelerated inverse optimization framework shows significant promise for model calibration or to improve the quality of history matched results.},
  date     = {2018-10-05},
  doi      = {10.1016/j.petrol.2018.09.089},
  file     = {:Naik2018 - History matching of surfactant-polymer flooding using polynomial chaos expansion.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.petrol.2018.09.089},
}

@Article{Wang2018,
  author   = {Han Wang and Zheng Yan and Xiaoyuan Xu and Kun He},
  title    = {Evaluating influence of variable renewable energy generation on islanded microgrid power flow},
  journal  = {IEEE Access},
  year     = {2018},
  pages    = {71339--71349},
  abstract = {With the proliferation of renewable energy, the uncertainty has challenged the continuous operation of microgrids; thus, it is of importance to tackle uncertainties in power system operation. In this paper, a global sensitivity analysis (GSA) method is proposed to evaluate the influence of uncertainties on the power flow of islanded microgrids (IMGs). First, a probabilistic power flow model for IMGs is established considering the droop-controlled distributed generation units and the uncertainties of renewable energy generation output and load demands. Then, the global sensitivity analysis is introduced to identify important variables that affect IMG power flow. In addition to conventional GSA indices, the Shapley value-based GSA index is designed to evaluate the influence of correlated input variables. Moreover, the sparse polynomial chaos expansion is used to establish the surrogate models of IMG power flow, which improves the efficiency of GSA. Finally, the proposed method is tested on the 33-bus and 69-bus IMG systems, and the simulation results are compared with those considering other methods. The rankings of random input variables that affect IMG power flow are given, and the influence of correlation between different variables is discussed.},
  date     = {2018-11-13},
  doi      = {10.1109/ACCESS.2018.2881189},
  file     = {:Wang2018 - Evaluating influence of variable renewable energy generation on islanded microgrid power flow.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  review   = {With the proliferation of renewable energy, the uncertainty has challenged the continuous operation of microgrids; thus, it is of importance to tackle uncertainties in power system operation. In this paper, a global sensitivity analysis (GSA) method is proposed to evaluate the influence of uncertainties on the power flow of islanded microgrids (IMGs). First, a probabilistic power flow model for IMGs is established considering the droop-controlled distributed generation units and the uncertainties of renewable energy generation output and load demands. Then, the global sensitivity analysis is introduced to identify important variables that affect IMG power flow. In addition to conventional GSA indices, the Shapley value-based GSA index is designed to evaluate the influence of correlated input variables. Moreover, the sparse polynomial chaos expansion is used to establish the surrogate models of IMG power flow, which improves the efficiency of GSA. Finally, the proposed method is tested on the 33-bus and 69-bus IMG systems, and the simulation results are compared with those considering other methods. The rankings of random input variables that affect IMG power flow are given, and the influence of correlation between different variables is discussed.},
  url      = {https://dx.doi.org/10.1109/ACCESS.2018.2881189},
}

@Article{Wang2018a,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  title    = {{ESC}: an efficient error-based stopping criterion for kriging-based reliability analysis methods},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  pages    = {1--17},
  abstract = {The ever-increasing complexity of numerical models and associated computational demands have challenged classical reliability analysis methods. Surrogate model-based reliability analysis techniques, and in particular those using kriging meta-model, have gained considerable attention recently for their ability to achieve high accuracy and computational efficiency. However, existing stopping criteria, which are used to terminate the training of surrogate models, do not directly relate to the error in estimated failure probabilities. This limitation can lead to high computational demands because of unnecessary calls to costly performance functions (e.g., involving finite element models) or potentially inaccurate estimates of failure probability due to premature termination of the training process. Here, we propose the error-based stopping criterion (ESC) to address these limitations. First, it is shown that the total number of wrong sign estimation of the performance function for candidate design samples by kriging, S, follows a Poisson binomial distribution. This finding is subsequently used to estimate the lower and upper bounds of S for a given confidence level for sets of candidate design samples classified by kriging as safe and unsafe. An upper bound of error of the estimated failure probability is subsequently derived according to the probabilistic properties of Poisson binomial distribution. The proposed upper bound is implemented in the kriging-based reliability analysis method as the stopping criterion. The efficiency and robustness of ESC are investigated here using five benchmark reliability analysis problems. Results indicate that the proposed method achieves the set accuracy target and substantially reduces the computational demand, in some cases by over 50\%.},
  date     = {2018-11-23},
  doi      = {10.1007/s00158-018-2150-9},
  file     = {:Wang2018a - ESC_ an efficient error-based stopping criterion for kriging-based reliability analysis methods.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1007/s00158-018-2150-9},
}

@Article{Groenquist2018,
  author   = {Philippe Grönquist and Falk K. Wittel and Markus Rüggeberg},
  title    = {Modeling and design of thin bending wooden bilayers},
  journal  = {PLoS ONE},
  year     = {2018},
  volume   = {13},
  number   = {10},
  abstract = {In recent architectural research, thin wooden bilayer laminates capable of self-actuation in response to humidity changes have been proposed as sustainable, programmed, and fully autonomous elements for facades or roofs for shading and climate regulation. Switches, humidistats, or motor elements represent further promising applications. Proper wood-adapted prediction models for actuation, however, are still missing. Here, a simple model that can predict bending deformation as a function of moisture content change, wood material parameters, and geometry is presented. We consider material anisotropy and moisture-dependency of elastic mechanical parameters. The model is validated using experimental data collected on bilayers made out of European beech wood. Furthermore, we present essential design aspects in view of facilitated industrial applications. Layer thickness, thickness-ratio, and growth ring angle of the wood in single layers are assessed by their effect on curvature, stored elastic energy, and generated axial stress. A sensitivity analysis is conducted to identify primary curvature-impacting model input parameters.},
  date     = {2018-10-16},
  doi      = {10.1371/journal.pone.0205607},
  file     = {:Groenquist2018 - Modeling and design of thin bending wooden bilayers.pdf:PDF},
  keywords = {material science and engineering, used UQLab, mechanical engineering},
  review   = {For the analysis, the Matlab-based uncertainty quantification tool UQLab was used [19]. The engineering constants EL, ER, and ET entering the model (for the plane strain case), the three swelling coefficients αL, αR, and αT, the two thicknesses h1, and h2, and the growth ring orientation φ2, were sampled using 107 MC samples. We assume a log-normal PDF for the material properties (), where a coefficient of variation (COV = σ/μ) of 10% is attributed to each property, and a Gaussian PDF for the geometrical properties () where standard deviations (σ) of 0.1 mm, 0.1 mm and 1° are attributed respectively. Exemplary, four beech bilayer configurations were analyzed (described along Results). The mean values μi (first moments of the input PDFs of i) for the material properties are taken as in Fig 1. The described input parameters were chosen in view of direct applicability in industry. The Young’s moduli, the differential swelling coefficients, and the geometry are deemed easier to record than other input parameters (i.e. shear moduli and Poisson ratios).},
  url      = {https://doi.org/10.1371/journal.pone.0205607},
}

@Article{Shahane2019a,
  author      = {Shantanu Shahane and Narayana Aluru and Placid Ferreira and Shiv G Kapoor and Surya Pratap Vanka},
  title       = {Finite volume simulation framework for die casting with uncertainty quantification},
  journal     = {Applied Mathematical Modeling},
  year        = {2019},
  volume      = {74},
  pages       = {132--150},
  abstract    = {The present paper describes the development of a novel and comprehensive computational framework to simulate solidification problems in materials processing, specifically casting processes. Heat transfer, solidification and fluid flow due to natural convection are modeled. Empirical relations are used to estimate the microstructure parameters and mechanical properties. The fractional step algorithm is modified to deal with the numerical aspects of solidification by suitably altering the coefficients in the discretized equation to simulate selectively only in the liquid and mushy zones. This brings significant computational speed up as the simulation proceeds. Complex domains are represented by unstructured hexahedral elements. The algebraic multigrid method, blended with a Krylov subspace solver is used to accelerate convergence. State of the art uncertainty quantification technique is included in the framework to incorporate the effects of stochastic variations in the input parameters. Rigorous validation is presented using published experimental results of a solidification problem.},
  date        = {2018-10-19},
  doi         = {10.1016/j.apm.2019.04.045},
  eprint      = {1810.08572v1},
  eprintclass = {cs.NA},
  eprinttype  = {arXiv},
  file        = {online:Shahane2018 - Finite Volume Simulation Framework for Die Casting with Uncertainty Quantification.pdf:PDF},
  keywords    = {mechanical engineering, used UQLab},
  review      = {Used UQLab to construct PCE and PCK metamodels and conduct Sobol' senstivity analysis.},
  url         = {https://doi.org/10.1016/j.apm.2019.04.045},
}

@Article{Hart2018,
  author     = {J. L. Hart and P. A. Gremaud and T. David},
  title      = {Global sensitivity analysis of high dimensional neuroscience models: an example of neurovascular coupling},
  journal    = {arXiv},
  year       = {2018},
  abstract   = {The complexity and size of state-of-the-art cell models have significantly increased in part due to the requirement that these models possess complex cellular functions which are thought--but not necessarily proven--to be important. Modern cell models often involve hundreds of parameters; the values of these parameters come, more often than not, from animal experiments whose relationship to the human physiology is weak with very little information on the errors in these measurements. The concomitant uncertainties in parameter values result in uncertainties in the model outputs or Quantities of Interest (QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual parameters (or sets of parameters) their relative contribution to output uncertainty thereby introducing a measure of influence or importance of said parameters. New GSA approaches are required to deal with increased model size and complexity; a three stage methodology consisting of screening (dimension reduction), surrogate modeling, and computing Sobol' indices, is presented. The methodology is used to analyze a physiologically validated numerical model of neurovascular coupling which possess 160 uncertain parameters. The sensitivity analysis investigates three quantities of interest (QoIs), the average value of $K^+$ in the extracellular space, the average volumetric flow rate through the perfusing vessel, and the minimum value of the actin/myosin complex in the smooth muscle cell. GSA provides a measure of the influence of each parameter, for each of the three QoIs, giving insight into areas of possible physiological dysfunction and areas of further investigation.},
  date       = {2018-11-20},
  eprint     = {1811.08498v1},
  eprinttype = {arXiv},
  file       = {online:Hart2018 - Global Sensitivity Analysis of High Dimensional Neuroscience Models_ An Example of Neurovascular Coupling.pdf:PDF},
  keywords   = {biology, used UQLab},
  url        = {https://arxiv.org/abs/1811.08498?context=q-bio},
}

@Article{Wang2018b,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  title    = {{REAK}: {Reliability} analysis through error rate-based adaptive {K}riging},
  journal  = {Reliability Engineering \& System Safety},
  year     = {2018},
  volume   = {182},
  pages    = {33--45},
  note     = {Didn't specifically say about using UQLab but cite two of the UQLab manuals. Date is "available online"},
  abstract = {As models in various fields are becoming more complex, associated computational demands have been increasing significantly. Reliability analysis for these systems when failure probabilities are small is very challenging, requiring a large number of costly simulations. To address this challenge, this paper introduces Reliability analysis through Error rate-based Adaptive Kriging (REAK). An extension of the Central Limit Theorem based on Lindeberg condition is adopted here to derive the distribution of the number of design samples with wrong sign estimate and subsequently determine the maximum error rate for failure probability estimates. This error rate enables optimal establishment of effective sampling regions at each stage of an adaptive scheme for strategic generation of design samples. Moreover, it facilitates setting a target accuracy for failure probability estimation, which is used as the stopping criterion for reliability analysis. These capabilities together can significantly reduce the number of calls to sophisticated, computationally demanding models. The application of REAK for four examples with varying extent of nonlinearity and dimension is presented. Results indicate that REAK is able to reduce the computational demand by as high as 50\% compared to the state-of-the-art methods of Adaptive Kriging with Monte Carlo Simulation (AK-MCS) and Improved Sequential Kriging Reliability Analysis (ISKRA).},
  date     = {2018-10-09},
  doi      = {10.1016/j.ress.2018.10.004},
  file     = {:Wang2018b - REAK_ Reliability analysis through error rate-based adaptive Kriging.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.ress.2018.10.004},
}

@Article{Sheng2018,
  author   = {Hao Sheng and Xiaozhe Wang},
  title    = {A non-intrusive low-rank approximation method for assessing the probabilistic available transfer capability},
  journal  = {IEEE Transactions on Smart Grid},
  year     = {2018},
  volume   = {In Review},
  abstract = {In this paper, a mathematical formulation of the probabilistic available transfer capability (PATC) problem is proposed to incorporate uncertainties from the large-scale renewable energy generation (e.g., wind farms and solar PV power plants). Moreover, a novel non-intrusive low-rank approximation (LRA) is developed to assess PATC, which can accurately and efficiently estimate the probabilistic characteristics (e.g., mean, variance, probability density function (PDF)) of the PATC. Numerical studies on the IEEE 24-bus reliability test system (RTS) and IEEE 118-bus system show that the proposed method can achieve accurate estimations for the probabilistic characteristics of the PATC with much less computational effort compared to the Latin hypercube sampling (LHS)-based Monte Carlo simulations (MCS). The proposed LRA-PATC method offers an efficient and effective way to determine the available transfer capability so as to fully utilize the transmission assets while maintaining the security of the grid.},
  date     = {2018-10-18},
  file     = {:Sheng2018 - A non-intrusive low-rank approximation method for assessing the probabilistic available transfer capability.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  url      = {https://arxiv.org/abs/1810.08156},
}

@Article{Emenike2018,
  author   = {Victor N. Emenike and Xiangzhong Xie and René Schenkendorf and Antje C. Spiess and Ulrike Krewer},
  title    = {Robust dynamic optimization of enzyme-catalized carboligation: a point estimate-based back-off approach},
  journal  = {Computers and Chemical Engineering},
  year     = {2018},
  volume   = {121},
  pages    = {232--247},
  abstract = {In this paper, we present a systematic robust dynamic optimization framework applied to the benzaldehyde lyase-catalyzed carboligation of propanal and benzaldehyde to produce (R)-2-hydroxy-1-phenylbutan-1-one (BA). First, the elementary process functions approach was used to screen between different dosing concepts, and it was found that simultaneously dosing propanal and benzaldehyde leads to the highest final concentration of BA. Next, we applied global sensitivity analysis and found that 10 out of 13 kinetic parameters are relevant. Time-varying back-offs were then used to handle parametric uncertainties due to these 10 parameters. A major contribution in our work is the use of the point estimate method instead of Monte Carlo simulations to calculate the back-offs in an efficient and reproducible manner. We show that this new approach is at least 10 times faster than the conventional Monte Carlo approach while achieving low approximation errors.},
  date     = {2018-10-25},
  doi      = {10.1016/j.compchemeng.2018.10.006},
  file     = {:Emenike2018 - Robust dynamic optimization of enzyme-catalized carboligation_ a point estimate-based back-off approach.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.compchemeng.2018.10.006},
}

@Article{Dimitrov2018,
  author   = {Nikolay Dimitrov and Mark C. Kelly and Andrea Vignaroli and Jacob Berg},
  title    = {From wind to loads: wind turbine site-specific load estimation with surrogate models trained on high-fidelity load databases},
  journal  = {Wind Energy Science},
  year     = {2018},
  volume   = {3},
  pages    = {767--790},
  abstract = {We define and demonstrate a procedure for quick assessment of site-specific lifetime fatigue loads using simplified load mapping functions (surrogate models), trained by means of a database with high-fidelity load simulations. The performance of five surrogate models is assessed by comparing site-specific lifetime fatigue load predictions at 10 sites using an aeroelastic model of the DTU 10 MW reference wind turbine. The surrogate methods are polynomial chaos expansion, quadratic response surface, universal Kriging, importance sampling, and nearest-neighbor interpolation. Practical bounds for the database and calibration are defined via nine environmental variables, and their relative effects on the fatigue loads are evaluated by means of Sobol sensitivity indices. Of the surrogate-model methods, polynomial chaos expansion provides an accurate and robust performance in prediction of the different site-specific loads. Although the Kriging approach showed slightly better accuracy, it also demanded more computational resources.},
  date     = {2018-10-24},
  doi      = {10.5194/wes-3-767-2018},
  file     = {:Dimitrov2018 - From wind to loads_ wind turbine site-specific load estimation with surrogate models trained on high-fidelity load databases.pdf:PDF},
  keywords = {mechanical engineering, used UQLab},
  url      = {https://doi.org/10.5194/wes-3-767-2018},
}

@Article{Shahane2019,
  author     = {Shantanu Shahane and Narayana R. Aluru and Surya Pratap Vanka},
  title      = {Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks},
  journal    = {International Jounral of Heat and Mass Transfer},
  year       = {2019},
  volume     = {139},
  pages      = {613--631},
  abstract   = {This paper analyzes the effects of input uncertainties on the outputs of a three dimensional natural convection problem in a differentially heated cubical enclosure. Two different cases are considered for parameter uncertainty propagation and global sensitivity analysis. In case A, stochastic variation is introduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers) with an assumption that the temperature boundary condition is uniform. Being a two dimensional stochastic problem, the polynomial chaos expansion (PCE) method is used as a surrogate model. Case B deals with non-uniform stochasticity in the boundary temperature. Instead of the traditional Gaussian process model with the Karhunen-Lo$\grave{e}$ve expansion, a novel approach is successfully implemented to model uncertainty in the boundary condition. The boundary is divided into multiple domains and the temperature imposed on each domain is assumed to be an independent and identically distributed (i.i.d) random variable. Deep neural networks are trained with the boundary temperatures as inputs and Nusselt number, internal temperature or velocities as outputs. The number of domains which is essentially the stochastic dimension is 4, 8, 16 or 32. Rigorous training and testing process shows that the neural network is able to approximate the outputs to a reasonable accuracy. For a high stochastic dimension like 32, it is computationally expensive to fit the PCE. This paper demonstrates a novel way of using the deep neural network as a surrogate modeling method of uncertainty quantification with the number simulations much lesser than that required for fitting the PCE thus, saving the computational cost.},
  date       = {2018-10-29},
  doi        = {10.1016/j.ijheatmasstransfer.2019.05.014},
  eprinttype = {arXiv},
  file       = {:Shahane2018a - Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks.pdf:PDF},
  keywords   = {mechanical engineering, used UQLab},
  review     = {Used UQLab to construct PCE and PCK metamodels and conduct Sobol' senstivity analysis.},
  url        = {https://doi.org/10.1016/j.ijheatmasstransfer.2019.05.014},
}

@Article{Vepsaelaeinen2019,
  author   = {Jari Vepsäläinen and Kevin Otto and Antti Lajunena and Kari Tammia},
  title    = {Computationally efficient model for energy demand prediction of electric city bus in varying operating conditions},
  journal  = {Energy},
  year     = {2019},
  volume   = {169},
  abstract = {The uncertainty of operating conditions such as weather and payload cause variations in the energy demand of electric city buses. Uncertain variation in energy demand is a challenge in the design of charging systems and on-board energy storages. To predict the energy demand, a computationally efficient model is required for real-time applications. We present a novel approach to predict energy demand variation with a wide range of uncertain factors. A factor identification is carried out to recognize the range of variation in the operating conditions. A computationally efficient surrogate model is generated based on a previously developed numerical simulation model. The surrogate model is shown to be 10 000 times faster than the numerical model. The surrogate model output corresponds with the numerical model with less than 1\% error. The energy demand of the surrogate model varied from 0.43 to 2.30 kWh/km, which is realistic in comparison to previous studies. Successful sensitivity analysis of the surrogate model revealed the most crucial factors. Uncertainty in temperature, rolling resistance and payload contributed most to the variation in energy demand. Variation in these factors should be taken into account when predicting energy consumption and while planning schedules for a bus network.},
  date     = {2019-02-15},
  doi      = {10.1016/j.energy.2018.12.064},
  file     = {:Vepsaelaeinen2019 - Computationally efficient model for energy demand prediction of electric city bus in varying operating conditions.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.energy.2018.12.064},
}

@Article{Cheng2019,
  author   = {Kai Cheng and Zhenzhou Lu and Kaichao Zhang},
  title    = {Multivariate output global sensitivity analysis using multi-output support vector regression},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2019},
  abstract = {Models with multivariate outputs are widely used for risk assessment and decision-making in practical applications. In this paper, multi-output support vector regression (M-SVR) is employed for global sensitivity analysis (GSA) with multivariate output models. The orthogonal polynomial kernel is used to build the M-SVR meta-model, and the covariance-based sensitivity indices of multivariate output are obtained analytically by post-processing the coefficients of M-SVR model. In order to improve the performance of the orthogonal polynomial kernel M-SVR model, a kernel function iteration algorithm is introduced further. The proposed method take advantage of the information of all outputs to get robust meta-model. To validate the performance of the proposed method, two high-dimensional analytical functions and a hydrological model (HYMOD) with multiple outputs are examined, and a detailed comparison is made with the sparse polynomial chaos expansion meta-model developed in UQLab Toolbox. Results show that the proposed methods are efficient and accurate for GSA of the complex multivariate output models.},
  doi      = {10.1007/s00158-018-2184-z},
  file     = {:Cheng2018a - Multivariate output global sensitivity analysis using multi-output support vector regression.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used UQLab to compare the new proposed method with sparse PCE for GSA.},
  url      = {https://doi.org/10.1007/s00158-018-2184-z},
}

@Article{Xu2019,
  author   = {Jun Xu and Shengzang Zhu},
  title    = {An efficient approach for high-dimensional structural reliability analysis},
  journal  = {Mechanical Systems and Signal Processing},
  year     = {2019},
  volume   = {122},
  pages    = {151--170},
  abstract = {This paper presents a new method to address the challenge of high-dimensional reliability analysis based on a small number of samples. The method is established based on the maximum entropy method (MEM) with the low-order fractional moments as constraints. A coordinate transformation is first implemented since a positive random variable is required for fractional operations. Then, an estimator-corrector scheme is employed to obtain the
probability density function (PDF) of the performance function. This scheme first promptly provides an estimated fractional orders and Lagrange multipliers by solving a linear system of equations as initial values, and then searches the more accurate solutions around the initial values to recover the PDF. Besides, the local optimum can be avoided by using the estimator-corrector scheme. The centered L2 (CL2) discrepancy oriented sequential Latin-hyper cube simulation is proposed to evaluate the low-order fractional moments involved in MEM, which is of critical importance to the efficiency and accuracy for high dimensional reliability analysis. Typical numerical examples are investigated to validate the proposed method. The results show that the proposed method is of efficacy for highdimensional reliability problems, even with very low failure probabilities.},
  doi      = {10.1016/j.ymssp.2018.12.007},
  file     = {:Xu2019 - An efficient approach for high-dimensional structural reliability analysis.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used UQLab for benchmark},
  url      = {https://doi.org/10.1016/j.ymssp.2018.12.007},
}

@Article{Blomfors2019,
  author   = {Mattias Blomfors and Oskar Larsson Ivanov and Dániel Honfí and Morten Engen},
  title    = {Partial safety factors for the anchorage capacity of corroded reinforcement bars in concrete},
  journal  = {Engineering Structures},
  year     = {2019},
  volume   = {181},
  pages    = {579--588},
  abstract = {Many reinforced concrete bridges in Europe and around the world are damaged by reinforcement corrosion and the annual maintenance costs are enormous. It is therefore important to develop reliable methods to assess the structural capacity of corroded reinforced concrete structures and avoid unnecessary maintenance costs. Although there are advanced models for determining the load carrying capacity of structures, it is not obvious how they should be used to verify the performance of existing structures. To confidently assess the bond of corroded reinforcement in concrete, for example, the calculation model must give a sufficient safety margin. When designing new structures, semi-probabilistic approaches (such as the partial safety factor method) are adopted to achieve the target reliabilities specified in structural design codes. This paper uses probabilistic methods to develop partial factors for application in an existing bond model, to assess the safety of corroded reinforced concrete structures. The response of the bond model was studied using Monte Carlo (MC) simulations for several design cases, with probability distributions fitted to the results. Partial factors were then derived, based on these distributions. Furthermore, an MC-based simulation technique called “importance sampling” was used to study the reliability of several deterministic bond assessments conducted using these partial factors. The results show that deterministic assessments which use the proposed partial factors lead to a safety level at least equal to the target value. The results presented in this paper will support the assessment of reinforced concrete structures with anchorage problems and give a reasonable approximation of the anchorage capacity with sufficient safety margin. When generalised to cover other failure modes and structural configurations, this will enable better utilisation of damaged structures and lead to major environmental and economical savings for society.},
  doi      = {10.1016/j.engstruct.2018.12.011},
  file     = {:Blomfors2019 - Partial safety factors for the anchorage capacity of corroded reinforcement bars in concrete.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://doi.org/10.1016/j.engstruct.2018.12.011},
}

@Article{Rajabi2019,
  author   = {Mohammad Mahdi Rajabi},
  title    = {Review and comparison of two meta-model-based uncertainty propagation analysis methods in groundwater applications: polynomial chaos expansion and {Gaussian} process emulation},
  journal  = {Stochastic Environmental Research and Risk Assessment},
  year     = {2019},
  volume   = {33},
  pages    = {607--631},
  abstract = {Gaussian process emulation (GPE) and polynomial chaos expansion (PCE) are tools for meta-model-based uncertainty propagation analysis (UPA) that have gained increasing attention in recent groundwater literature. Previous studies have shown that these two meta-models can provide satisfactorily accurate estimations of the model response in a wide range of groundwater UPA problems. However, PCE and GPE are based on very different mathematical concepts, and a question that arises is which one of these is more suitable for groundwater UPA. The current paper aims to provide an answer to this question by first presenting a theoretical comparison of the two meta-models, then reviewing previous comparisons of the two in other fields of engineering, and subsequently presenting an empirical comparison based on groundwater case studies. For this purpose, both meta-models are applied to two hypothetical test cases corresponding to seawater intrusion in coastal aquifers. The results show that: (1) GPE outperforms PCE in the estimation of the input–output relationships, by providing smaller root mean square errors. (2) In most cases assessed, PCE provides better accuracy in the estimation of
mean, standard deviation and the entire shape and the tail of probability distribution functions. (3) Replicates of PCE show less statistical dispersion in the estimation of mean and standard deviations. (4) A trend of increase in the predominance of PCE over GPE can be identified as the probability distributions that are meant to be estimated become noisier and more multi-modal.},
  doi      = {10.1007/s00477-018-1637-7},
  file     = {:Rajabi2019 - Review and comparison of two meta-model-based uncertainty propagation analysis methods in groundwater applications_ polynomial chaos expansion and Gaussian process emulation.pdf:PDF},
  keywords = {environmental engineering},
  url      = {https://doi.org/10.1007/s00477-018-1637-7},
}

@Article{Cheng2019a,
  author   = {Xu Cheng and Guoyuan Li and Robert Skulstad and Shengyong Chen and Hans Petter Hildre and Houxiang Zhang},
  title    = {A {Neural-Network-based} sensitivity analysis approach for data-driven modeling of ship motion},
  journal  = {IEEE Journal of Oceanic Engineering},
  year     = {2019},
  abstract = {Researchers have been investigating data-driven modeling as a key way to achieve ship intelligence for years. This paper presents a novel data analysis approach to data-driven modeling of ship motion. We propose a global sensitivity analysis (GSA) approach combining artificial neural network (ANN) and sparse polynomial chaos expansion (SPCE) techniques to accommodate high-dimensional sensor data collected from ship motion. An ANN is constructed as a surrogate model to associate ship sensor data with a certain type of ship motion. To account for the computational efficiency of GSA, an SPCE is integrated into the GSA to decrease the need for Monte Carlo (MC) samples generated by the ANN. A probe variable is designed to couple with the MC samples, which plays a role in determining the degree of convergence of variable importance. A test on benchmark function demonstrates the efficiency and accuracy of the proposed approach. A case study of ship heading with and without environment effects is conducted. The experimental results show that the proposed approach can identify and rank the most sensitive factors of ship motion. The proposed approach highlights the application of GSA in data-driven modeling for ship intelligence.},
  doi      = {10.1109/JOE.2018.2882276},
  file     = {:Cheng2019a - A Neural-Network-based sensitivity analysis approach for data-driven modeling of ship motion.pdf:PDF},
  keywords = {used UQLab, ocean engineering},
  review   = {Used UQLab to construct SPCE in combination with ANN for high-dimensional sensor data collected from ship motion.},
  url      = {http://dx.doi.org/10.1109/JOE.2018.2882276},
}

@Article{Sheng2019,
  author   = {Hao Sheng and Xiaozhe Wang},
  title    = {Probabilistic power flow using non-intrusive low-rank approximation method},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2019},
  abstract = {In this paper, a novel non-intrusive probabilistic power flow (PPF) analysis method based on the low-rank approximation (LRA) is proposed, which can accurately and efficiently estimate the probabilistic characteristics (e.g., mean, variance, probability density function) of the PPF solutions. This method aims at building up a statistically-equivalent surrogate for the PPF solutions through a small number of power flow evaluations. By exploiting the retained tensor-product form of the univariate polynomial basis, a sequential correction-updating scheme is applied, making the total number of unknowns to be linear rather than exponential to the number of random inputs. Consequently, the LRA method is particularly promising for dealing with high-dimensional problems with a large number of random inputs. Numerical studies on the IEEE 39-bus, 118-bus, and 1354-bus systems show that the proposed method can achieve accurate probabilistic characteristics of the PPF solutions with much less computational effort compared to the Monte Carlo simulations. Even compared to the polynomial chaos expansion method, the LRA method can achieve comparable accuracy, while the LRA method is more capable of handling higher-dimensional problems. Moreover, numerical results reveal that the randomness brought about by the renewable energy resources and loads may inevitably affect the feasibility of dispatch/planning schemes.},
  doi      = {10.1109/TPWRS.2019.2896219},
  file     = {:Sheng2019 - Probabilistic power flow using non-intrusive low-rank approximation method.pdf:PDF},
  keywords = {used UQLab, electrical engineering},
  url      = {https://doi.org/10.1109/TPWRS.2019.2896219},
}

@Article{Koohbor2019,
  author   = {Behshad Koohbor and Marwan Fahs and Behzad Ataie-Ashtiani and Benjamin Belfort and Craig T.Simmons and Anis Younes},
  title    = {Uncertainty analysis for seawater intrusion in fractured coastal aquifers: {Effects} of fracture location, aperture, density and hydrodynamic parameters},
  journal  = {Journal of Hydrology},
  year     = {2019},
  volume   = {571},
  pages    = {159--177},
  abstract = {In this study we use polynomial chaos expansion (PCE) to perform uncertainty analysis for seawater intrusion (SWI) in fractured coastal aquifers (FCAs) which is simulated using the coupled discrete fracture network (DFN) and variable-density flow (VDF) models. The DFN-VDF model requires detailed discontinuous analysis of the fractures. In real field applications, these characteristics are usually uncertain which may have a major effect on the predictive capability of the model. Thus, we perform global sensitivity analysis (GSA) to provide a preliminary assessment on how these uncertainties can affect the model outputs. As our conceptual model, we consider fractured configurations of the Henry Problem which is widely used to understand SWI processes. A finite element DFN-VDF model is developed in the framework of COMSOL Multiphysics. We examine the uncertainty of several SWI metrics and salinity distribution due to the incomplete knowledge of fracture characteristics. PCE is used as a surrogate model to reduce the computational burden. A new sparse PCE technique is used to allow for high polynomial orders at low computational cost. The Sobol’ indices (SIs) are used as sensitivity measures to identify the key variables driving the model outputs uncertainties. The proposed GSA methodology based on PCE and SIs is useful for identifying the source of uncertainties on the model outputs with an affordable computational cost and an acceptable accuracy. It shows that fracture hydraulic conductivity is the first source of uncertainty on the salinity distribution. The imperfect knowledge of fracture location and density affects mainly the toe position and the total flux of saltwater entering the aquifer. Marginal effects based on the PCE are used to understand the effects of fracture characteristics on SWI. The findings provide a technical support for monitoring, controlling and preventing SWI in FCAs.},
  doi      = {10.1016/j.jhydrol.2019.01.052},
  file     = {:Koohbor2019 - Uncertainty analysis for seawater intrusion in fractured coastal aquifers_ Effects of fracture location, aperture, density and hydrodynamic parameters.pdf:PDF},
  keywords = {used UQLab, hydrology},
  review   = {Used UQLab to compute total Sobol' indices with PCE.},
  url      = {https://doi.org/10.1016/j.jhydrol.2019.01.052},
}

@Article{Xie2019,
  author   = {Xiangzhong Xie and René Schenkendorf},
  title    = {Stochastic back-off-based robust process design for continuous crystallization of ibuprofen},
  journal  = {Computer \& Chemical Engineering},
  year     = {2019},
  volume   = {124},
  pages    = {80--92},
  abstract = {Robust model-based process design in continuous pharmaceutical manufacturing aims to implement quality by design principles under uncertainty. Notably, various studies have discussed the back-off concept to solve the underlying robust optimization problem; however, for the concept to have practical value, its efficiency and convergence must be improved. In this work, we introduce a novel, highly efficient stochastic back-off strategy. Instead of using statistical moments of limited validity, we incorporate the full statistical information of the constraints to solve the robust process design problem. To ensure manageable computational costs, we make use of polynomial chaos expansion for uncertainty quantification and propagation. The proposed concept is demonstrated with the design of a tubular crystallizer for ibuprofen crystallization. The results show that the novel stochastic back-off strategy is considerably faster compared with the standard back-off concept and provides more reliable quality by design results in general.},
  doi      = {10.1016/j.compchemeng.2019.02.009},
  file     = {:Xie2019 - Stochastic back-off-based robust process design for continuous crystallization of ibuprofen.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  review   = {Used UQLab to construct PCE.},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.02.009},
}

@Article{Dalemans2019,
  author   = {Floris Dalemans and Bart Muys and Miet Maertens},
  title    = {A framework for profitability evaluation of agroforestry‐based biofuel value chains: {An} application to pongamia in {India}},
  journal  = {GCB Bioenergy},
  year     = {2019},
  pages    = {1--19},
  abstract = {Biofuel production from oilseed trees in small‐scale agroforestry systems is considered as a strategy for energy security, rural development and ecosystem services provision in low‐income countries. However, the economic potential of these systems remains unclear, as profitability studies commonly ignore key methodological issues such as quantitative uncertainty analysis, full accounting for opportunity costs, and inclusion of all value chain actors. This study addresses these methodological shortcomings and develops a framework for quantifying the long‐term financial performance of agroforestry‐based biofuel value chains. The framework is applied to a case in South India, to calculate profitability of pongamia (Millettia pinnata) cultivation and processing. The results show that pongamia cultivation has limited financial potential, and is only profitable in small‐scale settings, in the middle to long term and for a subset of farmers. If biodiesel is envisaged as the end product, the value chain requires substantial fiscal and marketing support to be economically viable. For current prices, financial performance is much higher if the seed oil is marketed instead of processed to biodiesel. Increased mechanization, increased yields and optimized agroforestry set‐ups might improve financial outcomes and reduce risks for both farmers and processors. These findings are case‐specific, while the developed framework opens the door to comprehensive investigation of the financial performance of other oilseed tree species and in other regions.},
  doi      = {10.1111/gcbb.12605},
  file     = {:Dalemans2019 - A framework for profitability evaluation of agroforestry‐based biofuel value chains_ An application to pongamia in India.pdf:PDF},
  keywords = {used UQLab, energy engineering},
  review   = {Used UQLab for Monte Carlo simulation.},
  url      = {https://dx.doi.org/10.1111/gcbb.12605},
}

@Article{Papaioannou2019,
  author   = {Iason Papaioannou and Max Ehre and Daniel Straub},
  title    = {{PLS}-based adaptation for efficient {PCE} representation in high dimensions},
  journal  = {Journal of Computational Physics},
  year     = {2019},
  volume   = {387},
  pages    = {186--204},
  abstract = {Uncertainty quantification of engineering systems modeled by computationally intensive numerical models remains a challenging task, despite the increase in computer power. Efficient uncertainty propagation of such models can be performed by use of surrogate models, such as polynomial chaos expansions (PCE). A major drawback of standard PCE is that its predictive ability decreases with increase of the problem dimension for a fixed computational budget. This is related to the fact that the number of terms in the expansion increases fast with the input variable dimension. To address this issue, Tipireddy and Ghanem (2014) introduced a sparse PCE representation based on a transformation of the coordinate system in Gaussian input variable spaces. In this contribution, we propose to identify the projection operator underlying this transformation and approximate the coefficients of the resulting PCE through partial least squares (PLS) analysis. The proposed PCE-driven PLS algorithm identifies the directions with the largest predictive significance in the PCE representation based on a set of samples from the input random variables and corresponding response variable. This approach does not require gradient evaluations, which makes it efficient for high dimensional problems with black-box numerical models. We assess the proposed approach with three numerical examples in high-dimensional input spaces, comparing its performance with low-rank tensor approximations. These examples demonstrate that the PLS-based PCE method provides accurate representations even for strongly non-linear problems.},
  doi      = {10.1016/j.jcp.2019.02.046},
  file     = {:Papaioannou2019 - PLS-based adaptation for efficient PCE representation in high dimensions.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used LRA in UQLab to compare results with the proposed method of the paper.},
  url      = {https://doi.org/10.1016/j.jcp.2019.02.046},
}

@Article{Jones2019,
  author   = {Mark Nicholas Jones and Jérôme Frutiger and Nevin Gerek Ince and Gürkan Sin},
  title    = {The {Monte Carlo} driven and machine learning enhanced process simulator},
  journal  = {Computers \& Chemical Engineering},
  year     = {2019},
  volume   = {125},
  pages    = {324--338},
  abstract = {This study presents a methodology with tools integration to apply advanced uncertainty propagation and sensitivity analysis in connection with commercial process simulation software. The methodology was applied to two processes: a heat pump system and a molecular distillation process. The input parameters of the selected thermodynamic model, namely critical temperature, critical pressure and acentric factor, were considered as a source of uncertainty and analysed using Monte Carlo sampling techniques. This enabled the process model output uncertainty to be described as an empirical distribution function with a 95\% confidence interval. Variance-based decomposition such as the Sobol method or standard regression were used to analyse the sensitivity of the respective properties. We also show that machine learning methods such as polynomial chaos expansion (PCE) can be applied to reduce the number of necessary process simulations and obtained equivalent results in comparison with the more costly full Monte Carlo based procedure.},
  doi      = {10.1016/j.compchemeng.2019.03.016},
  file     = {:Jones2019 - The Monte Carlo driven and machine learning enhanced process simulator.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  review   = {Used UQLab to construct sparse PCE with emphasis on the adaptive algorithm.},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.03.016},
}

@Article{Zhou2019,
  author   = {Yicheng Zhou and Zhenzhou Lu and Kai Cheng and Yan Shi},
  title    = {An expanded sparse {Bayesian} learning method for polynomial chaos expansion},
  journal  = {Mechanical Systems and Signal Processing},
  year     = {2019},
  volume   = {128},
  pages    = {153--171},
  abstract = {Polynomial chaos expansion (PCE) has been proven to be a powerful tool for developing surrogate models in various engineering fields for uncertainty quantification. The computational cost of full PCE is unaffordable due to the “curse of dimensionality” of the expansion coefficients. In this paper, an expanded sparse Bayesian learning method for sparse PCE is proposed. Firstly, basis polynomials of the full PCE are partitioned into significant terms and complementary non-significant terms. The parameterized priors with distinct variance are assigned to the candidates for the significant terms. Then, the dimensionality of the parameter space is equivalent to the assumed sparsity level of the PCE. Secondly, an approximate Kashyap information criterion (KIC) rule which achieves a balance between model simplicity and goodness of fit is derived for model selection. Finally, an automatic search algorithm is proposed by minimizing the KIC objective function and using the variance contribution of each term to the model output to select significant terms. To assess the performance of the proposed method, a detailed comparison is completed with several well-established techniques. The results show that the proposed method is able to identify the most significant PC contributions with superior efficiency and accuracy.},
  doi      = {10.1016/j.ymssp.2019.03.032},
  file     = {:Zhou2019 - An expanded sparse Bayesian learning method for polynomial chaos expansion.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used LAR PCE of UQLab to benchmark the proposed method of the paper.},
  url      = {https://doi.org/10.1016/j.ymssp.2019.03.032},
}

@Article{Tan2019,
  author   = {Fengjie Tan and Tom Lahmer},
  title    = {Shape design of arch dams under load uncertainties with robust optimization},
  journal  = {Frontiers of Structural and Civil Engineering},
  year     = {2019},
  abstract = {Due to an increased need in hydro-electricity, water storage, and flood protection, it is assumed that a series of new dams will be build throughout the world. The focus of this paper is on the non-probabilistic-based design of new arch-type dams by applying means of robust design optimization (RDO). This type of optimization takes into account uncertainties in the loads and in the material properties of the structure. As classical procedures of probabilistic-based optimization under uncertainties, such as RDO and reliability-based design optimization (RBDO), are in general computationally expensive and rely on estimates of the system’s response variance, we will not follow a full-probabilistic approach but work with predefined confidence levels. This leads to a bi-level optimization program where the volume of the dam is optimized under the worst combination of the uncertain parameters. As a result, robust and reliable designs are obtained and the result is independent from any assumptions on stochastic properties of the random variables in the model. The optimization of an arch-type dam is realized here by a robust optimization method under load uncertainty, where hydraulic and thermal loads are considered. The load uncertainty is modeled as an ellipsoidal expression. Comparing with any traditional deterministic optimization method, which only concerns the minimum objective value and offers a solution candidate close to limit-states, the RDO method provides a robust solution against uncertainty. To reduce the computational cost, a ranking strategy and an approximation model are further involved to do a preliminary screening. By this means, the robust design can generate an improved arch dam structure that ensures both safety and serviceability during its lifetime.},
  doi      = {10.1007/s11709-019-0522-x},
  file     = {:Tan2019 - Shape design of arch dams under load uncertainties with robust optimization.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  review   = {Used Kriging tool of UQLab to construct a metamodel for predicting tensile stress and dam volume.},
  url      = {https://doi.org/10.1007/s11709-019-0522-x},
}

@Article{Loukrezis2019,
  author   = {Dimitrios Loukrezis and Herbert De Gersem},
  title    = {Approximation and uncertainty quantification of stochastic systems with arbitrary input distributions using weighted {Leja} interpolation},
  journal  = {arXiv},
  year     = {2019},
  abstract = {Approximation and uncertainty quantification methods based on Lagrange interpolation are typically abandoned in cases where the probability distributions of a stochastic system's input parameters are not normal, uniform, or closely related ones, due to the lack of suitable interpolation nodes. This paper suggests the use of weighted Leja node sequences as a remedy to this situation. We present the recently introduced weighted Leja interpolation rules, along with a dimension-adaptive sparse interpolation algorithm, to be employed in the case of high-dimensional input uncertainty. The performance and reliability of the suggested approach is verified by the results of three numerical experiments, where the respective models feature extreme value and truncated normal input distributions. The suggested approach is also compared against a well-established polynomial chaos method and found to be either comparable or superior in terms of approximation and statistical moment estimation accuracy.},
  file     = {:Loukrezis2019 - Approximation and uncertainty quantification of stochastic systems with arbitrary input distributions using weighted Leja Interpolation.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used sparse PCE based on degree-adaptive algorithm (LAR) of UQLab to benchmark the proposed method of the paper.},
  url      = {https://arxiv.org/abs/1904.07709},
}

@InProceedings{Borowiec2017,
  author       = {K. Borowiec and C. Wang and Tomasz Kozlowski and C. S. Brooks},
  title        = {Uncertainty quantification for steady-state {PSBT} benchmark using surrogate models},
  booktitle    = {Transactions of the American Nuclear Society},
  year         = {2017},
  volume       = {117},
  address      = {Washington, D.C., USA},
  organization = {American Nuclear Society},
  eventdate    = {2017-10-29/2017-11-02},
  file         = {:Borowiec2017 - Uncertainty quantification for steady-state PSBT benchmark using surrogate models.pdf:PDF},
  keywords     = {Used UQLab, nuclear engineering},
  review       = {Used UQLab to create PCK metamodel.},
  url          = {https://www.scopus.com/record/display.uri?eid=2-s2.0-85062036963&origin=resultslist},
}

@InProceedings{Wang2017,
  author    = {Chen Wang and Xu Wu and Tomasz Kozlowski},
  title     = {Surrogate-based inverse uncertainty quantification of {TRACE} physical model parameters using steady-state {PSBT} void fraction data},
  booktitle = {17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH) 2017},
  year      = {2017},
  address   = {Xi'an, Shaanxi, China},
  abstract  = {In the framework of BEPU (Best Estimate Plus Uncertainty) methodology, the uncertainties involved in simulations must be quantified to prove that the investigated design is reasonable and acceptable. The uncertainties in predictions are usually calculated by propagating input uncertainties through the simulation model, which requires prior knowledge of the model or code input uncertainties, for example, the means, variances, distribution types, etc. However, in best-estimate system thermal-hydraulics codes such as TRACE, some parameters in empirical correlations may have large uncertainties which are unknown to code users. So, the uncertainties associated these parameters are simply ignored or described by “expert opinion”. Inverse Uncertainty Quantification (UQ) is performed in the current study to replace such ad-hoc expert judgment. Inverse UQ is the process of quantifying the uncertainties in input parameters given relevant experimental measurements. The purpose of inverse UQ, and this paper, is to seek statistical descriptions of the input model parameters that are consistent with the observed data. Bayesian analysis is used to formulate the inverse UQ problem given relevant experiment data. In this study, the steady-state PSBT benchmark void fraction data is used. Within the Bayesian framework we seek the posterior distributions of the uncertain TRACE modeling parameters, which is updated from our prior knowledge given measurement data. Markov Chain Monte Carlo (MCMC) method is used to explore the posterior distributions, and surrogate models of TRACE are used to alleviate the computational burden. Gaussian Process (GP) is used to construct the surrogate model which can reduce the simulation time significantly. The outcomes will be the posterior distributions of several modeling parameters that are significant to PSBT experiment. Results of inverse UQ can be used for future forward uncertainty propagation and validation analysis, which will be presented in a companion paper.},
  eventdate = {2017-09-3/2017-09-08},
  file      = {:Wang2017 - Surrogate-based inverse uncertainty quantification of TRACE physical model parameters using steady-state PSBT void fraction data.pdf:PDF},
  keywords  = {used UQLab, nuclear engineering},
  review    = {Used UQLab for sensitivity analysis (Sobol')},
  url       = {https://www.researchgate.net/publication/321974326_Surrogate-based_Inverse_Uncertainty_Quantification_of_TRACE_Physical_Model_Parameters_using_Steady-State_PSBT_Void_Fraction_Data},
}

@InProceedings{Wang2017a,
  author    = {Chen Wang and Xu Wu and Tomasz Kozlowski},
  title     = {Sensitivity and uncertainty analysis of {TRACE} physical model parameters based on {PSBT} benchmark using {Gaussian} process emulator},
  booktitle = {17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH) 2017},
  year      = {2017},
  address   = {Xi'an, Shaanxi, China},
  abstract  = {For best estimate thermal-hydraulics codes like TRACE and RELAP5, one major source of uncertainties is the inaccuracy of closure laws (correlations) which are used to describe transfer terms in balance equations. These closure laws were originally studied in separate-effect experiments but are implemented in thermal-hydraulics codes and used for different conditions. Thus, the model parameters involved in such closure laws may have significant influences on model outputs but are subject to considerable uncertainties. The aim of this paper is to perform sensitivity and uncertainty study with respect to selected physical model parameters in TRACE. The PSBT steady-state void fraction data is used as Quantity-of-Interest (QoI). Statistics of these physical model parameters are obtained by inverse Uncertainty Quantification (UQ) from a companion paper. Sensitivity Analysis (SA) aims at determining how uncertain input parameters contribute to the variations of outputs. In this paper, the Sobol' indices are used as a sensitivity measure, which quantify how the variation in a QoI can be apportioned to each input factor. Based on the results from SA, several input parameters that have significant effects on the QoI are selected for the UQ process. The prediction uncertainty is evaluated by considering the propagation of uncertainties in selected TRACE physical model parameters. As thousands of TRACE runs are needed for the sensitivity and uncertainty studies, the use of direct Monte Carlo sampling is not a viable solution. We developed Gaussian Process (GP) emulator as a surrogate model of TRACE to reduce the computation time substantially.},
  eventdate = {2017-09-3/2017-09-08},
  file      = {:Wang2017a - Sensitivity and uncertainty analysis of TRACE physical model parameters based on PSBT benchmark using Gaussian process emulator.pdf:PDF},
  keywords  = {used UQLab, nuclear engineering},
  review    = {Used UQLab for Sobol' sensitivity indices, PCE and GP metamodels.},
  url       = {https://www.researchgate.net/publication/321974324_Sensitivity_and_Uncertainty_Analysis_of_TRACE_Physical_Model_Parameters_based_on_PSBT_benchmark_using_Gaussian_Process_Emulator},
}

@InProceedings{Lagouanelle2019,
  author       = {Paul Lagouanelle and Vang-Lang Krauth and Lionel Pichon},
  title        = {Uncertainty Quantification in the Assessment of Human Exposure near Wireless Power Transfer Systems in Automotive Applications},
  booktitle    = {2019 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE)},
  year         = {2019},
  pages        = {1--5},
  address      = {Torino, Italy},
  organization = {IEEE},
  abstract     = {This paper addresses the uncertainty quantification of  physical  and  geometrical  material  parameters  in  the  design  of wireless power transfer systems and in the assessment of the level  of  exposure  for  automotive  applications.  In  a  first  step,  Monte  Carlo  simulations  are  used  to  obtain  the  mean  and  confidence interval of the shielding effectiveness for conducting and composite materials over a wide frequency range in case of an  academic  shielding  configuration.  In  a  second  step,  a  non-intrusive  stochastics  technique  (Kriging  and  Polynomial  chaos  expansions)  are  combined  with  a  3D  finite  element  method  to  meta-models   in   order   to   study   a   simplified   but   realistic   configuration  of  a  power  transfer  system  at  85  kHz.  Such  an  approach  provides  fast  predictions  of  radiated  field  values  taking  into  account  the  variability  of  the  parameters  and  may  be useful to obtain the sensitivity of the model response.},
  doi          = {10.23919/EETA.2019.8804593},
  eventdate    = {2019-07-02/2019-07-04},
  file         = {:Lagouanelle2019 - Uncertainty Quantification in the Assessment of Human Exposure near Wireless Power Transfer Systems in Automotive Applications.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  review       = {Used Kriging as a metamodeling tool.},
  url          = {https://dx.doi.org/10.23919/EETA.2019.8804593},
}

@Article{Al2019,
  author   = {Resul Al and Chitta Ranjan Behera and Alexandr Zubov and Krist V.Gernaey and GürkanSin},
  title    = {Meta-modeling based efficient global sensitivity analysis for wastewater treatment plants – An application to the {BSM2} model},
  journal  = {Computers and Chemical Engineering},
  year     = {2019},
  volume   = {127},
  pages    = {223--246},
  abstract = {Global sensitivity analysis (GSA) is a powerful tool for quantifying the effects of model parameters on the performance outputs of engineering systems, such as wastewater treatment plants (WWTP). Due to the ever-growing sophistication of such systems and their models, significantly longer processing times are required to perform a system-wide simulation, which makes the use of traditional Monte Carlo (MC) based approaches for calculation of GSA measures, such as Sobol indices, impractical. In this work, we present a systematic framework to construct and validate highly accurate meta-models to perform an efficient GSA of complex WWTP models such as the Benchmark Simulation Model No. 2 (BSM2). The robustness and the efficacy of three meta-modeling approaches, namely polynomial chaos expansion (PCE), Gaussian process regression (GPR), and artificial neural networks (ANN), are tested on four engineering scenarios. The results reveal significant computational gains of the proposed framework over the MC-based approach without compromising accuracy.},
  doi      = {10.1016/j.compchemeng.2019.05.015},
  file     = {:Al2019 - Meta-modeling based efficient global sensitivity analysis for wastewater treatment plants – An application to the BSM2 model.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  review   = {Used UQLab to create a PCE metamodels.},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.05.015},
}

@MastersThesis{Ligeikis2019,
  author   = {Connor Ligeikis},
  title    = {Exploring Uncertainty in Real-Time Hybrid Substructuring},
  school   = {University of Connecticut},
  year     = {2019},
  type     = {mathesis},
  address  = {USA},
  abstract = {Real-time hybrid substructuring (RTHS) is a cyber-physical vibration testing method that partitions a structural system into numerical and physical substructures. RTHS incorporates sensing, actuation, and computing technologies to couple these substructures in real time. RTHS can be used to realistically examine the performance of structural systems with rate-dependent structural components that may be difficult to accurately model. However, uncertainty can be found in many aspects of RTHS testing including noise in sensor measurements, actuator performance and tracking, non-linearities in the physical test specimen, and numerical modeling assumptions. In the numerical substructure, uncertainty in material properties, stiffness, damping, or geometry may be considered random variables. Certain physical substructure characteristics may also be similarly parameterized. This thesis aims to develop and experimentally validate techniques that incorporate these parametric uncertainties into RTHS testing protocols, thereby improving the robustness of the RTHS method. This goal was accomplished through two studies.

First, a study was performed to extend and experimentally validate a proposed structural reliability method called Adaptive Kriging-Hybrid Simulation (AK-HS). The method combines a metamodeling technique known as Kriging, an adaptive learning algorithm, the Monte Carlo method, and RTHS testing to iteratively estimate a structural system’s probability of failure given random parameters in the numerical substructure. The method was validated with a series of bench-scale RTHS tests of a Taylor Devices, Inc. viscous damper connecting two adjacent 6-degree-of-freedom rigid body structures. The AK-HS method was found to accurately predict probabilities of failure for systems with up to 24 random variables using a reasonable number of RTHS tests.

The second study proposed and validated a method that can be used to develop metamodels of a system’s frequency response functions using Principal Component Analysis, Kriging, and RTHS. The proposed method was experimentally validated through a series of bench-scale RTHS tests of a Lord Corporation magnetorheological fluid damper controlling vibrations in a 2-degree-of-freedom mass-spring system subjected to an input ground acceleration. Uncertainty was introduced to the system by treating the numerical substructure spring stiffnesses and the physical damper current as random variables. It is shown that accurate, statistical metamodels can be created using a small number of RTHS tests. These metamodels may then be used to conduct Monte Carlo simulations to obtain distributions of the system’s frequency domain response behavior.},
  file     = {:Ligeikis2019 - Exploring Uncertainty in Real-Time Hybrid Substructuring.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  review   = {Used Kriging in UQLab.},
  url      = {https://opencommons.uconn.edu/gs_theses/1353/},
}

@InProceedings{Maze-Merceur2019,
  author       = {G. Mazé-Merceur and B. Etchessahar and Jean-Michel Geffrin and Amélie Litman and Antoine Roueff and Philippe Besnie},
  title        = {Satisfaction Indicators Taking into Account the Measurement and Computation Uncertainties for the Comparison of Data in Electromagnetics: Motivations and Scheduled Tasks of the {French National Working Group} {CDIIS}},
  booktitle    = {13th European Conference on Antennas and Propagation (EuCAP 2019)},
  year         = {2019},
  address      = {Krakow, Poland},
  organization = {IEEE},
  abstract     = {A  national  study  of  criteria  able  to  provide  a  satisfac-tion indicator about the comparison of data from electromagnetic measurement  and  computation,  taking  into  account  their  associ-ated  uncertainties,  has  been  organized  in  the  framework  of  a French  Working  Group  of  the  GdR  Ondes,  called  CDIIS  (Com-paraison  de  Données  entachées  d’Incertitudes:  Indicateurs  de Satisfaction). This  Working  Group involves  several industrial or academic research laboratories, including laboratories depending on  governmental  organisms.  Four  tasks  have  been  defined:  1/ Identification  of  a  set  of  satisfaction  indicators.  2/  Identification of  pertinent  test  cases  in  different  application  domains  of  elec-tromagnetics.  3/  Application  of  the  above  criteria  on  the  test cases.  4/  Conclusions:  which  criterion  is  best  adapted  to  a  given electromagnetism  problem.  This  paper  deals  with  Task  1  and discusses  the  results  of  various  indicators  applied  to  a  canonical RCS (Radar Cross Section) test case.},
  eventdate    = {2019-03-31/2019-04-05},
  file         = {:2019 - Satisfaction Indicators Taking into Account the Measurement and Computation Uncertainties for the Comparison of Data in Electromagnetics_ Motivations and Scheduled Tasks of the French National Working Group CDIIS.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  review       = {Used MCS in UQLab.},
  url          = {https://hal.archives-ouvertes.fr/hal-02133573/},
}

@InCollection{Ligeikis2019a,
  author    = {Connor Ligeikis and Richard Christenson},
  title     = {Incorporating Uncertainty in the Physical Substructure During Hybrid Substructuring},
  booktitle = {Model Validation and Uncertainty Quantification, Volume 3. Conference Proceedings of the Society for Experimental Mechanics Series.},
  publisher = {Springer},
  year      = {2019},
  editor    = {Robert Barthorpe},
  chapter   = {27},
  pages     = {237--239},
  abstract  = {In hybrid substructuring, a structural system is partitioned into a numerical substructure and a physical substructure. Typically, the physical substructure consists of a system component whose behavior is difficult to model while the numerical substructure consists of a computational model of the remainder of the system. Hybrid substructuring has previously been shown to be an effective method to quantify the effect of parametric uncertainties in the numerical substructure on the response of the system. This paper proposes and implements a methodology where the effect of parametric uncertainty can also be incorporated into the physical substructure. This idea is implemented in a series of small-scale Real-Time Hybrid Substructuring (RTHS) tests on a magneto-rheological fluid damper used to control a two degree-of-freedom mass-spring system. The physical current supplied to the damper is treated as a random variable. Using the RTHS test results, a metamodel of the system’s frequency domain behavior is developed using Principal Component Analysis and Kriging. This metamodel is then used to evaluate probabilistic system performance.},
  doi       = {10.1007/978-3-030-12075-7_27},
  file      = {:Ligeikis2019a - Incorporating Uncertainty in the Physical Substructure During Hybrid Substructuring.pdf:PDF},
  keywords  = {used UQLab, civil engineering},
  review    = {Used Kriging in UQLab.},
  url       = {https://doi.org/10.1007/978-3-030-12075-7_27},
}

@Article{Hamdi2019,
  author   = {Hamidreza Hamdi and Christopher R. Clarkson and Ali Esmail and Mario Costa Sousa},
  title    = {A {Bayesian} Approach for Optimizing the {Huff-n-Puff} Gas Injection Performance in Shale Reservoirs Under Parametric Uncertainty: A {Duvernay Shale} Example},
  journal  = {Society of Petroleum Engineers},
  year     = {2019},
  pages    = {1--23},
  abstract = {Recent  studies  have  indicated  that  Huff-n-Puff  (HNP)  gas  injection  has  the  potential  to  recover  anadditional 30-70\% oil from multi-fractured horizontal wells in shale reservoirs. Nonetheless, this techniqueis very sensitive to production constraints and is impacted by uncertainty related to measurement quality(particularly frequency and resolution), and lack of constraining data. In this paper, a Bayesian workflow isprovided to optimize the HNP process under uncertainty using a Duvernay shale well as an example.Compositional simulations are conducted which incorporate a tuned PVT model and a set of measuredcyclic injection/compaction pressure-sensitive permeability data. Markov chain Monte Carlo (McMC) is used to estimate  the  posterior  distributions  of  the  model  uncertain  variables  by  matching  the  primaryproduction data. The McMC process is accelerated by employing an accurate proxy model (kriging) whichis updated using a highly adaptive sampling algorithm. Gaussian Processes are then used to optimize theHNP control variables by maximizing the lower confidence interval (μ-σ) of cumulative oil production(after 10 years) across a fixed ensemble of uncertain variables sampled from posterior distributions.The uncertain variable space includes several parameters representing reservoir and fracture properties.The posterior distributions for some parameters, such as primary fracture permeability and effective half-length, are narrower, while wider distributions are obtained for other parameters. The results indicate thatthe impact of uncertain variables on HNP performance is nonlinear. Some uncertain variables (such asmolecular diffusion) that do not show strong sensitivity during the primary production strongly impact gasinjection HNP performance. The results of optimization under uncertainty confirm that the lower confidenceinterval  of  cumulative  oil  production  can  be  maximized  by  an  injection  time  of  around 1.5 months,  aproduction time of around 2.5 months, and very short soaking times. In addition, a maximum injection rateand a flowing bottomhole pressure around the bubble point are required to ensure maximum incrementalrecovery. Analysis of the objective function surface highlights some other sets of production constraintswith competitive results. Finally, the optimal set of production constraints, in combination with an ensembleof uncertain variables, results in a median HNP cumulative oil production that is 30\% greater than that forprimary production. 

The application of a Bayesian framework for optimizing the HNP performance in a real shale reservoir isintroduced for the first time. This work provides practical guidelines for the efficient application of advancedmachine learning techniques for optimization under uncertainty, resulting in better decision making.},
  doi      = {10.2118/195438-MS},
  file     = {:Hamdi2019 - A Bayesian Approach for Optimizing the Huff-n-Puff Gas Injection Performance in Shale Reservoirs Under Parametric Uncertainty_ A Duvernay Shale Example.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  review   = {Used Sobol' (PCE-based) and Cotter sensitivity indices.},
  url      = {https://doi.org/10.2118/195438-MS},
}

@InProceedings{Gorniak2019,
  author       = {Piotr Górniak},
  title        = {New Analytical {PCE} Coefficients for Uncertainty Quantification in Ray-Tracing Modeling},
  booktitle    = {13th European Conference on Antennas and Propagation (EuCAP 2019)},
  year         = {2019},
  address      = {Krakow, Poland},
  organization = {IEEE},
  abstract     = {Ray-tracing simulations of stochastic electromagnetic fields are considered in the paper. The author uses polynomial chaos expansion (PCE) coefficients for uncertainty quantification. The author introduces the new effective method, in terms of accuracy and calculation speed, for analytical derivation of PCE coefficients in ray-tracing modelling. The analytical PCE coefficients can be recalculated very fast when it is necessary to change probability densities of random variables of a simulation. A ray-tracing simulation of exemplary indoor scenario is used to compare the new method with general polynomial chaos (gPC) approach which requires numerical calculation of PCE coefficients for each set of probability densities of simulation random variables.},
  eventdate    = {2019-03-31/2019-04-05},
  file         = {:Gorniak2019 - New Analytical PCE Coefficients for Uncertainty Quantification in Ray-Tracing Modeling.pdf:PDF},
  keywords     = {used UQLab, computational science and engineering},
  review       = {Used PCE in UQLab},
  url          = {https://ieeexplore.ieee.org/document/8739844},
}

@Article{Zhou2019b,
  author   = {Yicheng Zhou and Zhenzhou Lu and Kai Cheng},
  title    = {A new surrogate modeling method combining polynomial chaos expansion and {Gaussian} kernel in a sparse {Bayesian} learning framework},
  journal  = {International Journal for Numerical Methods in Engineering},
  year     = {2019},
  pages    = {1--19},
  abstract = {Surrogate modeling techniques have been increasingly developed for optimization and uncertainty quantification problems in many engineering fields. The development of surrogates requires modeling high‐dimensional and nonsmooth functions with limited information. To this end, the hybrid surrogate modeling method, where different surrogate models are combined, offers an effective solution. In this paper, a new hybrid modeling technique is proposed by combining polynomial chaos expansion and kernel function in a sparse Bayesian learning framework. The proposed hybrid model possesses both the global characteristic advantage of polynomial chaos expansion and the local characteristic advantage of the Gaussian kernel. The parameterized priors are utilized to encourage the sparsity of the model. Moreover, an optimization algorithm aiming at maximizing Bayesian evidence is proposed for parameter optimization. To assess the performance of the proposed method, a detailed comparison is made with the well‐established PC‐Kriging technique. The results show that the proposed method is superior in terms of accuracy and robustness.},
  doi      = {10.1002/nme.6145},
  file     = {:Zhou2019b - A new surrogate modeling method combining polynomial chaos expansion and Gaussian kernel in a sparse Bayesian learning framework.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used Sequential PC-Kriging and Optimal PC-Kriging in UQLab for comparison purposes.},
  url      = {https://doi.org/10.1002/nme.6145},
}

@Article{Bernier2019,
  author   = {Carl Bernier and Jamie E. Padgett},
  title    = {Fragility and risk assessment of aboveground storage tanks subjected to concurrent surge, wave, and wind loads},
  journal  = {Reliability Engineering and System Safety},
  year     = {2019},
  volume   = {191},
  pages    = {1--11},
  abstract = {Comprehensive tools to assess the performance of aboveground storage tanks (ASTs) under multi-hazard storm conditions are currently lacking, despite the severe damage suffered by ASTs in past storms resulting in the release of hazardous substances. This paper presents a rigorous yet efficient methodology to develop fragility models and perform risk assessments of ASTs subjected to combined surge, wave, and wind loads. Parametrized fragility models are derived for buckling and dislocation from the ground. The buckling strength of ASTs is assessed using finite element analysis, while the stability against dislocation is evaluated using analytical limit state functions with surrogate modeling-based load models. Scenario and probabilistic risk assessments are then performed for a case study region by convolving the fragility models with hazard models. Results demonstrate that the derived fragility models are efficient tools to evaluate the performance of ASTs in industrial regions. Insights obtained from the fragility and risk assessments reveal that neglecting the multi-hazard nature of storms, as existing studies have done, can lead to a significant underestimation of vulnerability and risks. This paper also highlights how using surrogate model techniques can facilitate and reduce the computational complexity of fragility and risk assessments, particularly in multi-hazard settings.},
  doi      = {10.1016/j.ress.2019.106571},
  file     = {:Bernier2019 - Fragility and risk assessment of aboveground storage tanks subjected to concurrent surge, wave, and wind loads.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  review   = {Used Kriging in UQLab.},
  url      = {https://doi.org/10.1016/j.ress.2019.106571},
}

@Article{Tran2019,
  author   = {Vinh Ngoc Tran and Jongho Kim},
  title    = {Quantification of predictive uncertainty with a metamodel: toward more efficient hydrologic simulations},
  journal  = {Stochastic Environmental Research and Risk Assessment},
  year     = {2019},
  volume   = {33},
  number   = {7},
  pages    = {1453--1476},
  abstract = {Hydrologic flood prediction has been a quite complex and difficult task because of various sources of inherent uncertainty. Accurately quantifying these uncertainties plays a significant role in providing flood warnings and mitigating risk, but it is time-consuming. To offset the cost of quantifying the uncertainty, we adopted a highly efficient metamodel based on polynomial chaos expansion (PCE) theory and applied it to a lumped, deterministic rainfall–runoff model (Nedb{\o}r–Afstr{\o}mnings model, NAM) combined with generalized likelihood uncertainty estimation (GLUE). The central conclusions are: (1) the subjective aspects of GLUE (e.g., the cutoff threshold values of likelihood function) are investigated for 8 flood events that occurred in the Thu bon river watershed in Vietnam, resulting that the values of 0.82 for Nash–Sutcliffe efficiency, 4.05\% for peak error, and 4.35\% for volume error are determined as the acceptance thresholds. Moreover, the number of ensemble behavioral sets required to maintain the sufficient range of uncertainty but to avoid any unnecessary computation was set to 500. (2) The number of experiment designs (N) and degree of polynomial (p) are key factors in estimating PCE coefficients, and values of N = 50 and p = 4 are preferred. (3) The results computed using a PCE model consisting of polynomial bases are as good as those given by the NAM, while the total times required for making an ensemble in the PCE model are approximately seventeen times faster. (4) Two parameters (“CQOF” and “CK12”) turned out to be most dominant based on a visual inspection of the posterior distribution and the mathematical computations of the Sobol’ and Morris sensitivity analysis. Identification of the posterior parameter distributions from the calibration process helps to find the behavioral sets even faster. The unified framework that presents the most efficient ways of predicting flow regime and quantifying the uncertainty without deteriorating accuracy will ultimately be helpful for providing warnings and mitigating flood risk in a timely manner.},
  doi      = {10.1007/s00477-019-01703-0},
  file     = {:Tran2019 - Quantification of predictive uncertainty with a metamodel_ toward more efficient hydrologic simulations.pdf:PDF},
  keywords = {used UQLab, geoscience},
  review   = {Used PCE in UQLab.},
  url      = {https://doi.org/10.1007/s00477-019-01703-0},
}

@PhdThesis{Jones2019a,
  author   = {Mark Nicholas Jones},
  title    = {Design and Optimisation of Oleochemical Processes},
  school   = {Technical University of Denmark (DTU)},
  year     = {2019},
  type     = {phdthesis},
  address  = {Kgs. Lyngby, Denmark},
  abstract = {Process Systems Engineering (PSE) is a discipline which connects a wide range of chemical engineering topics in a systems view approach. The reason for this systematic view of this scientific field is the need of computational concepts, numerical methods and computer-aided tools which can be applied to different use cases in industry. The multi-scale framework developed in this work encompasses four levels of application: (I) A server based property prediction software prototype which quantifies the uncertainty of group contribution methods and quantitative structure property relationship models and provides the confidence bounds of the estimates. (II) A modelling development level which allows the user to develop models
in a flexible way by using common programming languages for fast prototyping (Python) and high performance computing (Fortran). (III) An interface to process simulators to analyse and optimise entire flowsheets with advanced routines. (IV) A superstructure optimisation layer where surrogate models generated from unit operations or process models can be embedded in a superstructure formulation and solved for the optimal process structure and operating point. The contributions presented in this work show how the developed framework allows to tackle research in machine learning, optimisation and Monte Carlo driven methods such as sensitivity analysis. The developed tools were applied to the oleochemical domain with selected processes. In conclusion, this work demonstrates that a modular approach to process systems engineering, combined with tools integration from various vendors, allows to gain new knowledge in a time-efficient and augmentable manner.},
  file     = {:Jones2019a - Design and Optimisation of Oleochemical Processes.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  review   = {Used PCE and Sobol' sensitivity analysis in UQLab to create a framework.},
  url      = {https://orbit.dtu.dk/en/publications/design-and-optimisation-of-oleochemical-processes(f63e262f-6567-467d-bcdb-c97fbe7bde66).html},
}

@Article{Slobbe2019,
  author   = {Arthur Slobbe and Árpád Rózsás and Diego L. Allaix and Agnieszka Bigaj‐van Vliet},
  title    = {On the value of a reliability‐based nonlinear finite element analysis approach in the assessment of concrete structures},
  journal  = {Structural Concrete},
  year     = {2019},
  pages    = {1--16},
  abstract = {The objective of this paper is to explore the value of reliability‐based nonlinear finite element analysis (NLFEA) over the currently available, standardized assessment methods. To our knowledge, no studies are available on this subject, and this paper provides a first insight into the value and reliability level of these assessment methods. The exploration is illustrated through three reinforced concrete structural members: a continuous girder, a continuous deep beam, and a high‐strength deep beam. The analysis is performed gradually: step by step advancing the approximation level of the mechanical and the probabilistic models. The added value of the reliability‐based NLFEA over the semi‐probabilistic Eurocode (EC) method is found to be on average 0.60. In other words: even if according to the semi‐probabilistic EC method the design action (Ed) is 60\% higher than the design resistance (Rd), the compliance with the target reliability criterion can be demonstrated by a reliability‐based NLFEA. Furthermore, it is observed that the gain and its cause (i.e., more advanced mechanical or probabilistic models) are different for the three cases. Though this outcome is restricted to the analyzed cases and should be interpreted as an upper limit added value, it indicates that a more detailed physical representation of the problem and an explicit treatment of uncertainties may uncover substantial reserves compared with the currently available, standardized assessment methods. Hence, the reliability‐based NLFEA method offers a promising alternative in the assessment of existing structures, enabling to avoid expensive measures that might be needed based on simplified methods.},
  doi      = {10.1002/suco.201800344},
  file     = {:Slobbe2019 - On the value of a reliability‐based nonlinear finite element analysis approach in the assessment of concrete structures.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  review   = {Used AK-MCS in UQLab.},
  url      = {https://doi.org/10.1002/suco.201800344},
}

@Article{Shi2019,
  author   = {Yuhan Shi and Wei Gong and Qingyun Duan and Jackson Charles and Cunde Xiao and Heng Wang},
  title    = {How parameter specification of an Earth system model of intermediate complexity influences its climate simulations},
  journal  = {Progress in Earth and Planetary Science},
  year     = {2019},
  volume   = {6},
  number   = {46},
  pages    = {1--18},
  abstract = {Earth system models (ESMs) consist of parameterization schemes based on one’s perception of how the Earth system functions. A typical ESM contains a large number of parameters (i.e., the constants and exponents in the parameterization schemes) whose specification can have a significant impact on an ESM’s simulation capabilities. Sensitivity analyses (SA) is an important tool for assessing how parameter specification influences model simulations. In this study, we used an Earth system model of intermediate complexity (EMIC)—LOVECLIM as an example to illustrate how SA methods can be used to identify the most sensitive parameters that control the simulations of several key global water and energy cycle variables, including global annual mean absolute surface air temperature (TG), precipitation and evaporation over the land and over the oceans (PL, PO, EL, EO), and land runoff (RL). We also demonstrate how judiciously specifying model parameters can improve the simulations of those variables. Three SA methods MARS, RF, and sparse PCE-based Sobol’ method were used to evaluate a pool of 25 adjustable parameters chosen from land, atmosphere, and ocean components of LOVECLIM and their results were intercompared to ensure robustness of the results. It is found that with different parameter specification, TG can vary from 10 to 20 oC, and the values of PL, PO, EL, and EO can change by more than 100\%. An interesting observation is that the value of RL vary from 13,000 to 35,000 km3, far below the observed climatological value of 40,000 km3, indicating a model structural deficiency in representing land runoff by LOVECLIM which must be corrected to obtain more reasonable global water budgets. We also note that parameter sensitivities are significantly different at different latitudes. Finally, we showed that global water and energy cycle simulations can be significantly improved by even a crude automatic parameter tuning, indicating that parameter optimization can be a viable way to improve ESM climate simulations. The results from this study should help us to understand the parameter uncertainty of a full-scale ESM.},
  doi      = {10.1186/s40645-019-0294-x},
  file     = {:Shi2019 - How parameter specification of an Earth system model of intermediate complexity influences its climate simulations.pdf:PDF},
  keywords = {used UQLab, geoscience},
  review   = {Used Sparse PCE in UQLab.},
  url      = {https://doi.org/10.1186/s40645-019-0294-x},
}

@InProceedings{Houret2019,
  author       = {Thomas Houret and Philippe Besnier and Stéphane Vauchamp and Philippe Pouliguen},
  title        = {Comparison of Surrogate Models for Extreme Quantile Estimation in the Context of {EMC} Risk Analysis (I)},
  booktitle    = {Joint International Symposium on Electromagnetic Compatibility and Asia-Pacific International Symposium on Electromagnetic Compatibility (EMC \& APEMC 2019)},
  year         = {2019},
  address      = {Sapporo, Japan},
  organization = {IEICE Communication Society},
  abstract     = {Various  EMC  problems   may  be  studied  from numerical  simulations  involving  3D  Maxwell  equation  solvers.  However,  the  EMC  risk  analysis,  either  from  a  susceptibility  or emissivity   point   of   view   requires   various   configurations   of coupling   paths   described   by   important   sets   of   unknown   or uncertain   parameters.   The   use   of   surrogate   models   is   very relevant  to  speed  up  the  risk  analysis  process.  More  specifically, values at  risk corresponding to extreme values of relevantfields, currents  or  voltages  are  often  the  most  important  information with  regard  to  a  possible  EMC  risk.  Specific  methods  such  as controlled stratification provide a way to sample the input space of  random  variables  in  an efficient  way  to  estimate  extreme quantiles of a distribution. However, it requires a simple (i.e. fast calculation time) companionmodel. This companion model has to be correlated to the  reference  model in  a specific  sense. Building a surrogate model would be a possible way. This paper discusses various  surrogate  models  and  provides  some conclusionsabout their abilityto  provide  either  a  direct  estimation  of  extreme quantiles  of  the  response  of  interest,  or a  companion  model  forthe controlled stratification method.},
  eventdate    = {2019-06-03/2019-06-07},
  file         = {:Houret2019 - Comparison of Surrogate Models for Extreme Quantile Estimation in the Context of EMC Risk Analysis (I).pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  review       = {Used SVM, PCE, Kriging, and PC-Kriging in UQLab.},
  url          = {https://www.researchgate.net/publication/334131327_Comparison_of_Surrogate_Models_for_Extreme_Quantile_Estimation_in_the_Context_of_EMC_Risk_Analysis},
}

@PhdThesis{Aleksankina2019,
  author   = {Ksenia Aleksankina},
  title    = {Application of global methods for sensitivity analysis and uncertainty assessment of atmospheric chemistry transport models},
  school   = {The University of Edinbrugh},
  year     = {2019},
  type     = {phdthesis},
  address  = {UK},
  abstract = {Atmospheric concentrations of air pollutants remain high, and air quality is still an issue that requires attention in many countries including the UK. Atmospheric chemistry transport models (ACTMs) are widely used to provide scientific support for policy development in relation to the mitigation of the detrimental effects of air pollution on human health and ecosystems. Hence it is important to assess the level of uncertainty associated with model predictions. In this work, the application of global sensitivity analysis and uncertainty assessment methods is investigated for two ACTMs of different complexity: the Fine Resolution Atmospheric Multi-pollutant Exchange (FRAME) model and the UK regional application of the European Monitoring and Evaluation Programme (EMEP4UK) model. For both models, the uncertainties in the outputs resulting from uncertainties in the model input emissions are quantified and apportioned. Additionally, the overall model response to variations in the input emissions within a ± 40 \% range from the baseline is investigated as this range of variation is typically used for future scenario simulations. FRAME is a Lagrangian ACTM with 5 km x 5 km horizontal resolution over the UK domain that is used to estimate annual average concentrations and deposition of sulphur and nitrogen species. In the model, air columns with 33 vertical layers of varying thickness (from 1 m at the surface to 100 m at the top of the mixing layer) move from the boundary of the domain along straight-line trajectories with different starting angles at a 1o resolution. The model utilises annually averaged meteorology to define the column trajectories and rainfall. The chemical scheme includes gaseous-and aqueous-phase reactions. FRAME supplies Source-Receptor Relationship (SRR) matrices for the UK Integrated Assessment Model, which directly underpins UK air pollution control policies. EMEP4UK is a 3-D Eulerian model with a horizontal resolution of 5 km × 5 km over the British Isles and 20 vertical levels, extending from the ground to 100 hPa, which is also extensively used to inform UK air quality assessment. The chemical scheme implemented in the model is EmChem09 with the MARS equilibrium module for gas-aerosol partitioning of secondary inorganic aerosol. In addition to the pollutants modelled by FRAME, EMEP4UK is capable of modelling ozone (O3) and speciated particulate matter (PM2.5 and PM10) concentrations, all at hourly temporal resolutions. In this study, the uncertainty ranges for the input emissions from UK anthropogenic land-based sources were assigned according to the data provided by the UK National Atmospheric Emissions Inventory. For the FRAME model, the uncertainties in the outputs were propagated from the uncertainties in the emissions of SO2, NOx, and NH3. For the EMEP4UK model, an increased number of input variables was used; the emissions of NOx, SO2, NH3, VOC, and primary PM2.5 were split into 13 model inputs based on the contributions from different emission source sectors. The optimised Latin hypercube sampling design was used for both models to construct model runs that covered a chosen range of input emission perturbations. The FRAME model was investigated using several regression techniques. The response of the model to emission perturbations within a \pm 40\% range from the baseline value was found to be substantially linear. Surface concentrations of SO2, NOx, and NH3 together with the deposition of S and N were found to be predominantly sensitive to the emissions of the respective pollutant, while sensitivities of secondary species such as HNO3 and particulate SO42−, NO3−, and NH4+ to pollutant emissions were more complex and geographically variable. Additionally, the uncertainty in the surface concentrations of NH3 and NOx and the depositions of NHx and NOy was shown to be due to uncertainty in a single input variable, NH3, and NOx respectively. In contrast, the uncertainty in concentration and deposition of sulfur containing species were affected by the uncertainties in both NH3 and SO2 emissions. Similarly, the relative uncertainties in the modelled surface concentrations of each of the secondary pollutant variables were affected by the uncertainty range of at least two input variables. An emulator-based approach was used to propagate and apportion uncertainty in EMEP4UK outputs and investigate the model response to input perturbations. A separate Gaussian process emulator was used to estimate model predictions at unsampled points in the space of the uncertain model inputs for every modelled grid cell. For the surface concentrations of O3, NO2, and PM2.5 (pollutants associated with the adverse effects on human health) the highest level of uncertainty was found to occur in the grid cells comprising urban areas, up to \pm7\%, \pm9\%, and \pm9\% respectively. However, overall uncertainty calculated for the land-based grid cells for the variables above was found to be low, which indicates that the outputs may be more sensitive to variation in other model input parameters, such as chemical or physical constants. Alternatively, UK land-based concentrations of O3, NO2, and PM2.5 may be dominated by the precursor emissions and long-range transport of pollutants from outside the UK. Investigating seasonal changes in uncertainty and sensitivity for the monthly-averaged model outputs allowed determination of the importance of the inputs that drive uncertainty changes throughout the year. For example, uncertainty in O3 was driven more by uncertainty in VOC emissions during the summer, and for PM2.5 the importance of NH3 in driving overall uncertainty increased during spring and summer. The aim of the global methods for sensitivity analysis and uncertainty assessment presented here is to quantify the confidence in model predictions associated with particular aspects of model operation. Furthermore, the model runs and emulators created for the analyses can be used to predict the ACTM response for any other combination of perturbed input emissions within the ranges set for the original Latin hypercube sampling design without the need to re-run the ACTM. This makes exploring different emission perturbation scenarios possible at a significantly reduced computational cost. The methods discussed in this study can be applied to any operational aspect of any ACTM.},
  file     = {:Aleksankina2019 - Application of global methods for sensitivity analysis and uncertainty assessment of atmospheric chemistry transport models.pdf:PDF},
  keywords = {used UQLab, geoscience},
  review   = {Use Kriging in UQLab.},
  url      = {http://hdl.handle.net/1842/35681},
}

@Article{Gorniak2019a,
  author   = {Piotr Górniak and Wojciech Bandurski},
  title    = {{PCE}-Based Approach to Worst-Case Scenario Analysis in WirelessTelecommunication System},
  journal  = {Progress In Electromagnetics Research B,},
  year     = {2019},
  volume   = {84},
  pages    = {153--170},
  abstract = {In the paper, we present a novel PCE-based approach for the effective analysis of worst-case scenario in a wireless telecommunication system. Usually, in such analysis derivation of polynomialchaos expansion (PCE meta-model) of a considered EM field function for one precise set of probabilitydensities of random variables does not provide enough information.  Consequently, a number of PCEmeta-models of the EM field function should be derived, each for the different joint probability density ofa vector of random variables, e.g., associated with different mean (nominal) values of random variables.The general polynomial chaos (gPC) approach requires numerical calculations for each PCE meta-modelderivation. In order to significantly decrease the time required to derive all of the PCE meta-models, thenovel approach has been introduced. It utilizes the novel so-called primary approximation and the novelanalytical formulas. They significantly decrease the number of numerical calculations required to deriveall of the PCE meta-models compared with the gPC approach. In the paper, we analyze the stochasticEM fields distributions in a telecommunication system in a spatial domain. For this purpose, analysis ofuncertainties associated with a propagation channel as well as with transmitting and receiving antennaswas introduced. We take advantage of a ray theory in our analysis. This allows us to provide the novelmethod for rapid calculation of a PCE meta-model of a telecommunication system transfer function byusing the separate PCE meta-models associated with antennas and a propagation channel.},
  doi      = {10.2528/PIERB19032004},
  file     = {:Gorniak2019a - PCE-Based Approach to Worst-Case Scenario Analysis in WirelessTelecommunication System.pdf:PDF},
  keywords = {used UQLab, electrical engineering},
  review   = {Used PCE in UQLab.},
  url      = {https://doi.org/10.2528/PIERB19032004},
}

@Article{Zhu2019,
  author   = {Bin Zhu and Huafu Pei and Qing Yang},
  title    = {An intelligent response surface method for analyzing slope reliability based on {Gaussian} process regression},
  journal  = {International Journal for Numerical and Analytical Methods in Geomechanics},
  year     = {2019},
  pages    = {1--18},
  abstract = {Problems in geotechnical engine ering inevitably involve many uncertainties in the analysis. Reliability methods are important for evaluating slope stability
and can take the uncertainties into consideration. In this paper, a novel intelligent response surface method is proposed in which a machine learning algo-
rithm, namely Gaussian process regression, is used to approximate the high‐dimensional and highly nonlinear response hypersurface. An iterative algorithm is also proposed for updating the response surface dynamically by adding the new training point nearest to the limit state surface to the initial training database at each step. The proposed Gaussian process response surface method is used to analyze three different case studies to assess its validity and efficiency. Direct Monte Carlo simulation is also carried out in each case to serve as the benchmark. Comparing with other methods confirms the accuracy and efficiency of the novel intelligent response surface method, which requires fewer performance function calls and avoids the need to normalize the correlative non‐normal variables.},
  doi      = {10.1002/nag.2988},
  file     = {:Zhu2019 - An intelligent response surface method for analyzing slope reliability based on Gaussian process regression.pdf:PDF},
  keywords = {used UQLab, geoscience},
  review   = {Used Kriging in UQLab.},
  url      = {https://doi.org/10.1002/nag.2988},
}

@InProceedings{Yang2019,
  author       = {Huoming Yang and Hendrik Just and Sibylle Dieckerhoff},
  title        = {Identification of Critical Parameters Affecting the Small-Signal Stability of Converter-based Microgrids},
  booktitle    = {20th Workshop on Control and Modeling for Power Electronics (COMPEL 2019)},
  year         = {2019},
  address      = {Toronto, Canada},
  organization = {IEEE},
  abstract     = {The analysis of the small-signal stability of converter-based microgrids is well established, but the existing deterministic approaches cannot fully capture the impact of random parameters resulting from complex converter control structures and stochastic operation scenarios. Accurate assessment of the relevance of different parameters can facilitate efficient modeling, online monitoring and control design of the microgrids. In this paper, a global sensitivity analysis (GSA) framework is proposed to identify the most relevant parameters that affect the small-signal stability of converter-based microgrids. First, the systematic approach to derive the complete small-signal state-space model of the microgrid is introduced and the stability index is defined. Then, the system operating states, the control and the network parameters are chosen as input variables, and the priority ranking procedure based on GSA is explained. Next, the Levenberg-Marquardt based power flow calculation is adopted to determine the steady-state operation point. The polynomial chaos expansion and the low-rank approximation methods are introduced into the GSA to further improve computation efficiency. Finally, the proposed methodology is applied to an exemplary microgrid. The critical parameters influencing the system small-signal stability are identified, and the results are compared with those of the conventional local sensitivity analysis. The correctness and effectiveness of the proposed methodology are verified by real-time simulation results.},
  doi          = {10.1109/COMPEL.2019.8769621},
  eventdate    = {2019-06-17/2019-06-20},
  file         = {:Yang2019 - Identification of Critical Parameters Affecting the Small-Signal Stability of Converter-based Microgrids.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  review       = {Used PCE and LRA for Sobol' sensitivity analysis.},
  url          = {https://doi.org/10.1109/COMPEL.2019.8769621},
}

@Article{Wang2019,
  author   = {Han Wang and Zheng Yan and Xiaoyuan Xu and Kun He},
  title    = {Probabilistic power flow analysis of microgrid with renewable energy},
  journal  = {International Journal of Electrical Power and Energy Systems},
  year     = {2019},
  volume   = {114},
  pages    = {1--10},
  abstract = {With the development of renewable-based distributed generation (RDG), there are increasing uncertainties in the operation of microgrids (MGs), and stochastic evaluation methods are attracting more attention nowadays. In this paper, a probabilistic power flow (PPF) analysis method is proposed to evaluate the influence of uncertainties on the power flow of MGs. First, the MG PPF model is established considering different operation modes of MGs and uncertainties of RDG and load demands. Then, the Borgonovo method, which is a density-based global sensitivity analysis (GSA) method, is used to evaluate the importance of input variables in PPF calculation. To improve the computational efficiency of GSA, the sparse polynomial chaos expansion (SPCE) is used to establish the surrogate model of MG PPF, and the Borgonovo index is calculated based on the surrogate model. Finally, the procedure of applying GSA to MG power flow is established. The proposed method is tested using 33-node and 123-node MGs, and is compared with other methods to validate its effectiveness. Simulation results indicate that the proposed method identifies critical uncertainties that affect MG power flow. Based on the rankings of input variables, the influences of critical uncertainties are diminished with energy storage systems.},
  doi      = {10.1016/j.ijepes.2019.105393},
  file     = {:Wang2019 - Probabilistic power flow analysis of microgrid with renewable energy.pdf:PDF},
  keywords = {used UQLab, energy engineering},
  review   = {Used Sparse PCE in UQLab.},
  url      = {https://doi.org/10.1016/j.ijepes.2019.105393},
}

@Article{Song2019,
  author   = {Jian Song and YunYang and Gan Chen and Xiaomin Sun and Jin Lin and Jianfeng Wu and Jichun Wu},
  title    = {Surrogate assisted multi-objective robust optimization for groundwater monitoring network design},
  journal  = {Journal of Hydrology},
  year     = {2019},
  volume   = {577},
  pages    = {1--16},
  abstract = {The robust optimization of groundwater quality monitoring network is subject to many conflicting objectives and high level of uncertainty in hydraulic conductivity. This study develops a two-stage stochastic optimization framework including the uncertainty quantification using a cheap-to-evaluate surrogate model and an improved epsilon multi-objective noisy memetic algorithm (\epsilon-MONMA) for monitoring network design. The surrogate model based on sparse polynomial chaos expansion (PCE) is constructed to replace expensive simulation model in the uncertainty quantification of concentrations at the pre-defined monitoring locations for reducing huge computational cost. Additionally, the scenario discovery strategy using sparse PCE model is applied to filter a typical scenario set and the centroid of contaminant plume is used as the diversity metric, which avoids enumerating all possible contamination plumes caused by the uncertain K-field in the optimization. The proposed algorithm is then employed to solve stochastic management model to achieve robust monitoring design, indicating the insensitivity of monitoring design to plume uncertainty no matter which of the many possible scenarios becomes the true distribution of contamination under the true K-field. A synthetic aquifer considering uncertainty in hydraulic conductivity is designed to optimize monitoring network design. The Pareto-optimal solutions to the synthetic example are achieved under three of plume scenario sets defined at deterministic scenario (Scenario A0), Monte Carlo based scenario discovery (Scenario A1) and surrogate assisted scenario discovery (Scenario A2), respectively. Comprehensive analysis demonstrates that the monitoring design based on Scenario A2 outperforms either of the two designs based on Scenarios A0 and A1 in terms of the improvement of robustness of designs evaluated against the typical scenario set. Meanwhile, the performance of monitoring network deteriorates as the uncertainty of plume (noisy strength) increases, indicating the significance of reducing parameter uncertainty in groundwater monitoring design. The research findings show that the developed stochastic optimization framework is a computationally efficient and promising tool for multi-objective design of groundwater monitoring network under uncertainty.},
  doi      = {10.1016/j.jhydrol.2019.123994},
  file     = {:- Surrogate assisted multi-objective robust optimization for groundwater monitoring network design.pdf:PDF},
  keywords = {used UQLab, monitoring and remote sensing},
  review   = {Used Sparse PCE in UQLab.},
  url      = {https://doi.org/10.1016/j.jhydrol.2019.123994},
}

@Article{Radaideh2019,
  author   = {Majdi I. Radaideh and Tomasz Kozlowski},
  title    = {Analyzing nuclear reactor simulation data and uncertainty with the {Group Method of Data Handling}},
  journal  = {Nuclear Engineering and Technology},
  year     = {2019},
  pages    = {1--9},
  abstract = {Group Method of Data Handling (GMDH) is considered one of the earliest deep learning methods. Deep learning gained additional interest in today's applications due to its capability to handle complex and high dimensional problems. In this study, multi-layer GMDH networks are used to perform uncertainty quantification (UQ) and sensitivity analysis (SA) of nuclear reactor simulations. GMDH is utilized as a surrogate/metamodel to replace high fidelity computer models with cheap-to-evaluate surrogate models, which facilitate UQ and SA tasks (e.g. variance decomposition, uncertainty propagation, etc.). GMDH performance is validated through two UQ applications in reactor simulations: (1) low dimensional input space (two-phase flow in a reactor channel), and (2) high dimensional space (8-group homogenized cross-sections). In both applications, GMDH networks show very good performance with small mean absolute and squared errors as well as high accuracy in capturing the target variance. GMDH is utilized afterward to perform UQ tasks such as variance decomposition through Sobol indices, and GMDH-based uncertainty propagation with large number of samples. GMDH performance is also compared to other surrogates including Gaussian processes and polynomial chaos expansions. The comparison shows that GMDH has competitive performance with the other methods for the low dimensional problem, and reliable performance for the high dimensional problem.},
  doi      = {10.1016/j.net.2019.07.023},
  file     = {:Radaideh2019 - Analyzing nuclear reactor simulation data and uncertainty with the Group Method of Data Handling.pdf:PDF},
  keywords = {used UQLab, nuclear engineering},
  review   = {Used PCE and Kriging in UQLab.},
  url      = {https://doi.org/10.1016/j.net.2019.07.023},
}

@Article{Paulson2019,
  author   = {Joel A. Paulson and Marc Martin-Casas and Ali Mesbah},
  title    = {Fast uncertainty quantification for dynamic flux balance analysis using non-smooth polynomial chaos expansions},
  journal  = {PLOS Computational Biology},
  year     = {2019},
  volume   = {15},
  number   = {8},
  pages    = {1--35},
  abstract = {We present a novel surrogate modeling method that can be used to accelerate the solution of uncertainty quantification (UQ) problems arising in nonlinear and non-smooth models of biological systems. In particular, we focus on dynamic flux balance analysis (DFBA) models that couple intracellular fluxes, found from the solution of a constrained metabolic network model of the cellular metabolism, to the time-varying nature of the extracellular substrate and product concentrations. DFBA models are generally computationally expensive and present unique challenges to UQ, as they entail dynamic simulations with discrete events that correspond to switches in the active set of the solution of the constrained intracellular model. The proposed non-smooth polynomial chaos expansion (nsPCE) method is an extension of traditional PCE that can effectively capture singularities in the DFBA model response due to the occurrence of these discrete events. The key idea in nsPCE is to use a model of the singularity time to partition the parameter space into two elements on which the model response behaves smoothly. Separate PCE models are then fit in both elements using a basis-adaptive sparse regression approach that is known to scale well with respect to the number of uncertain parameters. We demonstrate the effectiveness of nsPCE on a DFBA model of an E. coli monoculture that consists of 1075 reactions and 761 metabolites. We first illustrate how traditional PCE is unable to handle problems of this level of complexity. We demonstrate that over 800-fold savings in computational cost of uncertainty propagation and Bayesian estimation of parameters in the substrate uptake kinetics can be achieved by using the nsPCE surrogates in place of the full DFBA model simulations. We then investigate the scalability of the nsPCE method by utilizing it for global sensitivity analysis and maximum a posteriori estimation in a synthetic metabolic network problem with a larger number of parameters related to both intracellular and extracellular quantities.},
  doi      = {10.1371/journal.pcbi.1007308},
  file     = {:Paulson2019 - Fast uncertainty quantification for dynamic flux balance analysis using non-smooth polynomial chaos expansions.pdf:PDF},
  keywords = {used UQLab, life sciences, biology},
  review   = {Used sparse PCE of UQLab.},
  url      = {https://doi.org/10.1371/journal.pcbi.1007308},
}

@Article{Li2019,
  author   = {Yangtian Li and Haibin Li and Guangmei Wei},
  title    = {Dimension-adaptive algorithm-based {PCE} for models with many model parameters},
  journal  = {Engineering Computations},
  year     = {2019},
  volume   = {In-press},
  abstract = {Purpose–To present the models with many model parameters by polynomial chaos expansion (PCE), andimprove the accuracy, this paper aims to present dimension-adaptive algorithm-based PCE technique andverify the feasibility of the proposed method through taking solid rocket motor ignition under lowtemperature as an example.Design/methodology/approach–The main approaches of this work are as follows: presenting a two-step dimension-adaptive algorithm; through computing the PCE coefficients using dimension-adaptivealgorithm, improving the accuracy of PCE surrogate model obtained; and applying the proposed method touncertainty quantification (UQ) of solid rocket motor ignition under low temperature to verify the feasibilityof the proposed method.Findings–The result indicates that by means of comparing with some conventional non-invasive method,the proposed method is able to raise the computational accuracy significantly on condition of meeting theefficiency requirement.Originality/value–This paper proposes an approach in which the optimal non-uniform grid that canavoid the issue of overfitting or underfitting is obtained.},
  doi      = {10.1108/EC-12-2018-0595},
  file     = {:Li2019 - Dimension-adaptive algorithm-based PCE for models with many model parameters.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  review   = {Used UQLab for sparse PCE as a building block for a more advanced method.},
  url      = {https://doi.org/10.1108/EC-12-2018-0595},
}

@InProceedings{Gorniak2019b,
  author    = {Piotr Górniak},
  title     = {The Intrusive {PCE}-Based Method for Uncertainty Calculation in Ray-Tracing Analysis of {5G} {EM} Wave Propagation},
  booktitle = {IEEE-APS Conference on Antennas and Propagation in Wireless Communications},
  year      = {2019},
  address   = {Granada, Spain},
  eventdate = {2019-09-09/2019-09-13},
  file      = {:2019 - The Intrusive PCE-Based Method for Uncertainty Calculation in Ray-Tracing Analysis of 5G EM Wave Propagation.pdf:PDF},
  keywords  = {electrical engineering, used UQLab},
  url       = {http://www.multimedia.edu.pl/?page=publication&section=The-Intrusive-PCE-Based-Method-for-Uncertainty-Calculation-in-Ray-Tracing-Analysis-of-5G-EM-Wave-Propagation},
}

@MastersThesis{Trevisi2019,
  author   = {Trevisi, Filippo},
  title    = {Configuration optimisation of kite-based wind turbines},
  school   = {University of Padua},
  year     = {2019},
  type     = {mathesis},
  address  = {Italy},
  file     = {:Trevisi2019 - Configuration Optimisation of Kite-based Wind Turbines.pdf:PDF},
  keywords = {Used UQLab, mechanical engineering},
  review   = {Used UQLab for metamodeling (PCE) and sensitivity analysis},
  url      = {http://tesi.cab.unipd.it/62975/},
}

@Article{Razek2019,
  author   = {Razek, Adel and Pichon, Lionel and Kameni, Abelin and Makong, Ludovic and Rasm, Sahand},
  title    = {Evaluation of human exposure owing to wireless power transfer systems in electric vehicles},
  journal  = {Athens Journal of Technology \& Engineering},
  year     = {2019},
  file     = {:Razek2019 - Evaluation of human exposure owing to wireless power transfer systems in electric vehicles.pdf:PDF},
  keywords = {Used UQLab, electrical engineering},
}

@Article{Groenquist2019,
  author    = {Gr{\"o}nquist, Philippe and Wood, Dylan and Hassani, Mohammad M and Wittel, Falk K and Menges, Achim and R{\"u}ggeberg, Markus},
  title     = {Analysis of hygroscopic self-shaping wood at large scale for curved mass timber structures},
  journal   = {Science Advances},
  year      = {2019},
  volume    = {5},
  number    = {9},
  doi       = {10.1126/sciadv.aax1311},
  file      = {Supplemental file:aax1311_SM.pdf:PDF;Full text:Groenquist2019 - Analysis of hygroscopic self-shaping wood at large scale for curved mass timber structures.pdf:PDF},
  keywords  = {Used UQLab, civil engineering},
  publisher = {American Association for the Advancement of Science},
  url       = {https://doi.org/10.1126/sciadv.aax1311},
}

@InProceedings{Houret2019a,
  author        = {Houret, T and Besnier, P and Vauchamp, S and Pouliguen, P},
  title         = {Combining {Kriging} and controlled stratification to identify extreme levels of electromagnetic interference},
  booktitle     = {2019 International Symposium on Electromagnetic Compatibility (EMC EUROPE 2019)},
  year          = {2019},
  address       = {Barcelona, Spain},
  __markedentry = {EMC risk analysis requires various configurations of coupling paths described by important sets of unknown or uncertain parameters. More specifically, values at risk corresponding to extreme values of relevant fields, currents or voltages are often the most important information with regard to a possible EMC risk. Therefore, we aim at estimating extreme quantiles of the relevant field, current or voltage. Controlled stratification accelerates the standard Empirical estimation convergence to sample output extreme values, thus reducing the required number of calls to cost-expensive full-wave simulations. However, controlled stratification requires a simple (i.e. fast calculation time) model with sufficient correlation to the initial model. The main idea in this communication is to use a surrogate model as a simple model. Kriging was previously identified as a surrogate model with relevant properties. In this paper, we investigate the performance of combined kriging and control stratification. We show that this combination outperforms the stand-alone kriging surrogate model for estimating extreme quantiles. On the contrary, the latter performs better to identify less extreme quantiles.},
  abstract      = {EMC risk analysis requires various configurations of coupling paths described by important sets of unknown or uncertain parameters. More specifically, values at risk corresponding to extreme values of relevant fields, currents or voltages are often the most important information with regard to a possible EMC risk. Therefore, we aim at estimating extreme quantiles of the relevant field, current or voltage. Controlled stratification accelerates the standard Empirical estimation convergence to sample output extreme values, thus reducing the required number of calls to cost-expensive full-wave simulations. However, controlled stratification requires a simple (i.e. fast calculation time) model with sufficient correlation to the initial model. The main idea in this communication is to use a surrogate model as a simple model. Kriging was previously identified as a surrogate model with relevant properties. In this paper, we investigate the performance of combined kriging and control stratification. We show that this combination outperforms the stand-alone kriging surrogate model for estimating extreme quantiles. On the contrary, the latter performs better to identify less extreme quantiles.},
  eventdate     = {2019-09-02/2019-09-06},
  file          = {:Houret2019a - Combining Kriging and controlled stratification to identify extreme levels of electromagnetic interference.pdf:PDF},
  keywords      = {Used UQLab, electrical engineering},
  url           = {https://www.researchgate.net/publication/335789611_Combining_Kriging_and_Controlled_Stratification_to_Identify_Extreme_Levels_of_Electromagnetic_Interference},
}

@InProceedings{Nesterova2019,
  author       = {Mariia Nesterova and Marcel Nowak and Franziska Schmidt and Oliver Fischer},
  title        = {Reliability of a bridge with an orthotropic deck exposed to extreme traffic events},
  booktitle    = {11th International Conference on Mathematical Methods in Reliability (MMR 2019)},
  year         = {2019},
  address      = {Hong Kong, China},
  organization = {City University of Hong Kong},
  abstract     = {Predicting reliability levels for critical details of bridges based on limited statistical trafficdata is a relevant topic nowadays. That is why the comparison between results from variousstatistical approaches based on the recorded data for applied traffic actions is the main pointof interest of this work.  The object of the current study is the famous Millau viaduct, acable-stayed bridge with the steel orthotropic deck located in Southern France.  Values ofload effects that are used in analysis are derived from a finite element model of a part ofthe deck.  They are based on data from traffic monitoring that is provided from the bridgeWeigh-In-Motion system covering several months of axle loads, distances and speeds ofheavy trucks.  The methodology is based on a definition of limit state functions based onseveral statistical distributions in order to assess and compare reliability indexes for theultimate  limit  state.   It  includes  a  comparison  between  different  approaches  of  extremevalues theory, the methodology proposed in background works for European standards andthe design load model. Moreover, this work covers the influence of applied loads of a highamplitude, as global effects, onto stresses from axle loads, as local effects.},
  eventdate    = {2016-06-03/2016-06-07},
  file         = {:Nesterova2019 - Reliability of a bridge with an orthotropic deck exposed to extreme traffic events.pdf:PDF},
  keywords     = {Used UQLab, civil engineering},
  review       = {Used UQLab for SORM reliability analysis.},
  url          = {http://infrastar.eu/fileadmin/contributeurs/Infrastar/Outreach_Dissemination/Publications/2019_06_MMR_Nesterova.pdf},
}

@InProceedings{Ahmed2019,
  author    = {Hashmi S. S. Ahmed and Siddhartha Ghosh},
  title     = {Strength characterisation of a {CFS} section with initial geometric imperfections},
  booktitle = {Proceedings of the International Colloquia on Stability and Ductility of Steel Structures (SDSS 2019)},
  year      = {2019},
  pages     = {80},
  address   = {Prague, Czech Republic},
  eventdate = {2019-09-11/2019-09-13},
  keywords  = {civil engineering, used UQLab},
  review    = {used UQLab to create PCE metamodel.},
  url       = {https://books.google.ch/books?hl=en&lr=&id=nSysDwAAQBAJ&oi=fnd&pg=PA80&ots=a8ZaFbplk-&sig=1311sI2ka_xJ01LkqW1J1L3WVF8&redir_esc=y#v=onepage&q&f=false},
}

@Comment{jabref-meta: databaseType:bibtex;}
