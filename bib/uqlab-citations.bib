% Encoding: UTF-8

@Article{Abdallah2016,
  author   = {Imad Abdallah and Anand Natarajan and Jon D. Sørensen},
  title    = {Influence of the control system on wind turbine loads during power production in extreme turbulence: {S}tructural reliability},
  journal  = {Renewable Energy},
  year     = {2016},
  volume   = {87},
  pages    = {464--477},
  abstract = {The wind energy industry is continuously researching better computational models of wind inflow and turbulence to predict extreme loading (the nature of randomness) and their corresponding probability of occurrence. Sophisticated load alleviation control systems are increasingly being designed and deployed to specifically reduce the adverse effects of extreme load events resulting in lighter structures. The main objective herein is to show that despite large uncertainty in the extreme turbulence models, advanced load alleviation control systems yield both a reduction in magnitude and scatter of the extreme loads which in turn translates in a change in the shape of the annual maximum load distribution function resulting in improved structural reliability. Using a probabilistic loads extrapolation approach and the first order reliability method, a large multi-megawatt wind turbine blade and tower structural reliability are assessed when the extreme turbulence model is uncertain. The structural reliability is assessed for the wind turbine when three configurations of an industrial grade load alleviation control system of increasing complexity and performance are used. The load alleviation features include a cyclic pitch, individual pitch, static thrust limiter, condition based thrust limiter and an active tower vibration damper. We show that large uncertainties in the extreme turbulence model can be mitigated and significantly reduced while maintaining an acceptable structural reliability level when advanced load alleviation control systems are used. We end by providing a rational comparison between the long term loads extrapolation method and the environmental contour method for the three control configurations.},
  date     = {2015-11-10},
  doi      = {10.1016/j.renene.2015.10.044},
  file     = {:Abdallah2016 - Influence of the control system on wind turbine loads during power production in extreme turbulence_ Structural reliability.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {84946196677},
  url      = {http://dx.doi.org/10.1016/j.renene.2015.10.044},
}

@InProceedings{Abraham2016,
  author       = {Simon Abraham and Ghader Ghorbaniasl and Chris Lacor},
  booktitle    = {Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS Congress 2016)},
  title        = {A statistical approach for building sparse polynomial chaos expansions},
  year         = {2016},
  address      = {Crete Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  pages        = {6307--6315},
  abstract     = {Over the last years, a lot of effort has been made to make existing uncertainty quantification techniques more efficient in high dimensions. An important class of methods relies on the assumption that the polynomial chaos representation of the model response is sparse. This paper contributes to the validation and assessment of an innovative basis selection technique for building sparse polynomial chaos expansions. A regression approach is used for computing the polynomial chaos coefficients. The technique is based on statistical inference theory which provides information about the true regression model from an estimated regression model based on samples. The latter information is used to build iteratively the sparse polynomial chaos expansion. Using the developed methodology, a more robust and efficient basis selection technique is obtained. For validation purpose, the methodology is applied to high dimensional analytical test cases, including the Oakley & O'Hagan function (d=15) and the Morris function (d=20). The results are compared with those obtained from two state-of-the-art techniques, namely the LARS-based algorithm and compressive sampling. As compared to previous work, more comparisons with the LARS-based method are provided, through the use of UQLab, a MATLAB-based uncertainty quantification framework developed by Sudret and Marelli [1]. It is shown that, with equal settings, the developed methodology results in a more accurate polynomial chaos expansion compared to the aforementioned technique. In addition, a new criterion for building an optimal polynomial chaos expansion is further investigated. The conclusions are in-line with previous findings, i.e. the present criterion always builds a sparser polynomial chaos expansion which is, in addition, at least as accurate as compared to the optimal polynomial chaos expansion obtained from the classical cross validation technique.},
  comment      = {Used for benchmarking purpose.},
  doi          = {10.7712/100016.2259.7412},
  eventdate    = {2016-06-05/2016-06-10},
  file         = {:Abraham2016 - A statistical approach for building sparse polynomial chaos expansions.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {84995530441},
  url          = {http://dx.doi.org/10.7712/100016.2259.7412},
}

@Article{Abraham2017,
  author   = {Simon Abraham and Mehrdad Raisee and Ghader Ghorbaniasl and Francesco Contino and Chris Lacor},
  title    = {A robust and efficient stepwise regression method for building sparse polynomial chaos expansions},
  journal  = {Journal of Computational Physics},
  year     = {2017},
  volume   = {332},
  pages    = {461--474},
  abstract = {Polynomial Chaos (PC) expansions are widely used in various engineering fields for quantifying uncertainties arising from uncertain parameters. The computational cost of classical PC solution schemes is unaffordable as the number of deterministic simulations to be calculated grows dramatically with the number of stochastic dimension. This considerably restricts the practical use of PC at the industrial level. A common approach to address such problems is to make use of sparse PC expansions. This paper presents a non-intrusive regression-based method for building sparse PC expansions. The most important PC contributions are detected sequentially through an automatic search procedure. The variable selection criterion is based on efficient tools relevant to probabilistic method. Two benchmark analytical functions are used to validate the proposed algorithm. The computational efficiency of the method is then illustrated by a more realistic CFD application, consisting of the non-deterministic flow around a transonic airfoil subject to geometrical uncertainties. To assess the performance of the developed methodology, a detailed comparison is made with the well established LAR-based selection technique. The results show that the developed sparse regression technique is able to identify the most significant PC contributions describing the problem. Moreover, the most important stochastic features are captured at a reduced computational cost compared to the LAR method. The results also demonstrate the superior robustness of the method by repeating the analyses using random experimental designs.},
  date     = {2016-12-15},
  doi      = {10.1016/j.jcp.2016.12.015},
  file     = {:Abraham2017 - A robust and efficient stepwise regression method for building sparse polynomial chaos expansions.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85006944223},
  url      = {http://dx.doi.org/10.1016/j.jcp.2016.12.015},
}

@InProceedings{Acikgoz2016,
  author       = {Hulusi Acikgoz and Ravi Kumar Arya and Raj Mittra},
  title        = {Statistical analysis of {3D}-printed flat {GRIN} lenses},
  booktitle    = {2016 IEEE Antennas and Propagation Society International Symposium (APSURSI 2016)},
  year         = {2016},
  pages        = {473--474},
  address      = {Fajardo, Puerto Rico},
  organization = {IEEE},
  abstract     = {This paper presents the statistical analysis of a 3D printed flat lens by using the Polynomial Chaos Expansion (PCE) analysis technique. The flat lens is fabricated using the 3D printing technology and is based on the grading-index (GRIN) approach. It is composed of several concentric rings with graded relative permittivity, made of a single material with different air holes-host material volume ratio. We show that the hole size can have significant effect on the performance of the lens, especially on the focal distance. PCE analysis enables us also to determine the impact of each individual ring on the performance of the lens.},
  doi          = {10.1109/APS.2016.7695945},
  eventdate    = {2016-06-26/2016-07-01},
  file         = {:Acikgoz2016 - Statistical analysis of 3D-printed flat GRIN lenses.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {84997525285},
  url          = {http://dx.doi.org/10.1109/APS.2016.7695945},
}

@Article{Bdour2016,
  author   = {Tarek Bdour and Alain Reineix},
  title    = {Global sensitivity analysis and uncertainty quantification of radiated susceptibility in {PCB} using nonintrusive {P}olynomial {C}haos {E}xpansions},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  year     = {2016},
  pages    = {939--942},
  abstract = {In this paper, we introduce a workflow that uses uncertainty quantification (UQ) and sensitivity analysis (SA) in conjunction with a metamodeling technique called polynomial chaos expansion (PCE) to investigate the stochastic terminal response of a printed circuit board (PCB) due to an external plane wave excitation. The objective of PCE use is to reduce the computational burden of commonly used Monte Carlo (MC) approach for calculating statistical moments and Sobol' sensitivity indices in the presence of uncertain geometrical and electrical parameters of the investigated PCB. The statistical results computed by PCE has been shown to be very accurate compared to MC simulation results.},
  date     = {2016-03-08},
  doi      = {10.1109/TEMC.2016.2535266},
  file     = {:Bdour2016 - Global Sensitivity Analysis and Uncertainty Quantification of Radiated Susceptibility in PCB Using Nonintrusive Polynomial Chaos Expansions.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {84979468030},
  url      = {http://dx.doi.org/10.1109/TEMC.2016.2535266},
}

@InProceedings{Bilicz2016,
  author       = {Sándor Bilicz and Szabolcs Gyimóthy and József Pávó and Péter Horváth and Károly Marák},
  title        = {Uncertainty quantification of wireless power transfer systems},
  booktitle    = {2016 IEEE Wireless Power Transfer Conference (WPTC 2016)},
  year         = {2016},
  address      = {Aveiro, Portugal},
  organization = {IEEE},
  abstract     = {In this paper, the uncertainty of electric properties of wireless power transfer systems due to uncertain geometric design parameters is analyzed. An electromagnetic simulation tool, based on an integral formulation, is coupled with a stochastic method that quantitatively determines the contribution of each uncertain design variable to the output uncertainty in terms of Sobol' indices. A generalized polynomial chaos expansion - being a state-of-art surrogate modeling method - is used to reduce the computational cost involved by these stochastic simulations. The performance of the proposed method is illustrated via the analysis of a resonant wireless power transfer chain.},
  doi          = {10.1109/WPT.2016.7498861},
  eventdate    = {2016-05-05/2016-05-06},
  file         = {:Bilicz2016 - Uncertainty quantification of wireless power transfer systems.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {84979574638},
  url          = {http://dx.doi.org/10.1109/WPT.2016.7498861},
}

@InProceedings{Capellari2016,
  author       = {Giovanni Capellari and Eleni Chatzi and Stefano Mariani},
  title        = {An optimal sensor placement method for {SHM} based on {B}ayesian experimental design and {P}olynomial {C}haos {E}xpansion},
  booktitle    = {Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS Congress 2016)},
  year         = {2016},
  pages        = {6272--6282},
  address      = {Crete Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  abstract     = {We present an optimal sensor placement methodology for structural health monitoring (SHM) purposes, relying on a Bayesian experimental design approach. The unknown structural properties, e.g. the residual strength and stiffness, are inferred from data collected through a network of sensors, whose architecture, i.e., type and position may largely affect the accuracy of the monitoring system. In tackling this issue, an optimal network configuration is herein sought by maximizing the expected information gain between prior and posterior probability distributions of the parameters to be estimated. Since the objective function linked to the network topology cannot be analytically computed, a numerical approximation is provided by means of a Monte Carlo analysis, wherein each realization is obtained via finite element modeling. Since the computational burden linked to this procedure often grows infeasible, a Polynomial Chaos Expansion (PCE) approach is adopted for accelerating the computation of the forward problem. The analysis expands over joint samples covering both structural state and design variables, i.e., sensor locations. Via increase of the number of deployed sensors in the network, the optimization procedure soon turns computationally costly due to the curse of dimensionality. To this end, a stochastic optimization method is adopted for accelerating the convergence of the optimization process and thereby the damage detection capability of the SHM system. The proposed method is applied to thin flexible structures, and the resulting optimal sensor configuration is shown. The effects of the number of training samples, the polynomial degree of the approximation expansion and the optimization settings are also discussed.},
  doi          = {10.7712/100016.2257.6762},
  eventdate    = {2016-06-05/2016-06-10},
  file         = {:Capellari2016 - An optimal sensor placement method for SHM based on Bayesian experimental design and Polynomial Chaos Expansion.pdf:PDF},
  keywords     = {monitoring and remote sensing, used UQLab},
  piis         = {84995471069},
  url          = {http://dx.doi.org/10.7712/100016.2257.6762},
}

@Article{Chen2017,
  author   = {Cheng Chen and Weijie Xu and Tong Guo and Kai Chen},
  title    = {Analysis of actuator delay and its effect on uncertainty quantification for real-time hybrid simulation},
  journal  = {Earthquake Engineering and Engineering Vibration},
  year     = {2017},
  volume   = {16},
  number   = {4},
  pages    = {713--725},
  abstract = {Uncertainties in structure properties can result in different responses in hybrid simulations. Quantification of the effect of these uncertainties would enable researchers to estimate the variances of structural responses observed from experiments. This poses challenges for real-time hybrid simulation (RTHS) due to the existence of actuator delay. Polynomial chaos expansion (PCE) projects the model outputs on a basis of orthogonal stochastic polynomials to account for influences of model uncertainties. In this paper, PCE is utilized to evaluate effect of actuator delay on the maximum displacement from real-time hybrid simulation of a single degree of freedom (SDOF) structure when accounting for uncertainties in structural properties. The PCE is first applied for RTHS without delay to determine the order of PCE, the number of sample points as well as the method for coefficients calculation. The PCE is then applied to RTHS with actuator delay. The mean, variance and Sobol indices are compared and discussed to evaluate the effects of actuator delay on uncertainty quantification for RTHS. Results show that the mean and the variance of the maximum displacement increase linearly and exponentially with respect to actuator delay, respectively. Sensitivity analysis through Sobol indices also indicates the influence of the single random variable decreases while the coupling effect increases with the increase of actuator delay.},
  date     = {2017-11-07},
  doi      = {10.1007/s11803-017-0409-6},
  file     = {:Chen2017 - Analysis of actuator delay and its effect on uncertainty quantification for real-time hybrid simulation.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85033482190},
  url      = {http://dx.doi.org/10.1007/s11803-017-0409-6},
}

@Article{Cheng2018,
  author   = {Kai Cheng and Zhenzhou Lu},
  title    = {Sparse polynomial chaos expansion based on {D-MORPH} regression},
  journal  = {Applied Mathematics and Computation},
  year     = {2018},
  volume   = {323},
  pages    = {17--30},
  abstract = {Polynomial chaos expansion (PCE) is widely used by engineers and modelers in various engineering fields for uncertainty analysis. The computational cost of full PCE is unaffordable for the "curse of dimensionality" of the expansion coefficients. In this paper, a new method for developing sparse PCE is proposed based on the diffeomorphic modulation under observable response preserving homotopy (D-MORPH) algorithm. D-MORPH is a regression technique, it can construct the full PCE models with model evaluations much less than the unknown coefficients. This technique determines the unknown coefficients by minimizing the least-squared error and an objective function. For the purpose of developing sparse PCE, an iterative reweighted algorithm is proposed to construct the objective function. As a result, the objective in D-MORPH regression is converted to minimize the ℓ1 norm of PCE coefficients, and the sparse PCE is established after the proposed algorithm converges to the optimal value. To validate the performance of the developed methodology, several benchmark examples are investigated. The accuracy and efficiency are compared to the well-established least angle regression (LAR) sparse PCE, and results show that the developed method is superior to the LAR-based sparse PCE in terms of efficiency and accuracy.},
  date     = {2017-12-09},
  doi      = {10.1016/j.amc.2017.11.044},
  file     = {:Cheng2018 - Sparse polynomial chaos expansion based on D-MORPH regression.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85037524488},
  url      = {http://dx.doi.org/10.1016/j.amc.2017.11.044},
}

@Article{Chiaramello2017,
  author   = {Emma Chiaramello and Serena Fiocchi and Paolo Ravazzani and Marta Parazzini},
  title    = {Stochastic dosimetry for the assessment of children exposure to uniform 50 {Hz} magnetic field with uncertain orientation},
  journal  = {BioMed Research International},
  year     = {2017},
  abstract = {This study focused on the evaluation of the exposure of children aging from five to fourteen years to 50 Hz homogenous magnetic field uncertain orientation using stochastic dosimetry. Surrogate models allowed assessing how the variation of the orientation of the magnetic field influenced the induced electric field in each tissue of the central nervous system (CNS) and in the peripheral nervous system (PNS) of children. Results showed that the electric field induced in CNS and PNS tissues of children were within the ICNIRP basic restrictions for general public and that no significant difference was found in the level of exposure of children of different ages when considering 10000 possible orientations of the magnetic field. A "mean stochastic model," useful to estimate the level of exposure in each tissue of a representative child in the range of age from five to fourteen years, was developed. In conclusion, this study was useful to deepen knowledge about the ELF-MF exposure, including the evaluation of variable and uncertain conditions, thus representing a step towards a more realistic characterization of the exposure to EMF.},
  date     = {2017-10-31},
  doi      = {10.1155/2017/4672124},
  file     = {:Chiaramello2017 - Stochastic Dosimetry for the Assessment of Children Exposure to Uniform 50 Hz Magnetic Field with Uncertain Orientation.pdf:PDF},
  keywords = {biomedical science, used UQLab},
  piis     = {85042085356},
  url      = {http://dx.doi.org/10.1155/2017/4672124},
}

@Article{Colone2018,
  author   = {Lorenzo Colone and Anand Natarajan and Nikolay Dimitrov},
  title    = {Impact of turbulence induced loads and wave kinematic models on fatigue reliability estimates of offshore wind turbine monopiles},
  journal  = {Ocean Engineering},
  year     = {2018},
  volume   = {155},
  pages    = {295--309},
  abstract = {The cost of offshore wind turbine substructures has a significant impact on competitiveness of the wind energy market and is affected by conservative safety margins adopted in the design phase. This implies that an accurate design load prediction, especially of those resulting in fatigue damage accumulation, may help achieve more cost-effective solutions. In this article, the impact of turbulence and wave loads on fatigue reliability of pile foundations is investigated for a 5-MW offshore wind turbine. Loads obtained by varying turbulence percentiles are compared with those obtained from the full joint probability distribution of wind speed and turbulence through Monte Carlo (MC) simulations, and from the equivalent turbulence level currently adopted by IEC standards. The analyses demonstrate that a lower equivalent turbulence percentile leads to a more realistic and less conservative estimation of fatigue loads. Subsequently, the research focuses on studying the effects of uncertain marine environments on the fatigue load distribution, showing that the latter is insensitive to the random variability of the hydrodynamic coefficients. With respect to the wave kinematic model, a comparison between nonlinear and linear waves clearly suggests that hydrodynamic forces depend significantly on the kinematic model adopted and the operational conditions of the turbine. Furthermore, a term is derived to correct the error introduced by Wheeler stretching at finite water depths. The respective model uncertainties that originate from the nonlinear irregular wave model and Wheeler correction are quantified and employed in a reliability analysis. In a case study, the results are finally compared in terms of estimated probability of failure, with the aim to quantify the influence of environmental models on monopile reliability.},
  date     = {2018-03-23},
  doi      = {10.1016/j.oceaneng.2018.02.045},
  file     = {:Colone2018 - Impact of turbulence induced loads and wave kinematic models on fatigue reliability estimates of offshore wind turbine monopiles.pdf:PDF},
  keywords = {ocean engineering, used UQLab},
  piis     = {85042905073},
  url      = {http://dx.doi.org/10.1016/j.oceaneng.2018.02.045},
}

@InProceedings{Du2017,
  author       = {Jinxin Du and Christophe Roblin},
  title        = {Statistical modeling of the reflection coefficient of deformable antennas},
  booktitle    = {2017 11th European Conference on Antennas and Propagation (EUCAP 2017)},
  year         = {2017},
  pages        = {1928--1932},
  address      = {Paris, France},
  organization = {IEEE},
  abstract     = {A modeling methodology is proposed for characterizing the reflection coefficient S11(f) of narrow band antennas undergoing random disturbances. Firstly, identification techniques are used to get a parsimonious representation of the S11; then the Polynomial Chaos Expansion (PCE) method is used to characterize quantitatively the influence of random disturbances on the compressed S11. The derived S11 model can be used as efficient surrogate for statistical analysis of antennas' frequency behavior. We have applied the proposed methodology to two narrow band antennas - a deformable dipole and a textile patch - in order to demonstrate its performance. Models with good accuracy have been derived for both cases.},
  doi          = {10.23919/EuCAP.2017.7928373},
  eventdate    = {2017-03-19/2017-03-24},
  file         = {:Du2017 - Statistical modeling of the reflection coefficient of deformable antennas.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85020204287},
  url          = {http://dx.doi.org/10.23919/EuCAP.2017.7928373},
}

@Article{Du2017a,
  author   = {Jinxin Du and Christophe Roblin},
  title    = {Statistical modeling of disturbed antennas based on the polynomial chaos expansion},
  journal  = {IEEE Antennas and Wireless Propagation Letters},
  year     = {2017},
  volume   = {16},
  pages    = {1843--1846},
  abstract = {A new methodology of statistical modeling of the far field (FF) radiated by antennas undergoing random disturbances is presented. First, the radiated FF is transformed into a parsimonious form using the spherical modes expansion method (SMEM); then, a surrogate model relating the parsimonious field with the input random parameters is constructed using the polynomial chaos expansion method (PCEM). The combination of the SMEM and PCEM allows developing a compact and precise model with a minimized experimental design cost. The obtained model is computationally costless for generating statistical samples of disturbed antennas easily usable as surrogate models in various types of analyses. In order to demonstrate its performance, the proposed methodology is validated with a deformable canonical antenna-A dipole undergoing three independent random deformations (stretching, bending, and torsion), deriving a compact and precise surrogate model.},
  date     = {2016-09-14},
  doi      = {10.1109/LAWP.2016.2609739},
  file     = {:Du2017a - Statistical Modeling of Disturbed Antennas Based on the Polynomial Chaos Expansion.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85024483492},
  url      = {http://dx.doi.org/10.1109/LAWP.2016.2609739},
}

@Article{Dutta2018,
  author   = {Subhrajit Dutta and Siddhartha Ghosh and Mandar M. Inamdar},
  title    = {Optimisation of tensile membrane structures under uncertain wind loads using {PCE} and {K}riging based metamodels},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  volume   = {57},
  number   = {3},
  pages    = {1149--1161},
  abstract = {Tensile membrane structures (TMS) are light-weight flexible structures that are designed to span long distances with structural efficiency. The stability of a TMS is jeopardised under heavy wind forces due to its inherent flexibility and inability to carry out-of-plane moment and shear. A stable TMS under uncertain wind loads (without any tearing failure) can only be achieved by a proper choice of the initial prestress. In this work, a double-loop reliability-based design optimisation (RBDO) of TMS under uncertain wind load is proposed. Using a sequential polynomial chaos expansion (PCE) and kriging based metamodel, this RBDO reduces the cost of inner-loop reliability analysis involving an intensive finite element solver. The proposed general approach is applied to the RBDO of two benchmark TMS and its computational efficiency is demonstrated through these case studies. The method developed here is suggested for RBDO of large and complex engineering systems requiring costly numerical solution.},
  date     = {2017-09-08},
  doi      = {10.1007/s00158-017-1802-5},
  file     = {:Dutta2018 - Optimisation of tensile membrane structures under uncertain wind loads using PCE and kriging based metamodels.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85029009303},
  url      = {http://dx.doi.org/10.1007/s00158-017-1802-5},
}

@Article{Erten2016,
  author   = {Esra Erten and Juan M. Lopez-Sanchez and Onur Yuzugullu and Irena Hajnsek},
  title    = {Retrieval of agricultural crop height from space: {A} comparison of {SAR} techniques},
  journal  = {Remote Sensing of Environment},
  year     = {2016},
  volume   = {187},
  pages    = {130--144},
  abstract = {This paper deals with the retrieval of agricultural crop height from space by using multipolarization Synthetic Aperture Radar (SAR) images. Coherent and incoherent crop height estimation methods are discussed for the first time with a unique TanDEM-X dataset acquired over rice cultivation areas. Indeed, with its polarimetric and interferometric capabilities, the TanDEM-X mission enables the tracking of crop height through interferometric SAR (InSAR), polarimetric interferometric SAR (PolInSAR) and the inversion of radiative transfer-based backscattering model. The paper evaluates the three aforementioned techniques simultaneously with a data set acquired in September 2014 and 2015 over rice fields in Turkey during their reproductive stage. The assessment of the absolute height accuracy and the limitations of the approaches are provided. In-situ measurements conducted in the same cultivation periods are used for validation purposes. The PolInSAR and morphological backscattering model results showed better performance with low RMSEs (12 and 13 cm) compared to the differential InSAR result having RMSE of 18 cm. The spatial baseline, i.e. the distance between satellites, is a key parameter for coherent methods such as InSAR and PolInSAR. Its effect on the absolute height accuracy is discussed using TanDEM-X pairs separated by a baseline of 101.7m and 932m. Although the InSAR based approach is demonstrated to provide sufficient crop height accuracy, the availability of a precise vegetation-free digital elevation model and a structurally dense crop are basic requirements for achieving high accuracy. The PolInSAR approach provides reliable crop height estimation if the spatial baseline is large enough for the inversion. The impact of increasing spatial baseline on the absolute accuracy of the crop height estimation is evident for both methods. However, PolInSAR is more cost-efficient, e.g. there is no need for phase unwrapping and any external vegetation free surface elevation data. Instead, the usage of radiative transfer based backscattering models provides not only crop height but also other biophysical properties of the crops with consistent accuracy. The efficient retrieval of crop height with backscattering model is achieved by metamodelling, which makes the computational cost of backscattering inversion comparable to the ones of the coherent methods. However, effectiveness depends on not only the backscattering model, but also the integration of agronomic crop growth rules. Motivated by these results, a combination of backscattering and PolInSAR inversion models would provide a successful method of future precision farming studies.},
  date     = {2016-10-14},
  doi      = {10.1016/j.rse.2016.10.007},
  file     = {:Erten2016 - Retrieval of agricultural crop height from space_ A comparison of SAR techniques.pdf:PDF},
  keywords = {sensors engineering, used UQLab},
  piis     = {84991593765},
  url      = {http://dx.doi.org/10.1016/j.rse.2016.10.007},
}

@InProceedings{Erten2016a,
  author       = {Esra Erten and Onur Yuzugullu and Juan M. Lopez-Sanchez and Irena Hajnsek},
  title        = {{SAR} algorithms for crop height estimation: {T}he paddy-rice case study},
  booktitle    = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  year         = {2016},
  pages        = {7117--7120},
  address      = {Beijing, China},
  organization = {IEEE},
  abstract     = {This paper presents a study of the sensibility of the incoherent (electromagnetic backscattering model) and coherent (DInSAR and PolInSAR inversion) crop height estimation methods of SAR imaging. The methods were compared for paddy-rice crop height monitoring with a TanDEM-X dataset. For this, rice-cultivated agricultural fields located in Northern Turkey were selected. Intensive ground data collection during the cultivation period in 2015 was carried out. The accuracy analysis showed that the requirement of external (vegetation-free) DEM in DInSAR-based crop height estimation decreases its performance compared to the PolInSAR and backscattering inversion methods.},
  doi          = {10.1109/IGARSS.2016.7730857},
  eventdate    = {2016-07-10/2016-07-15},
  file         = {:Erten2016a - SAR algorithms for crop height estimation_ The paddy-rice case study.pdf:PDF},
  keywords     = {monitoring and remote sensing, used UQLab},
  piis         = {85007425666},
  url          = {http://dx.doi.org/10.1109/IGARSS.2016.7730857},
}

@Article{Tan2018,
  author   = {Fengjie Tan and Tom Lahmer},
  title    = {Shape optimization based design of arch-type dams under uncertainties},
  journal  = {Engineering Optimization},
  year     = {2018},
  volume   = {50},
  number   = {9},
  pages    = {1470--1482},
  abstract = {Comparing existing design methodologies for arch-type dams, model-based shape optimization can effectively reduce construction costs and leverage the properties of construction materials. To apply means of shape optimization, suitable variables need to be chosen to formulate the objective function, which is here the volume of the arch dam. A genetic algorithm is adopted as the optimization method, which allows a global search. The reliability index is considered as the main constraint. Its computation is realized by adaptive Kriging Monte Carlo simulation, which visibly increases the analysis efficiency compared with traditional Monte Carlo simulations. Constraints, such as the reliability index and further with respect to the geometry, are taken into consideration by a penalty formulation. By means of this approach, a reliability-based design can be found which ensures both the safety and serviceability of a newly designed arch-type dam.},
  date     = {2017-12-18},
  doi      = {10.1080/0305215X.2017.1409348},
  file     = {:Tan2018 - Shape optimization based design of arch-type dams under uncertainties.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85038396163},
  url      = {http://dx.doi.org/10.1080/0305215X.2017.1409348},
}

@InCollection{Gaspar2016,
  author    = {Bruno Gaspar and Carlos Guedes Soares and Ehsan Bahmyari and Mohammad Reza Khedmati},
  title     = {Application of polynomial chaos expansions in stochastic analysis of plate elements under lateral pressure},
  booktitle = {Maritime Technology and Engineering 3},
  publisher = {Taylor \& Francis Group},
  year      = {2016},
  pages     = {471--480},
  isbn      = {978-1-138-03000-8},
  abstract  = {An application of the polynomial chaos expansion technique in the stochastic analysis of plate elements under lateral pressure is presented. An adaptive non-intrusive technique based on regression is adopted to define sparse polynomial chaos expansion representations. The analysis is performed considering as response quantity of interest the maximum deflection of a steel rectangular plate element under lateral pressure representative of a ship bottom plate element. The material modulus of elasticity is considered to be a spatially varying property represented by a random field. The plate numerical model is based on the classical theory of thin plates, which is solved numerically using the element free Galerkin method. The accuracy and efficiency of the adaptive non-intrusive sparse polynomial chaos expansion technique adopted is demonstrated in the paper for different correlation lengths of the random field.},
  file      = {:Gaspar2016 - Application of polynomial chaos expansions in stochastic analysis of plate elements under lateral pressure.pdf:PDF},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016740809},
  url       = {https://www.researchgate.net/publication/304944256_Application_of_polynomial_chaos_expansions_in_stochastic_analysis_of_plate_elements_under_lateral_pressure},
}

@InCollection{Gaspar2016a,
  author    = {Bruno Gaspar and A. P. Teixeira and Carlos Guedes Soares},
  title     = {Sensitivity analysis of the {IACS-CSR} buckling strength requirements for stiffened panels},
  booktitle = {Maritime Technology and Engineering 3},
  publisher = {Taylor \& Francis Group},
  year      = {2016},
  pages     = {459--470},
  abstract  = {A sensitivity analysis of the IACS-CSR buckling strength requirements for stiffened panels under uniaxial compression is presented. The buckling strength requirements considered are defined on the basis of semi-empirical design formulations that account explicitly for three failure modes of the stiffened panel: uniaxial buckling of the plating between stiffeners, column buckling of stiffeners with attached plating and lateral-torsional buckling or tripping of stiffeners. A sample of deck stiffened panels obtained from a representative sample of double hull oil tanker designs is considered in the sensitivity analysis. Different local and global sensitivity analysis methods are used to compute sensitivity indices for the different stiffened panel designs and failure modes. The most important variables with respect to the contribution to the critical buckling stress variability are identified and the different local and global sensitivity indices compared.},
  file      = {:Gaspar2016a - Sensitivity analysis of the IACS-CSR buckling strength requirements for stiffened panels.pdf:PDF},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016821176},
  url       = {https://www.researchgate.net/publication/305872936_Sensitivity_analysis_of_the_IACS-CSR_buckling_strength_requirements_for_stiffened_panels_Proceedings_of_the_3rd_International_Conference_on_Maritime_Technology_and_Engineering_MARTECH_2016_Lisbon_Port},
}

@Article{Hamdi2017,
  author   = {Hamidreza Hamdi and Ivo Couckuyt and Mario Costa Sousa and Tom Dhaene},
  title    = {{G}aussian {P}rocesses for history-matching: application to an unconventional gas reservoir},
  journal  = {Computational Geosciences},
  year     = {2017},
  volume   = {21},
  number   = {2},
  pages    = {267--287},
  abstract = {The process of reservoir history-matching is a costly task. Many available history-matching algorithms either fail to perform such a task or they require a large number of simulation runs. To overcome such struggles, we apply the Gaussian Process (GP) modeling technique to approximate the costly objective functions and to expedite finding the global optima. A GP model is a proxy, which is employed to model the input-output relationships by assuming a multi-Gaussian distribution on the output values. An infill criterion is used in conjunction with a GP model to help sequentially add the samples with potentially lower outputs. The IC fault model is used to compare the efficiency of GP-based optimization method with other typical optimization methods for minimizing the objective function. In this paper, we present the applicability of using a GP modeling approach for reservoir history-matching problems, which is exemplified by numerical analysis of production data from a horizontal multi-stage fractured tight gas condensate well. The results for the case that is studied here show a quick convergence to the lowest objective values in less than 100 simulations for this 20-dimensional problem. This amounts to an almost 10 times faster performance compared to the Differential Evolution (DE) algorithm that is also known to be a powerful optimization technique. The sensitivities are conducted to explain the performance of the GP-based optimization technique with various correlation functions.},
  date     = {2017-01-21},
  doi      = {10.1007/s10596-016-9611-2},
  file     = {:Hamdi2017 - Gaussian Processes for history-matching_ application to an unconventional gas reservoir.pdf:PDF},
  keywords = {geoscience, used UQLab},
  piis     = {85009932344},
  url      = {http://dx.doi.org/10.1007/s10596-016-9611-2},
}

@Article{Hariri-Ardebili2018,
  author   = {Mohammad Amin Hariri-Ardebili and Farhad Pourkamali-Anaraki},
  title    = {Simplified reliability analysis of multi hazard risk in gravity dams via machine learning techniques},
  journal  = {Archives of Civil and Mechanical Engineering},
  year     = {2018},
  volume   = {18},
  number   = {2},
  pages    = {592--610},
  abstract = {Deterministic analysis does not provide a comprehensive model for concrete dam response under multi-hazard risk. Thus, the use of probabilistic approach is usually recommended which is problematic due to high computational demand. This paper presents a simplified reliability analysis framework for gravity dams subjected to flooding, earthquakes, and aging. A group of time-variant degradation models are proposed for different random variables. Response of the dam is presented by explicit limit state functions. The probability of failure is directly computed by either classical Monte Carlo simulation or the refined importance sampling technique. Next, three machine learning techniques (i.e., K-nearest neighbor, support vector machine, and naive Bayes classifier) are adopted for binary classification of the structural results. These methods are then demonstrated in terms of accuracy, applicability and computational time for prediction of the failure probability. Results are then generalized for different dam classes (based on the height-to-width ratio), various water levels, earthquake intensity, degradation rate, and cross-correlation between the random variables. Finally, a sigmoid-type function is proposed for analytical calculation of the failure probability for different classes of gravity dams. This function is then specialized for the hydrological hazard and the failure surface is presented as a direct function of the dam's height and width.},
  date     = {2017-11-09},
  doi      = {10.1016/j.acme.2017.09.003},
  file     = {:Hariri-Ardebili2018 - Simplified reliability analysis of multi hazard risk in gravity dams via machine learning techniques.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {85033218669},
  url      = {http://dx.doi.org/10.1016/j.acme.2017.09.003},
}

@InProceedings{Hashemian2017,
  author       = {Raoufehsadat Hashemian and Niklas Carlsson and Diwakar Krishnamurthy and Martin Arlitt},
  booktitle    = {Proceedings of the 2017 ACM/SPEC International Conference on Performance Engineering (ICPE 2017)},
  title        = {Iris: {I}te{R}ative and {I}ntelligent {E}xperiment {S}election},
  year         = {2017},
  address      = {L'Aquila, Italy},
  organization = {Association for Computing Machinery (ACM)},
  pages        = {143--154},
  abstract     = {Benchmarking is a widely-used technique to quantify the performance of software systems. However, the design and implementation of a benchmarking study can face several challenges. In particular, the time required to perform a benchmarking study can quickly spiral out of control, owing to the number of distinct variables to systematically examine. In this paper, we propose IRIS, an IteRative and Intelligent Experiment Selection methodology, to maximize the information gain while minimizing the duration of the benchmarking process. IRIS selects the region to place the next experiment point based on the variability of both dependent, i.e., response, and independent variables in that region. It aims to identify a performance function that minimizes the response variable prediction error for a constant and limited experimentation budget. We evaluate IRIS for a wide selection of experimental, simulated and synthetic systems with one, two and three independent variables. Considering a limited experimentation budget, the results show IRIS is able to reduce the performance function prediction error up to 4:3 times compared to equal distance experiment point selection. Moreover, we show that the error reduction can further improve through system-specific parameter tuning. Analysis of the error distributions obtained with IRIS reveals that the technique is particularly effective in regions where the response variable is sensitive to changes in the independent variables.},
  comment      = {Used Kriging module of UQLab},
  doi          = {10.1145/3030207.3030225},
  eventdate    = {2017-04-22/2017-04-26},
  file         = {:Hashemian2017 - Iris_ IteRative and Intelligent Experiment Selection.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85019034357},
  url          = {http://dx.doi.org/10.1145/3030207.3030225},
}

@Article{Kaintura2017,
  author   = {A. Kaintura and D. Spina and I. Couckuyt and L. Knockaert and W. Bogaerts and T. Dhaene},
  title    = {A {K}riging and {S}tochastic {C}ollocation ensemble for uncertainty quantification in engineering applications},
  journal  = {Engineering with Computers},
  year     = {2017},
  volume   = {33},
  number   = {4},
  pages    = {935--949},
  abstract = {We propose a new surrogate modeling approach by combining two non-intrusive techniques: Kriging and Stochastic Collocation. The proposed method relies on building a sufficiently accurate Stochastic Collocation model which acts as a basis to construct a Kriging model on the residuals, to combine the accuracy and efficiency of Stochastic Collocation methods in describing stochastic quantities with the flexibility and modeling power of Kriging-based approaches. We investigate and compare performance of the proposed approach with state-of-art techniques over benchmark problems and practical engineering examples on various experimental designs.},
  date     = {2017-03-17},
  doi      = {10.1007/s00366-017-0507-0},
  file     = {:Kaintura2017 - A Kriging and Stochastic Collocation ensemble for uncertainty quantification in engineering applications.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85015702883},
  url      = {http://dx.doi.org/10.1007/s00366-017-0507-0},
}

@InProceedings{Larbi2017,
  author       = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Crosstalk analysis of printed circuits with many uncertain parameters using sparse polynomial chaos metamodels},
  booktitle    = {2017 International Symposium on Electromagnetic Compatibility (EMC EUROPE 2017)},
  year         = {2017},
  address      = {Angers, France},
  organization = {IEEE},
  abstract     = {This paper presents a metamodel based on the sparse polynomial chaos approach, well adapted to high-dimensional uncertainty quantification problems, applied for the analysis of crosstalk in printed circuit board microstrip traces. It enables to estimate, with a low computational cost compared to Monte Carlo (MC) simulation, statistical quantities and provides a sensitivity analysis of the crosstalk effects considering numerous uncertain variables. The approach is validated against MC simulation and shows a good efficiency and accuracy.},
  doi          = {10.1109/EMCEurope.2017.8094623},
  eventdate    = {2017-09-04/2017-09-07},
  file         = {:Larbi2017 - Crosstalk analysis of printed circuits with many uncertain parameters using sparse polynomial chaos metamodels.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85040624369},
  url          = {http://dx.doi.org/10.1109/EMCEurope.2017.8094623},
}

@InProceedings{Larbi2017a,
  author       = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Analysis of a printed circuit board with many uncertain variables by sparse polynomial chaos},
  booktitle    = {2017 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization for RF, Microwave, and Terahertz Applications (NEMO 2017)},
  year         = {2017},
  pages        = {323--325},
  address      = {Seville, Spain},
  organization = {IEEE},
  abstract     = {This communication deals with the uncertainty quantification in high dimensional problems. It introduces a metamodel based on the sparse polynomial chaos for the analysis of a printed circuit board, depending on many uncertain variables. This metamodel allows to estimate statistical quantities of an output with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained have been validated by comparison with MC simulation.},
  doi          = {10.1109/NEMO.2017.7964274},
  eventdate    = {2017-05-17/2017-05-19},
  file         = {:Larbi2017a - Analysis of a printed circuit board with many uncertain variables by sparse polynomial chaos.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85028511104},
  url          = {https://dx.doi.org/10.1109/NEMO.2017.7964274},
}

@Article{Larbi2018,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {Variability impact of many design parameters: {T}he case of a realistic electronic link},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  year     = {2018},
  volume   = {60},
  number   = {1},
  pages    = {34--41},
  abstract = {In this paper, we adopt the so-called sparse polynomial chaos metamodel for the uncertainty quantification in the framework of high-dimensional problems. This metamodel is used to model a realistic electronic bus structure with a large number of uncertain input parameters such as those related to microstrip line geometries. It aims at estimating quantities of interest, such as statistical moments, probability density functions, and provides sensitivity analysis of a response. It drastically reduces the model computational cost with regard to brute force Monte Carlo (MC) simulation. The method presents a good performance and is validated in comparison with MC simulation.},
  date     = {2017-07-25},
  doi      = {10.1109/TEMC.2017.2727961},
  file     = {:Larbi2018 - Variability Impact of Many Design Parameters_ The Case of a Realistic Electronic Link.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85028942396},
  url      = {http://dx.doi.org/10.1109/TEMC.2017.2727961},
}

@Article{Larbi2018a,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {Identification of main factors of uncertainty in a microstrip line network},
  journal  = {Progress in Electromagnetics Research},
  year     = {2018},
  volume   = {162},
  pages    = {61--72},
  abstract = {This paper deals with uncertainty propagation applied to the analysis of crosstalk in printed circuit board microstrip traces. Complex interconnection networks generally are affected by many uncertain parameters and their point-to-point transfer functions are computationally expensive, thus making Monte-Carlo analyses rather inefficient. To overcome this situation, a metamodel is highly desirable. This paper presents a sparse and accelerated polynomial chaos approach, which proves to be well adapted for high-dimensional uncertainty quantification and well suited for the sensitivity analysis of crosstalk effects. We highlight the significant advantage of the advocated approach for the design of microstrip line networks of complex topology. In fact, we demonstrate how a small number of system simulations can help to quantify the statistics of the output variability and identify a reduced set of high-impact parameters.},
  date     = {2018-06-13},
  doi      = {10.2528/PIER18040607},
  file     = {:Larbi2018a - Identification of main factors of uncertainty in a microstrip line network.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85049190395},
  url      = {http://dx.doi.org/10.2528/PIER18040607},
}

@InProceedings{Li2017,
  author       = {Meng Li and Yinghong Wen and Jinbao Zhang and Dan Zhang},
  title        = {An {EMC} safety assessment model to analyze complex system in high speed railways},
  booktitle    = {Proceedings of the 2017 19th International Conference on Electromagnetics in Advanced Applications (ICEAA 2017)},
  year         = {2017},
  pages        = {626--629},
  address      = {Verona, Italy},
  organization = {IEEE},
  abstract     = {This paper presents a safety assessment model with regard to electromagnetic compatibility (EMC) fault suitable for estimating the EMC failure probability (EMC FP) of the complex system facing uncertain electromagnetic environment. First, the model is successfully applied to decompose the complex system into different independent subsystem and equipment. Then the practical EMI scenario is modeled as different typical EMC problem, such as the Plane Wave Coupling and Crosstalk respectively. After that the fault tree analysis (FTA) is proposed to use in combination with electromagnetic topology (EMT) calculating the EMC FP of the system. A focus is put on the practical case of the Desktop Management Interface (DMI) system. In this context, a system-level EMC safety assessment model has been applied to solve EMC FP by uncertainty analysis in high speed railways with the complex electromagnetic environment.},
  doi          = {10.1109/ICEAA.2017.8065324},
  eventdate    = {2017-09-11/2017-09-15},
  file         = {:Li2017 - An EMC safety assessment model to analyze complex system in high speed railways.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  piis         = {85035133738},
  url          = {http://dx.doi.org/10.1109/ICEAA.2017.8065324},
}

@Article{Mentani2016,
  author   = {Alessio Mentani and Laura Govoni and Guido Gottardi and Stéphane Lambert and Franck Bourrier and David Toe},
  journal  = {Procedia Engineering},
  title    = {A new approach to evaluate the effectiveness of rockfall barriers},
  year     = {2016},
  pages    = {398--403},
  volume   = {158},
  abstract = {The paper addresses the response of a semi-rigid rockfall protection barrier using numerical models. The study show a large dependence of the barrier response to the impact conditions. The block size and impact position, rather than the velocity direction and magnitude induce different modes of failure of the fence, which in turn result in different values of failure energy. As a result, the barrier capacity cannot be established in a deterministic way. The effectiveness of structures as such can be more successfully evaluated through a reliability probabilistic approach. Results can be used to create a meta-model of the barrier response which can be incorporated into rockfall simulation models, enabling a reliable and comprehensive design of rockfall mitigation interventions performed with this type of structure.},
  address  = {Bologna, Italy},
  comment  = {Used PCE in UQLab.},
  doi      = {10.1016/j.proeng.2016.08.462},
  file     = {:Mentani2016 - A New Approach to Evaluate the Effectiveness of Rockfall Barriers.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {84988353699},
  url      = {http://dx.doi.org/10.1016/j.proeng.2016.08.462},
}

@InProceedings{Mylonas2017,
  author       = {Charilaos Mylonas and Bemetz Valentin and Eleni Chatzi},
  title        = {Multiscale surrogate modeling and uncertainty quantification for periodic composite structures},
  booktitle    = {Proceedings of the 2nd International Conference on Uncertainty Quantification in Computational Sciences and Engineering (UNCECOMP 2017)},
  year         = {2017},
  pages        = {406--418},
  address      = {Rhodes Island, Greece},
  organization = {European Community on Computational Methods in Applied Sciences (ECCOMAS)},
  abstract     = {Computational modeling of the structural behavior of continuous fiber composite materials often takes into account the periodicity of the underlying micro-structure. A well established method dealing with the structural behavior of periodic micro-structures is the socalled Asymptotic Expansion Homogenization (AEH). By considering a periodic perturbation of the material displacement, scale bridging functions, also referred to as elastic correctors, can be derived in order to connect the strains at the level of the macro-structure with microstructural strains. For complicated inhomogeneous micro-structures, the derivation of such functions is usually performed by the numerical solution of a PDE problem -Typically with the Finite Element Method. Moreover, when dealing with uncertain micro-structural geometry and material parameters, there is considerable uncertainty introduced in the actual stresses experienced by the materials. Due to the high computational cost of computing the elastic correctors, the choice of a pure Monte-Carlo approach for dealing with the inevitable material and geometric uncertainties is clearly computationally intractable. This problem is even more pronounced when the effect of damage in the micro-scale is considered, where re-evaluation of the micro-structural representative volume element is necessary for every occurring damage. The novelty in this paper is that a non-intrusive surrogate modeling approach is employed with the purpose of directly bridging the macro-scale behavior of the structure with the material behavior in the micro-scale, therefore reducing the number of costly evaluations of corrector functions, allowing for future developments on the incorporation of fatigue or static damage in the analysis of composite structural components.},
  doi          = {10.7712/120217.5379.16904},
  eventdate    = {2017-06-15/2017-06-17},
  file         = {:Mylonas2017 - Multiscale surrogate modeling and uncertainty quantification for periodic composite structures.pdf:PDF},
  keywords     = {civil engineering, used UQLab},
  piis         = {85043484343},
  url          = {http://dx.doi.org/10.7712/120217.5379.16904},
}

@InProceedings{Mylonas2017a,
  author       = {Charilaos Mylonas and Imad Abdallah and Eleni Chatzi},
  title        = {Surrogate modelling for fatigue damage of wind-turbine blades using polynomial chaos expansions and non-negative matrix factorization},
  booktitle    = {IABSE Symposium Vancouver 2017: Engineering the Future},
  year         = {2017},
  pages        = {809--816},
  address      = {Vancouver, British Columbia, Canada},
  organization = {International Association for Bridge and Structural Engineering (IABSE)},
  abstract     = {A computational approach for the estimation of fatigue degradation of composite wind turbine blades by means of time domain aero-servo-elastic simulations is proposed. Wind turbine blades are subjected throughout their lifetime to highly stochastic loading. Fatigue damage of the composite reinforcement of the wind turbine blades has been identified early on in the wind turbine design practice as a factor driving design. A simple fatigue accumulation model is utilized for the spar cap reinforcement of a wind-turbine blade. Non-Negative Matrix Factorization (NMF) for the damage accumulation random field is used for dimensionality reduction. An approximate computationally efficient model, relying on Polynomial Chaos Expansion (PCE) of the damage state with respect to probabilistically modelled mean wind and turbulence intensity is derived. The framework is exemplified in a case-study of a 1.5MW wind turbine.},
  eventdate    = {2017-09-19/2017-09-19},
  keywords     = {civil engineering, used UQLab},
  piis         = {85050026817},
  url          = {https://www.ingentaconnect.com/contentone/iabse/report/2017/00000109/00000058/art00001},
}

@Article{Ni2016,
  author   = {Fei Ni and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {Basis-adaptive sparse polynomial chaos expansion for probabilistic power flow},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2016},
  volume   = {32},
  number   = {1},
  pages    = {694--704},
  abstract = {This paper introduces the basis-adaptive sparse polynomial chaos (BASPC) expansion to perform the probabilistic power flow (PPF) analysis in power systems. The proposed method takes advantage of three state-of-the-art uncertainty quantification methodologies reasonably: the hyperbolic scheme to truncate the infinite polynomial chaos (PC) series; the least angle regression (LARS) technique to select the optimal degree of each univariate PC series; and the Copula to deal with nonlinear correlations among random input variables. Consequently, the proposed method brings appealing features to PPF, including the ability to handle the large-scale uncertainty sources; to tackle the nonlinear correlation among the random inputs; to analytically calculate representative statistics of the desired outputs; and to dramatically alleviate the computational burden as of traditional methods. The accuracy and efficiency of the proposed method are verified through either quantitative indicators or graphical results of PPF on both the IEEE European Low Voltage Test Feeder and the IEEE 123 Node Test Feeder, in the presence of more than 100 correlated uncertain input variables.},
  date     = {2016-04-27},
  doi      = {10.1109/TPWRS.2016.2558622},
  file     = {:Ni2016 - Basis-Adaptive Sparse Polynomial Chaos Expansion for Probabilistic Power Flow.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85008659770},
  url      = {http://dx.doi.org/10.1109/TPWRS.2016.2558622},
}

@Article{Ni2017,
  author   = {Fei Ni and Michiel Nijhuis and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {Variance-based global sensitivity analysis for power systems},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2017},
  volume   = {33},
  number   = {2},
  pages    = {1670--1682},
  abstract = {Knowledge of the impact of uncertain inputs is valuable, especially in power systems with large amounts of stochastic renewable generations. A global sensitivity analysis (GSA) can determine the impact of input uncertainties on the output quantity of interest in a certain physical or mathematical model. The GSA has not been widely employed in power systems due to the prohibitively computational burden. In this paper, it is demonstrated that, via the implementation of a basis-Adaptive sparse polynomial chaos expansion, a GSA can be applied to the power system with numerous uncertain inputs. The performance of the proposed method is tested on both the IEEE 13-bus test feeder and the IEEE 123-node test system, in presence of a large amount of independent or correlated uncertain inputs. The possible application of a GSA on the basis of the basis-Adaptive sparse polynomial chaos expansion in power systems are discussed in terms of various sensitivities. The findings cannot only be used to rank the most influential input uncertainties with respect to a specific output, such as variances of the nodal power, but also to identify the most sensitive or robust electrical variables such as the bus voltage with respect to input uncertainties.},
  date     = {2017-06-27},
  doi      = {10.1109/TPWRS.2017.2719046},
  file     = {:Ni2017a - Variance-Based Global Sensitivity Analysis for Power Systems.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  piis     = {85023758823},
  url      = {http://dx.doi.org/10.1109/TPWRS.2017.2719046},
}

@InProceedings{Palar2017,
  author       = {Pramudita Satria Palar and Koji Shimoyama},
  title        = {Polynomial-chaos-kriging-assisted efficient global optimization},
  booktitle    = {2017 IEEE Symposium Series on Computational Intelligence (SSCI 2017)},
  year         = {2017},
  pages        = {1--8},
  address      = {Honolulu, Hawaii, USA},
  organization = {IEEE},
  abstract     = {In this paper, we explore the use of the recently proposed polynomial chaos-Kriging (PCK) surrogate model to assist a single-objective efficient global optimization (EGO) framework in order to solve expensive optimization problems. PCK is a form of universal Kriging (UK) that employs orthogonal polynomials and least-angle-regression (LARS) algorithm to select the proper set of polynomial basis. The use of LARS within the PCK algorithm eliminates the need for the manual selection of UK's trend function. Investigation on the capability of PCK-EGO is performed on five synthetic and one aerodynamic test problems. In light of the results, we observe that PCK-EGO performs in a similar way to standard EGO in cases with no clear polynomial-like trend. However, PCK-EGO shows a notable faster convergence in problems where the objective function exhibits a landscape trend that can be captured by polynomials. Application to the subsonic wing problem further demonstrates that PCK-EGO is more efficient than EGO in a real-world aerodynamic optimization problem.},
  doi          = {10.1109/SSCI.2017.8280831},
  eventdate    = {2017-11-27/2017-12-01},
  file         = {:Palar2017 - Polynomial-chaos-kriging-assisted efficient global optimization.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85046164050},
  url          = {http://dx.doi.org/10.1109/SSCI.2017.8280831},
}

@InProceedings{Palar2018,
  author       = {Pramudita Satria Palar and Koji Shimoyama},
  title        = {Ensemble of kriging with multiple kernel functions for engineering design optimization},
  booktitle    = {International Conference on Bioinspired Methods and Their Applications (BIOMA 2018)},
  year         = {2018},
  pages        = {211--222},
  address      = {Paris, France},
  organization = {University of Lille and Jožef Stefan Institute},
  abstract     = {We introduce the ensemble of Kriging with multiple kernel functions guided by cross-validation error for creating a robust and accurate surrogate model to handle engineering design problems. By using the ensemble of Kriging models, the resulting ensemble model preserves the uncertainty structure of Kriging, thus, can be further exploited for Bayesian optimization. The objective of this paper is to develop a Kriging methodology that eliminates the needs for manual kernel selection which might not be optimal for a specific application. Kriging models with three kernel functions, that is, Gaussian, Matérn-3/2, and Matérn-5/2 are combined through a global and a local ensemble technique where their approximation quality are investigated on a set of aerodynamic problems. Results show that the ensemble approaches are more robust in terms of accuracy and able to perform similarly to the best performing individual kernel function or avoiding misspecification of kernel.},
  doi          = {10.1007/978-3-319-91641-5_18},
  eventdate    = {2018-05-16/2018-05-18},
  file         = {:Palar2018 - Ensemble of kriging with multiple kernel functions for engineering design optimization.pdf:PDF},
  keywords     = {computational science and engineering, used UQLab},
  piis         = {85047468928},
  url          = {http://dx.doi.org/10.1007/978-3-319-91641-5_18},
}

@Article{Perko2016,
  author   = {Zoltán Perkó and Sebastian R. Van Der Voort and Steven Van De Water and Charlotte M. H. Hartman and Mischa Hoogeman and Danny Lathouwers},
  title    = {Fast and accurate sensitivity analysis of {IMPT} treatment plans using {P}olynomial {C}haos {E}xpansion},
  journal  = {Physics in Medicine and Biology},
  year     = {2016},
  volume   = {61},
  pages    = {4646--4664},
  abstract = {The highly conformal planned dose distribution achievable in intensity modulated proton therapy (IMPT) can severely be compromised by uncertainties in patient setup and proton range. While several robust optimization approaches have been presented to address this issue, appropriate methods to accurately estimate the robustness of treatment plans are still lacking. To fill this gap we present Polynomial Chaos Expansion (PCE) techniques which are easily applicable and create a meta-model of the dose engine by approximating the dose in every voxel with multidimensional polynomials. This Polynomial Chaos (PC) model can be built in an automated fashion relatively cheaply and subsequently it can be used to perform comprehensive robustness analysis. We adapted PC to provide among others the expected dose, the dose variance, accurate probability distribution of dose-volume histogram (DVH) metrics (e.g. minimum tumor or maximum organ dose), exact bandwidths of DVHs, and to separate the effects of random and systematic errors. We present the outcome of our verification experiments based on 6 head-and-neck (HN) patients, and exemplify the usefulness of PCE by comparing a robust and a non-robust treatment plan for a selected HN case. The results suggest that PCE is highly valuable for both research and clinical applications.},
  date     = {2016-05-26},
  doi      = {10.1088/0031-9155/61/12/4646},
  file     = {:Perko2016 - Fast and accurate sensitivity analysis of IMPT treatment plans using Polynomial Chaos Expansion.pdf:PDF},
  keywords = {biomedical science, used UQLab},
  piis     = {84975041431},
  url      = {http://dx.doi.org/10.1088/0031-9155/61/12/4646},
}

@Article{Sanctis2016,
  author   = {Gianluca De Sanctis and Mario Fontana},
  title    = {Risk-based optimisation of fire safety egress provisions based on the {LQI} acceptance criterion},
  journal  = {Reliability Engineering and System Safety},
  year     = {2016},
  volume   = {152},
  pages    = {339--350},
  abstract = {Life safety is the primary objective of fire safety design and is mainly ensured by an appropriate design of the means of egress, which reduce the risk to life. An unlimited reduction of this risk is not desirable since it would lead to an immense investment of resources. The Life Quality Index (LQI) acceptance criterion is a societal indicator that is used to judge the efficiency of safety measures to reduce the risk to life and can be used to optimise the investments into life saving measures. The risk is assessed by a probabilistic engineering approach that considers the fire development and the individual-based evacuation process as well as the associated uncertainties. Advanced uncertainty propagation methods are applied in order to face the difficulties by using computational expensive models and by using individual-based models. Sensitivity measures are applied to reveal the contribution of the uncertainties on the estimation of the risk. The approach is applied for a risk-based optimisation of the minimal required door width for retail buildings.},
  date     = {2016-04-09},
  doi      = {10.1016/j.ress.2016.04.001},
  file     = {:Sanctis2016 - Risk-based optimisation of fire safety egress provisions based on the LQI acceptance criterion.pdf:PDF},
  keywords = {civil engineering, used UQLab},
  piis     = {84964397298},
  url      = {http://dx.doi.org/10.1016/j.ress.2016.04.001},
}

@InCollection{Schenkendorf2017,
  author    = {René Schenkendorf and Xiangzhong Xie and Ulrike Krewer},
  title     = {An efficient polynomial chaos expansion strategy for active fault identification of chemical processes},
  booktitle = {Computer Aided Chemical Engineering},
  publisher = {Elsevier},
  year      = {2017},
  pages     = {1675--1680},
  abstract  = {To gain profit from complex chemical processes, it is essential to ensure its proper operation, i.e. to avoid costly unexpected downtimes of underlying processing units. This paper explores a highly efficient active fault detection and isolation (FDI) framework, which facilitates the discriminability of a set of analysed model candidates including the reference model (nominal behaviour) as well as pre-defined failure models (faulty behaviour). Practically, an auxiliary, model-discriminating input is derived by solving a dynamic optimization problem. While using a model-based approach, the active FDI implementation has to be robustified against the inherent model parameter uncertainties. To this end, a non-intrusive polynomial chaos expansion (PCE) is used to address these uncertainties. To guarantee a computationally feasible performance, the original PCE setting has been considerably improved. Here, the basic idea is to render the design variables (auxiliary inputs) into random variables as well. Thus, the derived PCE results are not only sensitive to the model parameters but also to the design variables. To lower the computational burden further, a least angle regression strategy is applied utilizing the sparsity property of the PCE approach. The overall effectiveness of this One-Short Sparse Polynomial Chaos Expansion (OS2-PCE) concept for FDI is illustrated conceptually by analysing a tubular plug flow reactor.},
  date      = {2017-10-09},
  doi       = {10.1016/B978-0-444-63965-3.50281-6},
  file      = {:Schenkendorf2017 - An Efficient Polynomial Chaos Expansion Strategy for Active Fault Identification of Chemical Processes.pdf:PDF},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041401594},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50281-6},
}

@Article{Toe2017,
  author   = {David Toe and Franck Bourrier and Ignacio Olmedo and Jean-Matthieu Monnet and Frédéric Berger},
  title    = {Analysis of the effect of trees on block propagation using a {DEM} model: implications for rockfall modelling},
  journal  = {Landslides},
  year     = {2017},
  volume   = {14},
  number   = {5},
  pages    = {1603--1614},
  abstract = {The objective of this research was to use numerical models based on mechanical approaches to improve the integration of the protective role of forests against rockfall into block propagation models. A model based on the discrete element method (DEM) was developed to take into account the complex mechanical processes involved during the impact of a block on a tree. This modelling approach requires the definition of many input parameters and cannot be directly integrated into block propagation models. A global sensitivity analysis identified the leading parameters of the block kinematics after impact (i.e. block energy reduction, trajectory changes, and rotational velocity): the impact velocity, the tree diameter, and the impact point horizontal location (i.e. eccentricity). Comparisons with the previous experimental and numerical studies of block impacts on trees demonstrated the applicability of the DEM model and showed some of the limitations of earlier approaches. Our sensitivity analysis highlights the significant influence of the impact velocity on the reduction of the block’s kinetic energy. Previous approaches usually also focus on parameters such as impact height, impact vertical incidence, and tree species, whose importance is only minor according to the present results. This suggests that the integration of forest effects into block propagation models could be both improved and simplified. The DEM model can also be used as an alternative to classical approaches for the integration of forest effects by directly coupling it with block propagation models. This direct coupling only requires the additional definition of the location and the diameter of each tree. Indeed, the input parameters related to the mechanical properties of the stem and the block/stem interaction in the DEM model can be set to average values because they are not leading parameters. The other input parameters are already defined or calculated in the block propagation model.},
  date     = {2017-03-18},
  doi      = {10.1007/s10346-017-0799-6},
  file     = {:Toe2017 - Analysis of the effect of trees on block propagation using a DEM model_ implications for rockfall modelling.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {85015610979},
  url      = {http://dx.doi.org/10.1007/s10346-017-0799-6},
}

@Article{Toe2018,
  author   = {David Toe and Alessio Mentani and Laura Govoni and Franck Bourrier and Guido Gottardi and Stéphane Lambert},
  title    = {Introducing meta-models for a more efficient hazard mitigation strategy with rockfall protection barriers},
  journal  = {Rock Mechanics and Rock Engineering},
  year     = {2018},
  volume   = {51},
  number   = {4},
  pages    = {1097--1109},
  abstract = {The paper presents a new approach to assess the effecctiveness of rockfall protection barriers, accounting for the wide variety of impact conditions observed on natural sites. This approach makes use of meta-models, considering a widely used rockfall barrier type and was developed from on FE simulation results. Six input parameters relevant to the block impact conditions have been considered. Two meta-models were developed concerning the barrier capability either of stopping the block or in reducing its kinetic energy. The outcome of the parameters range on the meta-model accuracy has been also investigated. The results of the study reveal that the meta-models are effective in reproducing with accuracy the response of the barrier to any impact conditions, providing a formidable tool to support the design of these structures. Furthermore, allowing to accommodate the effects of the impact conditions on the prediction of the block–barrier interaction, the approach can be successfully used in combination with rockfall trajectory simulation tools to improve rockfall quantitative hazard assessment and optimise rockfall mitigation strategies.},
  date     = {2018-01-10},
  doi      = {10.1007/s00603-017-1394-9},
  file     = {:Toe2018 - Introducing Meta-models for a More Efficient Hazard Mitigation Strategy with Rockfall Protection Barriers.pdf:PDF},
  keywords = {geomechanics, used UQLab},
  piis     = {85040370552},
  url      = {http://dx.doi.org/10.1007/s00603-017-1394-9},
}

@Article{Turati2018,
  author   = {Pietro Turati and Antonio Cammi and Stefano Lorenzi and Nicola Pedroni and Enrico Zio},
  title    = {Adaptive simulation for failure identification in the {A}dvanced {L}ead {F}ast {R}eactor {E}uropean {D}emonstrator},
  journal  = {Progress in Nuclear Energy},
  year     = {2018},
  volume   = {103},
  pages    = {176--190},
  abstract = {The identification undesired or abnormal states of a nuclear power plant is of primary importance for defining accident prevention and mitigation actions. To this aim, computational models and simulators are frequently employed, as they allow to study the system response to different operational conditions. For complex systems like the nuclear power plants, this is in general challenging because the simulation tools are i) high-dimensional; ii) black-box; iii) dynamic and iv) computationally demanding. In this paper, an adaptive simulation framework recently proposed by some of the authors is tailored for the analysis of accident scenarios involving the control system of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED). The results confirm that the adaptive simulation framework proposed is effective in identifying critical regions of operation with a limited number of calls to the computationally expensive model. The time of occurrence and magnitude of the failures of the components of the control system are identified as key factors to characterize the critical regions. In particular, it is shown that the order of occurrence of the components’ failures strongly affects the evolution of the accident scenarios.},
  date     = {2017-12-22},
  doi      = {10.1016/j.pnucene.2017.11.013},
  file     = {:Turati2018 - Adaptive simulation for failure identification in the Advanced Lead Fast Reactor European Demonstrator.pdf:PDF},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85037379518},
  url      = {http://dx.doi.org/10.1016/j.pnucene.2017.11.013},
}

@Article{Vohra2018,
  author   = {Manav Vohra and Ali Yousefzadi Nobakht and Seungha Shin and Sankaran Mahadevan},
  title    = {Uncertainty quantification in non-equilibrium molecular dynamics simulations of thermal transport},
  journal  = {International Journal of Heat and Mass Transfer},
  year     = {2018},
  volume   = {127},
  pages    = {297--307},
  abstract = {Bulk thermal conductivity estimates based on predictions from non-equilibrium molecular dynamics (NEMD) using the so-called direct method are known to be severely under-predicted since finite simulation length-scales are unable to mimic bulk transport. Moreover, subjecting the system to a temperature gradient by means of thermostatting tends to impact phonon transport adversely. Additionally, NEMD predictions are tightly coupled with the choice of the inter-atomic potential and the underlying values associated with its parameters. In the case of silicon (Si), nominal estimates of the Stillinger-Weber (SW) potential parameters are largely based on a constrained regression approach aimed at agreement with experimental data while ensuring structural stability. However, this approach has its shortcomings and it may not be ideal to use the same set of parameters to study a wide variety of Si-based systems subjected to different thermodynamic conditions. In this study, NEMD simulations are performed on a Si bar to investigate the impact of bar-length, and the applied temperature gradient on the discrepancy between predictions and the available measurement for bulk thermal conductivity at 300 K by constructing statistical response surfaces at different temperatures. The approach helps quantify the discrepancy, observed to be largely dependent on the system-size, with minimal computational effort. A computationally efficient approach based on derivative-based sensitivity measures to construct a reduced-order polynomial chaos surrogate for NEMD predictions is also presented. The surrogate is used to perform parametric sensitivity analysis, forward propagation of the uncertainty, and calibration of the important SW potential parameters in a Bayesian setting. It is found that only two (out of seven) parameters contribute significantly to the uncertainty in bulk thermal conductivity estimates for Si.},
  date     = {2018-07-29},
  doi      = {10.1016/j.ijheatmasstransfer.2018.07.073},
  file     = {:Vohra2018 - Uncertainty quantification in non-equilibrium molecular dynamics simulations of thermal transport.pdf:PDF},
  keywords = {mechanical engineering, used UQLab},
  piis     = {85050528394},
  url      = {http://dx.doi.org/10.1016/j.ijheatmasstransfer.2018.07.073},
}

@Article{Wu2018,
  author   = {Xu Wu and Tomasz Kozlowski and Hadi Meidani},
  title    = {Kriging-based inverse uncertainty quantification of nuclear fuel performance code {BISON} fission gas release model using time series measurement data},
  journal  = {Reliability Engineering and System Safety},
  year     = {2018},
  volume   = {169},
  pages    = {422--436},
  abstract = {In nuclear reactor fuel performance simulation, fission gas release (FGR) and swelling involve treatment of several complicated and interrelated physical processes, which inevitably depend on uncertain input parameters. However, the uncertainties associated with these input parameters are only known by “expert judgment”. In this paper, inverse Uncertainty Quantification (UQ) under the Bayesian framework is applied to BISON code FGR model based on Risø-AN3 time series experimental data. Inverse UQ seeks statistical descriptions of the uncertain input parameters that are consistent with the available measurement data. It always captures the uncertainties in its estimates rather than merely determining the best-fit values. Kriging metamodel is applied to greatly reduce the computational cost during Markov Chain Monte Carlo sampling. We performed a dimension reduction for the FGR time series data using Principal Component Analysis. We also projected the original FGR time series measurement data onto the PC subspace as “transformed experiment data”. A forward uncertainty propagation based on the posterior distributions shows that the agreement between BISON simulation and Ris{\n}-AN3 time series measurement data is greatly improved. The posterior distributions for the uncertain input factors can be used to replace the expert specifications for future uncertainty/sensitivity analysis.},
  date     = {2017-10-03},
  doi      = {10.1016/j.ress.2017.09.029},
  file     = {:Wu2018a - Kriging-based inverse uncertainty quantification of nuclear fuel performance code BISON fission gas release model using time series measurement data.pdf:PDF},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85030669828},
  url      = {http://dx.doi.org/10.1016/j.ress.2017.09.029},
}

@InCollection{Xie2017,
  author    = {Xiangzhong Xie and René Schenkendorf and Ulrike Krewer},
  title     = {Robust design of chemical processes based on a one-shot sparse polynomial chaos expansion concept},
  booktitle = {Computer Aided Chemical Engineering},
  publisher = {Elsevier},
  year      = {2017},
  pages     = {613--618},
  abstract  = {The application of robust model-based design concepts for complex chemical processes is limited due to the repeated cpu-intensive uncertainty quantification step for any new tested process design configuration. Therefore, an efficient One-Shot Sparse Polynomial Chaos Expansion (OS2-PCE) based process design framework is introduced in this work. The key idea is to define the process design variables as uncertain quantities as well and, in consequence, they become an integral part of the robust optimization routine. Moreover, by utilizing the sparsity feature of the PCE approach, the implementation of a least angle regression (LAR) concept leads to a significant reduction in computational costs. The overall performance of the novel OS2-PCE approach is illustrated by a robust process design study of a jacketed tubular reactor. In comparison to state-of-the-art concepts, the proposed framework shows promising results in terms of efficiency and robustness.},
  date      = {2017-10-01},
  doi       = {10.1016/B978-0-444-63965-3.50104-5},
  file      = {:Xie2017 - Robust Design of Chemical Processes Based on a One-Shot Sparse Polynomial Chaos Expansion Concept.pdf:PDF},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041412536},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50104-5},
}

@Article{Xie2018,
  author   = {Xiangzhong Xie and Rüdiger Ohs and Antje Spieß and Ulrike Krewer and René Schenkendorf},
  title    = {Moment-independent sensitivity analysis of enzyme-catalyzed reactions with correlated model parameters},
  journal  = {IFAC-PapersOnLine},
  year     = {2018},
  volume   = {51},
  number   = {2},
  pages    = {753--758},
  abstract = {The dynamic models used for biological and chemical process analysis and design usually include a significant number of uncertain model parameters. Sensitivity analysis is frequently applied to provide quantitative information regarding the influence of the parameters, as well as their uncertainties, on the model output. Various techniques are available in the literature to calculate parameter sensitivities based on local derivatives or changes in dedicated statistical moments of the model output. However, these methods may lead to an inevitable loss of information for a proper sensitivity analysis and are not directly available for problems with correlated model parameters. In this work, we demonstrate the use of a moment-independent sensitivity analysis concept in the presence and absence of parameter correlations and investigate the correlation effect in more detail. Moment-independent sensitivity analysis calculates parameter sensitivities based on changes in the entire probability density distribution of the model output and is formulated independently of whether the parameters are correlated or not. Technically, a single-loop Monte Carlo simulation method in combination with polynomial chaos expansion is implemented to reduce the computational cost significantly. A sampling procedure derived from Gaussian copula formalism is used to generate sample points for arbitrarily correlated uncertain parameters. The proposed concept is demonstrated with a case study of an enzyme-catalyzed reaction network. We observe evident differences in the parameter sensitivities for cases with independent and correlated model parameters.},
  date     = {2018-05-03},
  doi      = {10.1016/j.ifacol.2018.04.004},
  file     = {:Xie2018 - Moment-Independent Sensitivity Analysis of Enzyme-Catalyzed Reactions with Correlated Model Parameters.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  piis     = {85046655059},
  url      = {http://dx.doi.org/10.1016/j.ifacol.2018.04.004},
}

@Article{Yuzugullu2017,
  author   = {Onur Yuzugullu and Esra Erten and Irena Hajnsek},
  title    = {Estimation of rice crop height from {X-and C-Band PolSAR} by metamodel-based optimization},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year     = {2017},
  volume   = {10},
  number   = {1},
  pages    = {194--204},
  abstract = {Rice crops are important in global food economy and are monitored by precise agricultural methods, in which crop morphology in high spatial resolution becomes the point of interest. Synthetic aperture radar (SAR) technology is being used for such agricultural purposes. Using polarimetric SAR (PolSAR) data, plant morphology dependent electromagnetic scattering models can be used to approximate the backscattering behaviors of the crops. However, the inversion of such models for the morphology estimation is complex, ill-posed, and computationally expensive. Here, a metamodel-based probabilistic inversion algorithm is proposed to invert the morphology-based scattering model for the crop biophysical parameter mainly focusing on the crop height estimation. The accuracy of the proposed approach is tested with ground measured biophysical parameters on rice fields in two different bands (X and C) and several channel combinations. Results show that in C-band the combination of the HH and VV channels has the highest overall accuracy through the crop growth cycle. Finally, the proposed metamodel-based probabilistic biophysical parameter retrieval algorithm allows estimation of rice crop height using PolSAR data with high accuracy and low computation cost. This research provides a new perspective on the use of PolSAR data in modern precise agriculture studies.},
  date     = {2016-06-29},
  doi      = {10.1109/JSTARS.2016.2575362},
  file     = {:Yuzugullu2017 - Estimation of Rice Crop Height from X-and C-Band PolSAR by Metamodel-Based Optimization.pdf:PDF},
  keywords = {sensors engineering, used UQLab},
  piis     = {84976511966},
  url      = {http://dx.doi.org/10.1109/JSTARS.2016.2575362},
}

@Article{Xu2018,
  author   = {Xiaoyuan Xu and Zheng Yan and Mohammad Shahidehpour and Sijie Chen and Han Wang and Quan Zhou},
  title    = {Maximum loadability of islanded microgrids with renewable energy generation},
  journal  = {IEEE Transactions on Smart Grid},
  year     = {2018},
  volume   = {early access},
  abstract = {A microgrid (MG) is a small-scale power system which is fed by constrained distributed generation (DG) units and its continuous operation is affected by the variability of available generation resources. In this paper a global sensitivity analysis (GSA) method is proposed to evaluate the impact of variable energy resources on the maximum loadability of islanded MGs (IMGs). First, a probabilistic optimization problem is formulated to calculate the IMG load margin considering the droop characteristics of DG units and uncertainties of renewable generation, loads and distribution feeder parameters. Then, the global sensitivity (GS) is introduced that can identify the impact of independent and correlated variables on the IMG loadability. Next, the sparse polynomial chaos expansion (SPCE) method is used to obtain the probabilistic models for IMG load margins, and an efficient GSA method is proposed to calculate the GS of IMG loadability to prevailing variables. The probabilistic models are considered for IMG input variables and the impact of variable correlations on IMG loadability is analyzed. The proposed method for calculating the maximum loadability is tested using a 33-bus IMG and the results are compared with those of other GSA and local sensitivity analysis (LSA) methods.},
  date     = {2018-06-19},
  doi      = {10.1109/TSG.2018.2848958},
  file     = {:Xu2018 - Maximum loadability of islanded microgrids with renewable energy generation.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  url      = {https://dx.doi.org/10.1109/TSG.2018.2848958},
}

@Article{Yuzugullu2018,
  author   = {Onur Yuzugullu and Esra Erten and Irena Hajnsek},
  title    = {Assessment of paddy rice height: sequential inversion of coherent and incoherent models},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year     = {2018},
  volume   = {11},
  number   = {9},
  pages    = {3001--3013},
  abstract = {This paper investigates the evolution of canopy height of rice fields for a complete growth cycle. For this purpose, copolar interferometric Synthetic Aperture Radar (Pol-InSAR) time series data were acquired during the large across-track baseline (>1 km) science phase of the TanDEM-X mission. The height of rice canopies is estimated by three different model-based approaches. The first approach evaluates the inversion of the Random Volume over Ground (RVoG) model. The second approach evaluates the inversion of a metamodel-driven electromagnetic backscattering model by including a priori morphological information. The third approach combines the previous two processes. The validation analysis was carried out using the Pol-InSAR and ground measurement data acquired between May and September in 2015 over rice fields located in Ipsala district of Edirne, Turkey. The results of presented height estimation algorithms demonstrated the advantage of Pol-InSAR data. The combined RvoG model and EM metamodel height estimation approach provided rice canopy heights with errors less than 20 cm for the complete growth cycle.},
  date     = {2018-06-26},
  doi      = {10.1109/JSTARS.2018.2844798},
  file     = {:Yuzugullu2018 - Assessment of paddy rice height_ sequential inversion of coherent and incoherent models.pdf:PDF},
  keywords = {monitoring and remote sensing, used UQLab},
  url      = {https://dx.doi.org/10.1109/JSTARS.2018.2844798},
}

@InProceedings{Larbi2018b,
  author       = {Mourad Larbi and H. M. Torun and M. Swaminathan and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title        = {Uncertainty quantification of {SiP} based integrated voltage regulator},
  booktitle    = {2018 IEEE 22nd Workshop on Signal and Power Integrity (SPI)},
  year         = {2018},
  address      = {Brest, France},
  organization = {IEEE},
  abstract     = {This paper deals with the uncertainty quantification applied to the analysis of Integrated Voltage Regulator (IVR) efficiency. It presents a meta-model based on a sparse polynomial chaos technique, aiming at estimating statistical quantities of a response with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained are validated against MC simulation.},
  date         = {2018-07-02},
  doi          = {10.1109/SaPIW.2018.8401677},
  eventdate    = {2018-05-22/2018-05-25},
  file         = {:Larbi2018b - Uncertainty quantification of SiP based integrated voltage regulator.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  url          = {https://dx.doi.org/10.1109/SaPIW.2018.8401677},
}

@InProceedings{Zhang2018,
  author       = {Jiangjiang Zhang and Laosheng Wu and Lingzao Zeng},
  title        = {Surrogate-based {Bayesian} inverse modeling of the hydrological system: an adaptive approach considering surrogate structural error},
  booktitle    = {The AGU 2018 Fall Meeting},
  year         = {2018},
  address      = {Washington, D.C., USA},
  organization = {American Geophysical Union (AGU)},
  abstract     = {Inverse modeling is vital for an improved hydrological prediction. However, this process can be computationally demanding as it usually requires a large number of model evaluations. To address this issue, one can take advantage of surrogate modeling techniques, e.g., the one based on sparse polynomial chaos expansion (PCE). Nevertheless, when structural error of the surrogate model is neglected in inverse modeling, the inversion results will be biased. In this paper, we develop a surrogate-based Bayesian inversion framework that rigorously quantifies and gradually eliminates the structural error of the surrogate. Specifically, two strategies are proposed and compared. The first strategy works by obtaining an ensemble of sparse PCE surrogates with Markov chain Monte Carlo sampling, while the second one uses Gaussian process (GP) to simulate the structural error of a single sparse PCE surrogate. With an active learning process, the surrogate structural error can be gradually reduced to a negligible level in the posterior region, where the original input-output relationship can be much more easily captured by PCE than in the prior. Demonstrated by one numerical case of groundwater contaminant source identification with 28 unknown input variables, it is found that both strategies can efficiently reduce the bias introduced by surrogate modeling, while the second strategy has a better performance as it integrates two methods (i.e., PCE and GP) that complement each other.},
  date         = {2018-12-14},
  eventdate    = {2018-12-10/2018-12-14},
  file         = {:Zhang2018 - Surrogate-based Bayesian inverse modeling of the hydrological system_ an adaptive approach considering surrogate structural error.pdf:PDF},
  keywords     = {geoscience, used UQLab},
  url          = {https://arxiv.org/abs/1807.05187},
}

@InProceedings{Miers2018,
  author       = {Collier Miers and Amy Marconnet},
  title        = {Uncertainty quantification for a high temperature {Z-Meter} characterization system},
  booktitle    = {2018 17th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm)},
  year         = {2018},
  pages        = {572--581},
  address      = {San Diego, California, USA},
  organization = {IEEE},
  note         = {Date is the "date added to IEEE Xplore"},
  abstract     = {Few systems have been developed for simultaneous measurement of thermal, electrical, and thermoelectric properties at high temperatures (> 600K) relevant to high temperature waste heat recovery applications. The Z-Meter approach enables simultaneous measurements of the three thermoelectric properties from which the figure of merit, ZT, is calculated. The method combines an ASTM D5470 reference bar measurement for thermal conductivity (often used for thermal interface materials) with the added functionality of electrical measurements for electrical conductivity and Seebeck Coefficient. Traditionally, this technique has been employed utilizing a very small temperature difference across the sample (ΔT ≈ 1 - 2K) and test temperatures between approximately 300 - 600K, but thermoelectric materials must generally have sizable temperature gradients across them to achieve desirable performance as increased operating temperatures generally allow access to higher grade waste heat, and the relevant properties are also significantly temperature dependent. Here, we design a Z-meter system to evaluate the performance of materials under large temperature differences relevant to such applications. The design of the instrument is driven by uncertainty quantification to minimize measurement error. A detailed design of experiments model enables informed decisions regarding the component and system designs (e.g. placement of the temperature sensors). This design of experiments model includes effects of radiative losses in the meter bars as well as the losses due to the measurement probes installed along the bars. The system as designed is capable of hot side temperatures of 1350K with the thermoelectric material to temperature gradients on the order of 500K. The elevated temperatures are necessary to fill a gap in characterization equipment for very high temperature thermoelectric applications. The measurement system requires vacuum pressure of 1 μTorr for suppression of convection losses and to prevent of oxidation of the hot system components. The system design employs a loading platform that allows the interfacial sample/bar loading to be changed in situ without breaking system vacuum allowing for high measurement throughput. Additionally, a triad of integrated load cells ensure repeatable loading conditions for the sample interface. The combination of simultaneous thermal, electrical, and thermoelectric properties at high temperatures with controlled mechanical loading opens the doors to new understanding of thermoelectrics for high temperature waste heat recovery. Ultimately, this uncertainty analysis coupled with the design process demonstrates the limits of using this system and guides operating conditions for given expected sampleproperties.},
  date         = {2018-07-26},
  doi          = {10.1109/ITHERM.2018.8419511},
  eventdate    = {2018-05-29/2018-06-01},
  file         = {:Miers2018 - Uncertainty quantification for a high temperature Z-Meter characterization system.pdf:PDF},
  keywords     = {electrical engineering, used UQLab},
  url          = {https://dx.doi.org/10.1109/ITHERM.2018.8419511},
}

@Article{Palar2018a,
  author   = {Pramudita Satria Palar and Koji Shimoyama},
  title    = {Efficient global optimization with ensemble and selection of kernel functions for engineering design},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  pages    = {1--24},
  abstract = {In this paper, we investigate the use of multiple kernel functions for assisting single-objective Kriging-based efficient global optimization (EGO). The primary objective is to improve the robustness of EGO in terms of the choice of kernel function for solving a variety of black-box optimization problems in engineering design. Specifically, three widely used kernel functions are studied, that is, Gaussian, Matérn-3/2, and Matérn-5/2 function. We investigate both model selection and ensemble techniques based on Akaike information criterion (AIC) and cross-validation error on a set of synthetic (noiseless and noisy) and non-algebraic (aerodynamic and parameter tuning) optimization problems; in addition, the use of cross-validation-based local (i.e., pointwise) ensemble is also studied. Since all the constituent surrogate models in the ensemble scheme are Kriging models, it is possible to perform EGO since the Kriging uncertainty structure is still preserved. Through analyses of empirical experiments, it is revealed that the ensemble techniques improve the robustness and performance of EGO. It is also revealed that the use of Matérn-kernels yields better results than those of the Gaussian kernel when EGO with a single kernel is considered. Furthermore, we observe that model selection methods do not yield any substantial improvement over single kernel EGO. When averaged across all types of problem (i.e., noise level, dimensionality, and synthetic/non-algebraic), the local ensemble technique achieves the best performance.},
  date     = {2018-07-28},
  doi      = {10.1007/s00158-018-2053-9},
  file     = {:Palar2018a - Efficient global optimization with ensemble and selection of kernel functions for engineering design.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1007/s00158-018-2053-9},
}

@InProceedings{Prakash2018,
  author       = {Chandra Prakash and I. Emre Gunduz and Vikas Tomar},
  title        = {Uncertainty quantification in nanoscale impact experiment in energetic materials},
  booktitle    = {Proceedings of the 36th IMAC, a Conference and Exposition on Structural Dynamics},
  year         = {2018},
  pages        = {257--263},
  address      = {Orlando, Florida, USA},
  organization = {Society for Experimental Mechanics},
  abstract     = {Finite element method is extensively used for the analysis of impact response in complex materials. The prediction from finite element model may exhibit significant difference from that of experiments due to uncertainties in model, experimental measurements, and parameters that are derived based on experiments for model development. The quantification of parametric uncertainties, such as parameters in constitutive relation, associated with the numerical model is an important aspect that needs to be investigated for a credible computational prediction. This work considers uncertainty quantification in finite element modeling of nanoscale dynamic impact problems. A viscoplastic power law constitutive model is obtained from nanoscale impact experiments on Hydroxyl-terminated polybutadiene (HTPB)-Ammonium Perchlorate (AP) samples. The constitutive model is used in a finite element model to simulate impact experiments. The measured response from impact experiment and FEM simulation is used to quantify the parametric uncertainties in the constitutive model for the analyzed HTPB-AP sample.},
  date         = {2018-07-31},
  doi          = {10.1007/978-3-319-74793-4_30},
  eventdate    = {2018-02-12/2018-02-15},
  file         = {:Prakash2018 - Uncertainty quantification in nanoscale impact experiment in energetic materials.pdf:PDF},
  keywords     = {mechanical engineering, used UQLab},
  url          = {https://doi.org/10.1007/978-3-319-74793-4_30},
}

@InProceedings{Ligeikis2018,
  author       = {Connor Ligeikis and Alex Freeman and Richard Christenson},
  title        = {Assessing structural reliability at the component test stage using real-time hybrid substructuring},
  booktitle    = {Proceedings of the 36th IMAC, a Conference and Exposition on Structural Dynamics},
  year         = {2018},
  pages        = {75--78},
  address      = {Orlando, Florida, USA},
  organization = {Society for Experimental Mechanics},
  abstract     = {The propagation of uncertainties through complex systems is a challenging endeavor. While numerical simulations can be used to accurately predict the dynamic performance of structural systems, there are some instances where the dynamics and uncertainties of specific components may be less understood or difficult to accurately model. This paper will implement a structural reliability assessment employing the cyber-physical real-time hybrid substructuring (RTHS) method to combine a numerical model of a larger structural system, incorporating uncertainty in specific parameters, with a physical test specimen of a component of the system while fully incorporating the system-level dynamic interactions and uncertainty propagation. This RTHS approach will allow for uncertainty and reliability to be addressed in the early stage of the design process as components become available and the remainder of the system remains numerically modeled. A small-scale RTHS experiment will be used to demonstrate the probability of failure of a spring-mass-damper system with a relatively small number of component tests by employing the previously proposed Adaptive Kriging-Hybrid Simulation (AK-HS) reliability method.},
  date         = {2018-07-31},
  doi          = {10.1007/978-3-319-74793-4_11},
  eventdate    = {2018-02-12/2018-02-15},
  file         = {:Ligeikis2018 - Assessing structural reliability at the component test stage using real-time hybrid substructuring.pdf:PDF},
  keywords     = {civil engineering, used UQLab},
  url          = {https://doi.org/10.1007/978-3-319-74793-4_11},
}

@Article{Aleksankina2018,
  author   = {Ksenia Aleksankina and Stefan Reis and Massimo Vieno and Mathew R. Heal},
  title    = {Advanced methods for uncertainty assessment and global sensitivity analysis of a {E}ulerian atmospheric chemistry transport model},
  journal  = {Atmospheric Chemistry and Physics Discussion},
  year     = {2018},
  volume   = {in review},
  abstract = {Atmospheric chemistry transport models (ACTMs) are extensively used to provide scientific support for the development of policies to mitigate against the detrimental effects of air pollution on human health and ecosystems. Therefore, it is essential to quantitatively assess the level of model uncertainty and to identify the model input parameters that contribute the most to the uncertainty. For complex process-based models, such as ACTMs, uncertainty and global sensitivity analyses are still challenging and are often limited by computational constraints due to the requirement of a large number of model runs. In this work, we demonstrate an emulator-based approach to uncertainty quantification and variance-based sensitivity analysis for the EMEP4UK model (regional application of the European Monitoring and Evaluation Programme Meteorological Synthesizing Centre-West). A separate Gaussian process emulator was used to estimate model predictions at unsampled points in the space of the uncertain model inputs for every modelled grid cell. The training points for the emulator were chosen using an optimised Latin hypercube sampling design. The uncertainties in surface concentrations of O3, NO2, and PM2.5 were propagated from the uncertainties in the anthropogenic emissions of NOx, SO2, NH3, VOC, and primary PM2.5 reported by the UK National Atmospheric Emissions Inventory. The results of the EMEP4UK uncertainty analysis for the annually averaged model predictions indicate that modelled surface concentrations of O3, NO2, and PM2.5 have the highest level of uncertainty in the grid cells comprising urban areas (up to \pm 7\%, \pm 9\%, and \pm 9\% respectively). The uncertainty in the surface concentrations of O3 and NO2 were dominated by uncertainties in NOx emissions combined from non-dominant sectors (i.e. all sectors excluding energy production and road transport) and shipping emissions. Additionally, uncertainty in OO3 was driven by uncertainty VOC emissions combined from sectors excluding solvent use. Uncertainties in the modelled PM2.5 concentrations were mainly driven by uncertainties in primary PM2.5 emissions and NH3 emissions from the agricultural sector. Uncertainty and sensitivity analyses were also performed for five selected grid sells for monthly averaged model predictions to illustrate the seasonal change in the magnitude of uncertainty and change in the contribution of different model inputs to the overall uncertainty. Our study demonstrates the viability of a Gaussian process emulator-based approach for uncertainty and global sensitivity analyses, which can be applied to other ACTMs. Conducting these analyses helps to increase the confidence in model predictions. Additionally, the emulators created for these analyses can be used to predict the ACTM response for any other combination of perturbed input emissions within the ranges set for the original Latin hypercube sampling design without the need to re-run the ACTM, thus allowing fast exploratory assessments at significantly reduced computational costs.},
  date     = {2018-07-26},
  doi      = {10.5194/acp-2018-690},
  file     = {:Aleksankina2018 - Advanced methods for uncertainty assessment and global sensitivity analysis of a Eulerian atmospheric chemistry transport model.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.5194/acp-2018-690},
}

@InProceedings{Errante2018,
  author       = {Paolo Errante and Christophe Corre},
  title        = {Uncertainty quantification for the {E}ulerian-{L}agrangian simulation of evaporating sprays},
  booktitle    = {the 14th Triennial International Conference on Liquid Atomization and Spray Systems (ICLASS 2018)},
  year         = {2018},
  address      = {Chicago, Illinois, USA},
  organization = {Institute for Liquid Atomization and Spray Systems (ILASS)},
  abstract     = {Evaporating sprays can be almost routinely simulated using an Eulerian-Lagrangian approach which relies on a Reynolds Averaged Navier Stokes (RANS) modeling of the continuous phase and a Lagrangian description of the discrete phase, including a turbulent dispersion model to express the effect of turbulent fluctuations within the carrier phase on the spray particles and an evaporation model for the spray droplets. Both descriptions are coupled through a two-way approach which accounts for the effects of the continuous phase on the droplets and the retroaction of the droplets on the carrier phase. Several experiments available in the literature have been used to calibrate the physical models involved in the numerical prediction of evaporating sprays (see [6][7]). The present study focuses on the simulation of evaporative sprays taking into account existing experimental and modeling uncertainties. Due to the significant cost of the numerical prediction, a surrogate model is used to quantify the effect of these uncertainties on quantities of interest. The non-intrusive Polynomial Chaos Expansion (PCE) technique available in the uncertainty quantification framework software UQLab is coupled with the deterministic Computational Fluid Dynamics (CFD) solver to yield statistical outputs (typically mean value and variance) of key quantities, such as the liquid mass flow rate for instance. The study analyzes the sensitivity of these outputs to experimental uncertainties on the continuous and discrete phase inputs (gas inlet velocity distribution, inlet liquid mass flow rate) and to modeling uncertainties in the turbulent dispersion model and evaporation model.},
  date         = {2018-07-26},
  eventdate    = {2018-07-22/2018-07-26},
  file         = {:Errante2018 - Uncertainty quantification for the Eulerian-Lagrangian simulation of evaporating sprays.pdf:PDF},
  keywords     = {mechanical engineering, used UQLab},
  url          = {https://www.researchgate.net/publication/326741120_Uncertainty_Quantification_for_the_Eulerian-Lagrangian_simulation_of_evaporating_sprays},
}

@Article{Naik2018,
  author   = {Pratik Naik and Soroush Aramideh and Arezoo M. Ardekani},
  title    = {History matching of surfactant-polymer flooding using polynomial chaos expansion},
  journal  = {Journal of Petroleum Science and Engineering},
  year     = {2018},
  volume   = {173},
  pages    = {1438--1452},
  note     = {Date is "available online"},
  abstract = {This paper proposes a robust framework for history matching which employs a sequential execution of sensitivity analysis, proxy modeling and inverse optimization to determine the optimized parameter space of model parameters. A mechanistic surfactant-polymer (SP) flood model is considered for history matching with an ultimate goal of accurately calibrating models that describe physical subprocesses of surfactant flooding, polymer flooding and displacement process. The employed model calibration algorithm starts with Sobol sensitivity analysis which reduces the large uncertain space of model parameters to determine the most important stochastic variables. The resulting low-dimensional parameter space is then represented via appropriate orthonormal basis of polynomial chaos expansion (PCE-proxy). An inverse optimization problem is then posed that minimizes the miss-fit between PCE-proxy response and experimental observations by employing a Genetic Algorithm. Finally, the epistemic uncertainty in PCE-proxy is quantified by combining it with a Gaussian regression process called Kriging.

We use this framework to calibrate the SP flood model by history matching a single coreflood experiment for quantities of interest such as pressure drop profile and cumulative oil recovery curve. We then show that the calibrated model is successfully able to predict all our quantities of interest for two other coreflood experiments without any ad-hoc tuning of parameters. The proposed proxy-accelerated inverse optimization framework shows significant promise for model calibration or to improve the quality of history matched results.},
  date     = {2018-10-05},
  doi      = {10.1016/j.petrol.2018.09.089},
  file     = {:Naik2018 - History matching of surfactant-polymer flooding using polynomial chaos expansion.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.petrol.2018.09.089},
}

@Article{Wang2018,
  author   = {Han Wang and Zheng Yan and Xiaoyuan Xu and Kun He},
  journal  = {IEEE Access},
  title    = {Evaluating influence of variable renewable energy generation on islanded microgrid power flow},
  year     = {2018},
  pages    = {71339--71349},
  abstract = {With the proliferation of renewable energy, the uncertainty has challenged the continuous operation of microgrids; thus, it is of importance to tackle uncertainties in power system operation. In this paper, a global sensitivity analysis (GSA) method is proposed to evaluate the influence of uncertainties on the power flow of islanded microgrids (IMGs). First, a probabilistic power flow model for IMGs is established considering the droop-controlled distributed generation units and the uncertainties of renewable energy generation output and load demands. Then, the global sensitivity analysis is introduced to identify important variables that affect IMG power flow. In addition to conventional GSA indices, the Shapley value-based GSA index is designed to evaluate the influence of correlated input variables. Moreover, the sparse polynomial chaos expansion is used to establish the surrogate models of IMG power flow, which improves the efficiency of GSA. Finally, the proposed method is tested on the 33-bus and 69-bus IMG systems, and the simulation results are compared with those considering other methods. The rankings of random input variables that affect IMG power flow are given, and the influence of correlation between different variables is discussed.},
  comment  = {With the proliferation of renewable energy, the uncertainty has challenged the continuous operation of microgrids; thus, it is of importance to tackle uncertainties in power system operation. In this paper, a global sensitivity analysis (GSA) method is proposed to evaluate the influence of uncertainties on the power flow of islanded microgrids (IMGs). First, a probabilistic power flow model for IMGs is established considering the droop-controlled distributed generation units and the uncertainties of renewable energy generation output and load demands. Then, the global sensitivity analysis is introduced to identify important variables that affect IMG power flow. In addition to conventional GSA indices, the Shapley value-based GSA index is designed to evaluate the influence of correlated input variables. Moreover, the sparse polynomial chaos expansion is used to establish the surrogate models of IMG power flow, which improves the efficiency of GSA. Finally, the proposed method is tested on the 33-bus and 69-bus IMG systems, and the simulation results are compared with those considering other methods. The rankings of random input variables that affect IMG power flow are given, and the influence of correlation between different variables is discussed.},
  date     = {2018-11-13},
  doi      = {10.1109/ACCESS.2018.2881189},
  file     = {:Wang2018 - Evaluating influence of variable renewable energy generation on islanded microgrid power flow.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  url      = {https://dx.doi.org/10.1109/ACCESS.2018.2881189},
}

@Article{Wang2018a,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  title    = {{ESC}: an efficient error-based stopping criterion for kriging-based reliability analysis methods},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2018},
  pages    = {1--17},
  abstract = {The ever-increasing complexity of numerical models and associated computational demands have challenged classical reliability analysis methods. Surrogate model-based reliability analysis techniques, and in particular those using kriging meta-model, have gained considerable attention recently for their ability to achieve high accuracy and computational efficiency. However, existing stopping criteria, which are used to terminate the training of surrogate models, do not directly relate to the error in estimated failure probabilities. This limitation can lead to high computational demands because of unnecessary calls to costly performance functions (e.g., involving finite element models) or potentially inaccurate estimates of failure probability due to premature termination of the training process. Here, we propose the error-based stopping criterion (ESC) to address these limitations. First, it is shown that the total number of wrong sign estimation of the performance function for candidate design samples by kriging, S, follows a Poisson binomial distribution. This finding is subsequently used to estimate the lower and upper bounds of S for a given confidence level for sets of candidate design samples classified by kriging as safe and unsafe. An upper bound of error of the estimated failure probability is subsequently derived according to the probabilistic properties of Poisson binomial distribution. The proposed upper bound is implemented in the kriging-based reliability analysis method as the stopping criterion. The efficiency and robustness of ESC are investigated here using five benchmark reliability analysis problems. Results indicate that the proposed method achieves the set accuracy target and substantially reduces the computational demand, in some cases by over 50\%.},
  date     = {2018-11-23},
  doi      = {10.1007/s00158-018-2150-9},
  file     = {:Wang2018a - ESC_ an efficient error-based stopping criterion for kriging-based reliability analysis methods.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1007/s00158-018-2150-9},
}

@Article{Groenquist2018,
  author   = {Philippe Grönquist and Falk K. Wittel and Markus Rüggeberg},
  journal  = {PLoS ONE},
  title    = {Modeling and design of thin bending wooden bilayers},
  year     = {2018},
  number   = {10},
  volume   = {13},
  abstract = {In recent architectural research, thin wooden bilayer laminates capable of self-actuation in response to humidity changes have been proposed as sustainable, programmed, and fully autonomous elements for facades or roofs for shading and climate regulation. Switches, humidistats, or motor elements represent further promising applications. Proper wood-adapted prediction models for actuation, however, are still missing. Here, a simple model that can predict bending deformation as a function of moisture content change, wood material parameters, and geometry is presented. We consider material anisotropy and moisture-dependency of elastic mechanical parameters. The model is validated using experimental data collected on bilayers made out of European beech wood. Furthermore, we present essential design aspects in view of facilitated industrial applications. Layer thickness, thickness-ratio, and growth ring angle of the wood in single layers are assessed by their effect on curvature, stored elastic energy, and generated axial stress. A sensitivity analysis is conducted to identify primary curvature-impacting model input parameters.},
  comment  = {For the analysis, the Matlab-based uncertainty quantification tool UQLab was used [19]. The engineering constants EL, ER, and ET entering the model (for the plane strain case), the three swelling coefficients αL, αR, and αT, the two thicknesses h1, and h2, and the growth ring orientation φ2, were sampled using 107 MC samples. We assume a log-normal PDF for the material properties (), where a coefficient of variation (COV = σ/μ) of 10% is attributed to each property, and a Gaussian PDF for the geometrical properties () where standard deviations (σ) of 0.1 mm, 0.1 mm and 1° are attributed respectively. Exemplary, four beech bilayer configurations were analyzed (described along Results). The mean values μi (first moments of the input PDFs of i) for the material properties are taken as in Fig 1. The described input parameters were chosen in view of direct applicability in industry. The Young’s moduli, the differential swelling coefficients, and the geometry are deemed easier to record than other input parameters (i.e. shear moduli and Poisson ratios).},
  date     = {2018-10-16},
  doi      = {10.1371/journal.pone.0205607},
  file     = {:Groenquist2018 - Modeling and design of thin bending wooden bilayers.pdf:PDF},
  keywords = {material science and engineering, used UQLab, mechanical engineering},
  url      = {https://doi.org/10.1371/journal.pone.0205607},
}

@Article{Shahane2019a,
  author      = {Shantanu Shahane and Narayana Aluru and Placid Ferreira and Shiv G Kapoor and Surya Pratap Vanka},
  journal     = {Applied Mathematical Modeling},
  title       = {Finite volume simulation framework for die casting with uncertainty quantification},
  year        = {2019},
  pages       = {132--150},
  volume      = {74},
  abstract    = {The present paper describes the development of a novel and comprehensive computational framework to simulate solidification problems in materials processing, specifically casting processes. Heat transfer, solidification and fluid flow due to natural convection are modeled. Empirical relations are used to estimate the microstructure parameters and mechanical properties. The fractional step algorithm is modified to deal with the numerical aspects of solidification by suitably altering the coefficients in the discretized equation to simulate selectively only in the liquid and mushy zones. This brings significant computational speed up as the simulation proceeds. Complex domains are represented by unstructured hexahedral elements. The algebraic multigrid method, blended with a Krylov subspace solver is used to accelerate convergence. State of the art uncertainty quantification technique is included in the framework to incorporate the effects of stochastic variations in the input parameters. Rigorous validation is presented using published experimental results of a solidification problem.},
  comment     = {Used UQLab to construct PCE and PCK metamodels and conduct Sobol' senstivity analysis.},
  date        = {2018-10-19},
  doi         = {10.1016/j.apm.2019.04.045},
  eprint      = {1810.08572v1},
  eprintclass = {cs.NA},
  eprinttype  = {arXiv},
  file        = {online:Shahane2018 - Finite Volume Simulation Framework for Die Casting with Uncertainty Quantification.pdf:PDF},
  keywords    = {mechanical engineering, used UQLab},
  url         = {https://doi.org/10.1016/j.apm.2019.04.045},
}

@Article{Hart2018,
  author     = {J. L. Hart and P. A. Gremaud and T. David},
  title      = {Global sensitivity analysis of high dimensional neuroscience models: an example of neurovascular coupling},
  journal    = {arXiv},
  year       = {2018},
  abstract   = {The complexity and size of state-of-the-art cell models have significantly increased in part due to the requirement that these models possess complex cellular functions which are thought--but not necessarily proven--to be important. Modern cell models often involve hundreds of parameters; the values of these parameters come, more often than not, from animal experiments whose relationship to the human physiology is weak with very little information on the errors in these measurements. The concomitant uncertainties in parameter values result in uncertainties in the model outputs or Quantities of Interest (QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual parameters (or sets of parameters) their relative contribution to output uncertainty thereby introducing a measure of influence or importance of said parameters. New GSA approaches are required to deal with increased model size and complexity; a three stage methodology consisting of screening (dimension reduction), surrogate modeling, and computing Sobol' indices, is presented. The methodology is used to analyze a physiologically validated numerical model of neurovascular coupling which possess 160 uncertain parameters. The sensitivity analysis investigates three quantities of interest (QoIs), the average value of $K^+$ in the extracellular space, the average volumetric flow rate through the perfusing vessel, and the minimum value of the actin/myosin complex in the smooth muscle cell. GSA provides a measure of the influence of each parameter, for each of the three QoIs, giving insight into areas of possible physiological dysfunction and areas of further investigation.},
  date       = {2018-11-20},
  eprint     = {1811.08498v1},
  eprinttype = {arXiv},
  file       = {online:Hart2018 - Global Sensitivity Analysis of High Dimensional Neuroscience Models_ An Example of Neurovascular Coupling.pdf:PDF},
  keywords   = {biology, used UQLab},
  url        = {https://arxiv.org/abs/1811.08498?context=q-bio},
}

@Article{Wang2018b,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  title    = {{REAK}: {Reliability} analysis through error rate-based adaptive {K}riging},
  journal  = {Reliability Engineering \& System Safety},
  year     = {2018},
  volume   = {182},
  pages    = {33--45},
  note     = {Didn't specifically say about using UQLab but cite two of the UQLab manuals. Date is "available online"},
  abstract = {As models in various fields are becoming more complex, associated computational demands have been increasing significantly. Reliability analysis for these systems when failure probabilities are small is very challenging, requiring a large number of costly simulations. To address this challenge, this paper introduces Reliability analysis through Error rate-based Adaptive Kriging (REAK). An extension of the Central Limit Theorem based on Lindeberg condition is adopted here to derive the distribution of the number of design samples with wrong sign estimate and subsequently determine the maximum error rate for failure probability estimates. This error rate enables optimal establishment of effective sampling regions at each stage of an adaptive scheme for strategic generation of design samples. Moreover, it facilitates setting a target accuracy for failure probability estimation, which is used as the stopping criterion for reliability analysis. These capabilities together can significantly reduce the number of calls to sophisticated, computationally demanding models. The application of REAK for four examples with varying extent of nonlinearity and dimension is presented. Results indicate that REAK is able to reduce the computational demand by as high as 50\% compared to the state-of-the-art methods of Adaptive Kriging with Monte Carlo Simulation (AK-MCS) and Improved Sequential Kriging Reliability Analysis (ISKRA).},
  date     = {2018-10-09},
  doi      = {10.1016/j.ress.2018.10.004},
  file     = {:Wang2018b - REAK_ Reliability analysis through error rate-based adaptive Kriging.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.ress.2018.10.004},
}

@Article{Sheng2018,
  author   = {Hao Sheng and Xiaozhe Wang},
  title    = {A non-intrusive low-rank approximation method for assessing the probabilistic available transfer capability},
  journal  = {IEEE Transactions on Smart Grid},
  year     = {2018},
  volume   = {In Review},
  abstract = {In this paper, a mathematical formulation of the probabilistic available transfer capability (PATC) problem is proposed to incorporate uncertainties from the large-scale renewable energy generation (e.g., wind farms and solar PV power plants). Moreover, a novel non-intrusive low-rank approximation (LRA) is developed to assess PATC, which can accurately and efficiently estimate the probabilistic characteristics (e.g., mean, variance, probability density function (PDF)) of the PATC. Numerical studies on the IEEE 24-bus reliability test system (RTS) and IEEE 118-bus system show that the proposed method can achieve accurate estimations for the probabilistic characteristics of the PATC with much less computational effort compared to the Latin hypercube sampling (LHS)-based Monte Carlo simulations (MCS). The proposed LRA-PATC method offers an efficient and effective way to determine the available transfer capability so as to fully utilize the transmission assets while maintaining the security of the grid.},
  date     = {2018-10-18},
  file     = {:Sheng2018 - A non-intrusive low-rank approximation method for assessing the probabilistic available transfer capability.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  url      = {https://arxiv.org/abs/1810.08156},
}

@Article{Emenike2018,
  author   = {Victor N. Emenike and Xiangzhong Xie and René Schenkendorf and Antje C. Spiess and Ulrike Krewer},
  title    = {Robust dynamic optimization of enzyme-catalized carboligation: a point estimate-based back-off approach},
  journal  = {Computers and Chemical Engineering},
  year     = {2018},
  volume   = {121},
  pages    = {232--247},
  abstract = {In this paper, we present a systematic robust dynamic optimization framework applied to the benzaldehyde lyase-catalyzed carboligation of propanal and benzaldehyde to produce (R)-2-hydroxy-1-phenylbutan-1-one (BA). First, the elementary process functions approach was used to screen between different dosing concepts, and it was found that simultaneously dosing propanal and benzaldehyde leads to the highest final concentration of BA. Next, we applied global sensitivity analysis and found that 10 out of 13 kinetic parameters are relevant. Time-varying back-offs were then used to handle parametric uncertainties due to these 10 parameters. A major contribution in our work is the use of the point estimate method instead of Monte Carlo simulations to calculate the back-offs in an efficient and reproducible manner. We show that this new approach is at least 10 times faster than the conventional Monte Carlo approach while achieving low approximation errors.},
  date     = {2018-10-25},
  doi      = {10.1016/j.compchemeng.2018.10.006},
  file     = {:Emenike2018 - Robust dynamic optimization of enzyme-catalized carboligation_ a point estimate-based back-off approach.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.compchemeng.2018.10.006},
}

@Article{Dimitrov2018,
  author   = {Nikolay Dimitrov and Mark C. Kelly and Andrea Vignaroli and Jacob Berg},
  title    = {From wind to loads: wind turbine site-specific load estimation with surrogate models trained on high-fidelity load databases},
  journal  = {Wind Energy Science},
  year     = {2018},
  volume   = {3},
  pages    = {767--790},
  abstract = {We define and demonstrate a procedure for quick assessment of site-specific lifetime fatigue loads using simplified load mapping functions (surrogate models), trained by means of a database with high-fidelity load simulations. The performance of five surrogate models is assessed by comparing site-specific lifetime fatigue load predictions at 10 sites using an aeroelastic model of the DTU 10 MW reference wind turbine. The surrogate methods are polynomial chaos expansion, quadratic response surface, universal Kriging, importance sampling, and nearest-neighbor interpolation. Practical bounds for the database and calibration are defined via nine environmental variables, and their relative effects on the fatigue loads are evaluated by means of Sobol sensitivity indices. Of the surrogate-model methods, polynomial chaos expansion provides an accurate and robust performance in prediction of the different site-specific loads. Although the Kriging approach showed slightly better accuracy, it also demanded more computational resources.},
  date     = {2018-10-24},
  doi      = {10.5194/wes-3-767-2018},
  file     = {:Dimitrov2018 - From wind to loads_ wind turbine site-specific load estimation with surrogate models trained on high-fidelity load databases.pdf:PDF},
  keywords = {mechanical engineering, used UQLab},
  url      = {https://doi.org/10.5194/wes-3-767-2018},
}

@Article{Shahane2019,
  author     = {Shantanu Shahane and Narayana R. Aluru and Surya Pratap Vanka},
  journal    = {International Jounral of Heat and Mass Transfer},
  title      = {Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks},
  year       = {2019},
  pages      = {613--631},
  volume     = {139},
  abstract   = {This paper analyzes the effects of input uncertainties on the outputs of a three dimensional natural convection problem in a differentially heated cubical enclosure. Two different cases are considered for parameter uncertainty propagation and global sensitivity analysis. In case A, stochastic variation is introduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers) with an assumption that the temperature boundary condition is uniform. Being a two dimensional stochastic problem, the polynomial chaos expansion (PCE) method is used as a surrogate model. Case B deals with non-uniform stochasticity in the boundary temperature. Instead of the traditional Gaussian process model with the Karhunen-Lo$\grave{e}$ve expansion, a novel approach is successfully implemented to model uncertainty in the boundary condition. The boundary is divided into multiple domains and the temperature imposed on each domain is assumed to be an independent and identically distributed (i.i.d) random variable. Deep neural networks are trained with the boundary temperatures as inputs and Nusselt number, internal temperature or velocities as outputs. The number of domains which is essentially the stochastic dimension is 4, 8, 16 or 32. Rigorous training and testing process shows that the neural network is able to approximate the outputs to a reasonable accuracy. For a high stochastic dimension like 32, it is computationally expensive to fit the PCE. This paper demonstrates a novel way of using the deep neural network as a surrogate modeling method of uncertainty quantification with the number simulations much lesser than that required for fitting the PCE thus, saving the computational cost.},
  comment    = {Used UQLab to construct PCE and PCK metamodels and conduct Sobol' senstivity analysis.},
  date       = {2018-10-29},
  doi        = {10.1016/j.ijheatmasstransfer.2019.05.014},
  eprinttype = {arXiv},
  file       = {:Shahane2018a - Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks.pdf:PDF},
  keywords   = {mechanical engineering, used UQLab},
  url        = {https://doi.org/10.1016/j.ijheatmasstransfer.2019.05.014},
}

@Article{Vepsaelaeinen2019,
  author   = {Jari Vepsäläinen and Kevin Otto and Antti Lajunena and Kari Tammia},
  title    = {Computationally efficient model for energy demand prediction of electric city bus in varying operating conditions},
  journal  = {Energy},
  year     = {2019},
  volume   = {169},
  abstract = {The uncertainty of operating conditions such as weather and payload cause variations in the energy demand of electric city buses. Uncertain variation in energy demand is a challenge in the design of charging systems and on-board energy storages. To predict the energy demand, a computationally efficient model is required for real-time applications. We present a novel approach to predict energy demand variation with a wide range of uncertain factors. A factor identification is carried out to recognize the range of variation in the operating conditions. A computationally efficient surrogate model is generated based on a previously developed numerical simulation model. The surrogate model is shown to be 10 000 times faster than the numerical model. The surrogate model output corresponds with the numerical model with less than 1\% error. The energy demand of the surrogate model varied from 0.43 to 2.30 kWh/km, which is realistic in comparison to previous studies. Successful sensitivity analysis of the surrogate model revealed the most crucial factors. Uncertainty in temperature, rolling resistance and payload contributed most to the variation in energy demand. Variation in these factors should be taken into account when predicting energy consumption and while planning schedules for a bus network.},
  date     = {2019-02-15},
  doi      = {10.1016/j.energy.2018.12.064},
  file     = {:Vepsaelaeinen2019 - Computationally efficient model for energy demand prediction of electric city bus in varying operating conditions.pdf:PDF},
  keywords = {energy engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.energy.2018.12.064},
}

@Article{Cheng2019,
  author   = {Kai Cheng and Zhenzhou Lu and Kaichao Zhang},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Multivariate output global sensitivity analysis using multi-output support vector regression},
  year     = {2019},
  abstract = {Models with multivariate outputs are widely used for risk assessment and decision-making in practical applications. In this paper, multi-output support vector regression (M-SVR) is employed for global sensitivity analysis (GSA) with multivariate output models. The orthogonal polynomial kernel is used to build the M-SVR meta-model, and the covariance-based sensitivity indices of multivariate output are obtained analytically by post-processing the coefficients of M-SVR model. In order to improve the performance of the orthogonal polynomial kernel M-SVR model, a kernel function iteration algorithm is introduced further. The proposed method take advantage of the information of all outputs to get robust meta-model. To validate the performance of the proposed method, two high-dimensional analytical functions and a hydrological model (HYMOD) with multiple outputs are examined, and a detailed comparison is made with the sparse polynomial chaos expansion meta-model developed in UQLab Toolbox. Results show that the proposed methods are efficient and accurate for GSA of the complex multivariate output models.},
  comment  = {Used UQLab to compare the new proposed method with sparse PCE for GSA.},
  doi      = {10.1007/s00158-018-2184-z},
  file     = {:Cheng2018a - Multivariate output global sensitivity analysis using multi-output support vector regression.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1007/s00158-018-2184-z},
}

@Article{Xu2019,
  author   = {Jun Xu and Shengzang Zhu},
  journal  = {Mechanical Systems and Signal Processing},
  title    = {An efficient approach for high-dimensional structural reliability analysis},
  year     = {2019},
  pages    = {151--170},
  volume   = {122},
  abstract = {This paper presents a new method to address the challenge of high-dimensional reliability analysis based on a small number of samples. The method is established based on the maximum entropy method (MEM) with the low-order fractional moments as constraints. A coordinate transformation is first implemented since a positive random variable is required for fractional operations. Then, an estimator-corrector scheme is employed to obtain the
probability density function (PDF) of the performance function. This scheme first promptly provides an estimated fractional orders and Lagrange multipliers by solving a linear system of equations as initial values, and then searches the more accurate solutions around the initial values to recover the PDF. Besides, the local optimum can be avoided by using the estimator-corrector scheme. The centered L2 (CL2) discrepancy oriented sequential Latin-hyper cube simulation is proposed to evaluate the low-order fractional moments involved in MEM, which is of critical importance to the efficiency and accuracy for high dimensional reliability analysis. Typical numerical examples are investigated to validate the proposed method. The results show that the proposed method is of efficacy for highdimensional reliability problems, even with very low failure probabilities.},
  comment  = {Used UQLab for benchmark},
  doi      = {10.1016/j.ymssp.2018.12.007},
  file     = {:Xu2019 - An efficient approach for high-dimensional structural reliability analysis.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1016/j.ymssp.2018.12.007},
}

@Article{Blomfors2019,
  author   = {Mattias Blomfors and Oskar Larsson Ivanov and Dániel Honfí and Morten Engen},
  title    = {Partial safety factors for the anchorage capacity of corroded reinforcement bars in concrete},
  journal  = {Engineering Structures},
  year     = {2019},
  volume   = {181},
  pages    = {579--588},
  abstract = {Many reinforced concrete bridges in Europe and around the world are damaged by reinforcement corrosion and the annual maintenance costs are enormous. It is therefore important to develop reliable methods to assess the structural capacity of corroded reinforced concrete structures and avoid unnecessary maintenance costs. Although there are advanced models for determining the load carrying capacity of structures, it is not obvious how they should be used to verify the performance of existing structures. To confidently assess the bond of corroded reinforcement in concrete, for example, the calculation model must give a sufficient safety margin. When designing new structures, semi-probabilistic approaches (such as the partial safety factor method) are adopted to achieve the target reliabilities specified in structural design codes. This paper uses probabilistic methods to develop partial factors for application in an existing bond model, to assess the safety of corroded reinforced concrete structures. The response of the bond model was studied using Monte Carlo (MC) simulations for several design cases, with probability distributions fitted to the results. Partial factors were then derived, based on these distributions. Furthermore, an MC-based simulation technique called “importance sampling” was used to study the reliability of several deterministic bond assessments conducted using these partial factors. The results show that deterministic assessments which use the proposed partial factors lead to a safety level at least equal to the target value. The results presented in this paper will support the assessment of reinforced concrete structures with anchorage problems and give a reasonable approximation of the anchorage capacity with sufficient safety margin. When generalised to cover other failure modes and structural configurations, this will enable better utilisation of damaged structures and lead to major environmental and economical savings for society.},
  doi      = {10.1016/j.engstruct.2018.12.011},
  file     = {:Blomfors2019 - Partial safety factors for the anchorage capacity of corroded reinforcement bars in concrete.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://doi.org/10.1016/j.engstruct.2018.12.011},
}

@Article{Cheng2019a,
  author   = {Xu Cheng and Guoyuan Li and Robert Skulstad and Shengyong Chen and Hans Petter Hildre and Houxiang Zhang},
  journal  = {IEEE Journal of Oceanic Engineering},
  title    = {A {Neural-Network-based} sensitivity analysis approach for data-driven modeling of ship motion},
  year     = {2019},
  abstract = {Researchers have been investigating data-driven modeling as a key way to achieve ship intelligence for years. This paper presents a novel data analysis approach to data-driven modeling of ship motion. We propose a global sensitivity analysis (GSA) approach combining artificial neural network (ANN) and sparse polynomial chaos expansion (SPCE) techniques to accommodate high-dimensional sensor data collected from ship motion. An ANN is constructed as a surrogate model to associate ship sensor data with a certain type of ship motion. To account for the computational efficiency of GSA, an SPCE is integrated into the GSA to decrease the need for Monte Carlo (MC) samples generated by the ANN. A probe variable is designed to couple with the MC samples, which plays a role in determining the degree of convergence of variable importance. A test on benchmark function demonstrates the efficiency and accuracy of the proposed approach. A case study of ship heading with and without environment effects is conducted. The experimental results show that the proposed approach can identify and rank the most sensitive factors of ship motion. The proposed approach highlights the application of GSA in data-driven modeling for ship intelligence.},
  comment  = {Used UQLab to construct SPCE in combination with ANN for high-dimensional sensor data collected from ship motion.},
  doi      = {10.1109/JOE.2018.2882276},
  file     = {:Cheng2019a - A Neural-Network-based sensitivity analysis approach for data-driven modeling of ship motion.pdf:PDF},
  keywords = {used UQLab, ocean engineering},
  url      = {http://dx.doi.org/10.1109/JOE.2018.2882276},
}

@Article{Sheng2019,
  author   = {Hao Sheng and Xiaozhe Wang},
  title    = {Probabilistic power flow using non-intrusive low-rank approximation method},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2019},
  abstract = {In this paper, a novel non-intrusive probabilistic power flow (PPF) analysis method based on the low-rank approximation (LRA) is proposed, which can accurately and efficiently estimate the probabilistic characteristics (e.g., mean, variance, probability density function) of the PPF solutions. This method aims at building up a statistically-equivalent surrogate for the PPF solutions through a small number of power flow evaluations. By exploiting the retained tensor-product form of the univariate polynomial basis, a sequential correction-updating scheme is applied, making the total number of unknowns to be linear rather than exponential to the number of random inputs. Consequently, the LRA method is particularly promising for dealing with high-dimensional problems with a large number of random inputs. Numerical studies on the IEEE 39-bus, 118-bus, and 1354-bus systems show that the proposed method can achieve accurate probabilistic characteristics of the PPF solutions with much less computational effort compared to the Monte Carlo simulations. Even compared to the polynomial chaos expansion method, the LRA method can achieve comparable accuracy, while the LRA method is more capable of handling higher-dimensional problems. Moreover, numerical results reveal that the randomness brought about by the renewable energy resources and loads may inevitably affect the feasibility of dispatch/planning schemes.},
  doi      = {10.1109/TPWRS.2019.2896219},
  file     = {:Sheng2019 - Probabilistic power flow using non-intrusive low-rank approximation method.pdf:PDF},
  keywords = {used UQLab, electrical engineering},
  url      = {https://doi.org/10.1109/TPWRS.2019.2896219},
}

@Article{Koohbor2019,
  author   = {Behshad Koohbor and Marwan Fahs and Behzad Ataie-Ashtiani and Benjamin Belfort and Craig T.Simmons and Anis Younes},
  journal  = {Journal of Hydrology},
  title    = {Uncertainty analysis for seawater intrusion in fractured coastal aquifers: {Effects} of fracture location, aperture, density and hydrodynamic parameters},
  year     = {2019},
  pages    = {159--177},
  volume   = {571},
  abstract = {In this study we use polynomial chaos expansion (PCE) to perform uncertainty analysis for seawater intrusion (SWI) in fractured coastal aquifers (FCAs) which is simulated using the coupled discrete fracture network (DFN) and variable-density flow (VDF) models. The DFN-VDF model requires detailed discontinuous analysis of the fractures. In real field applications, these characteristics are usually uncertain which may have a major effect on the predictive capability of the model. Thus, we perform global sensitivity analysis (GSA) to provide a preliminary assessment on how these uncertainties can affect the model outputs. As our conceptual model, we consider fractured configurations of the Henry Problem which is widely used to understand SWI processes. A finite element DFN-VDF model is developed in the framework of COMSOL Multiphysics. We examine the uncertainty of several SWI metrics and salinity distribution due to the incomplete knowledge of fracture characteristics. PCE is used as a surrogate model to reduce the computational burden. A new sparse PCE technique is used to allow for high polynomial orders at low computational cost. The Sobol’ indices (SIs) are used as sensitivity measures to identify the key variables driving the model outputs uncertainties. The proposed GSA methodology based on PCE and SIs is useful for identifying the source of uncertainties on the model outputs with an affordable computational cost and an acceptable accuracy. It shows that fracture hydraulic conductivity is the first source of uncertainty on the salinity distribution. The imperfect knowledge of fracture location and density affects mainly the toe position and the total flux of saltwater entering the aquifer. Marginal effects based on the PCE are used to understand the effects of fracture characteristics on SWI. The findings provide a technical support for monitoring, controlling and preventing SWI in FCAs.},
  comment  = {Used UQLab to compute total Sobol' indices with PCE.},
  doi      = {10.1016/j.jhydrol.2019.01.052},
  file     = {:Koohbor2019 - Uncertainty analysis for seawater intrusion in fractured coastal aquifers_ Effects of fracture location, aperture, density and hydrodynamic parameters.pdf:PDF},
  keywords = {used UQLab, hydrology},
  url      = {https://doi.org/10.1016/j.jhydrol.2019.01.052},
}

@Article{Xie2019,
  author   = {Xiangzhong Xie and René Schenkendorf},
  journal  = {Computer \& Chemical Engineering},
  title    = {Stochastic back-off-based robust process design for continuous crystallization of ibuprofen},
  year     = {2019},
  pages    = {80--92},
  volume   = {124},
  abstract = {Robust model-based process design in continuous pharmaceutical manufacturing aims to implement quality by design principles under uncertainty. Notably, various studies have discussed the back-off concept to solve the underlying robust optimization problem; however, for the concept to have practical value, its efficiency and convergence must be improved. In this work, we introduce a novel, highly efficient stochastic back-off strategy. Instead of using statistical moments of limited validity, we incorporate the full statistical information of the constraints to solve the robust process design problem. To ensure manageable computational costs, we make use of polynomial chaos expansion for uncertainty quantification and propagation. The proposed concept is demonstrated with the design of a tubular crystallizer for ibuprofen crystallization. The results show that the novel stochastic back-off strategy is considerably faster compared with the standard back-off concept and provides more reliable quality by design results in general.},
  comment  = {Used UQLab to construct PCE.},
  doi      = {10.1016/j.compchemeng.2019.02.009},
  file     = {:Xie2019 - Stochastic back-off-based robust process design for continuous crystallization of ibuprofen.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.02.009},
}

@Article{Dalemans2019,
  author   = {Floris Dalemans and Bart Muys and Miet Maertens},
  journal  = {GCB Bioenergy},
  title    = {A framework for profitability evaluation of agroforestry‐based biofuel value chains: {An} application to pongamia in {India}},
  year     = {2019},
  pages    = {1--19},
  abstract = {Biofuel production from oilseed trees in small‐scale agroforestry systems is considered as a strategy for energy security, rural development and ecosystem services provision in low‐income countries. However, the economic potential of these systems remains unclear, as profitability studies commonly ignore key methodological issues such as quantitative uncertainty analysis, full accounting for opportunity costs, and inclusion of all value chain actors. This study addresses these methodological shortcomings and develops a framework for quantifying the long‐term financial performance of agroforestry‐based biofuel value chains. The framework is applied to a case in South India, to calculate profitability of pongamia (Millettia pinnata) cultivation and processing. The results show that pongamia cultivation has limited financial potential, and is only profitable in small‐scale settings, in the middle to long term and for a subset of farmers. If biodiesel is envisaged as the end product, the value chain requires substantial fiscal and marketing support to be economically viable. For current prices, financial performance is much higher if the seed oil is marketed instead of processed to biodiesel. Increased mechanization, increased yields and optimized agroforestry set‐ups might improve financial outcomes and reduce risks for both farmers and processors. These findings are case‐specific, while the developed framework opens the door to comprehensive investigation of the financial performance of other oilseed tree species and in other regions.},
  comment  = {Used UQLab for Monte Carlo simulation.},
  doi      = {10.1111/gcbb.12605},
  file     = {:Dalemans2019 - A framework for profitability evaluation of agroforestry‐based biofuel value chains_ An application to pongamia in India.pdf:PDF},
  keywords = {used UQLab, energy engineering},
  url      = {https://dx.doi.org/10.1111/gcbb.12605},
}

@Article{Papaioannou2019,
  author   = {Iason Papaioannou and Max Ehre and Daniel Straub},
  journal  = {Journal of Computational Physics},
  title    = {{PLS}-based adaptation for efficient {PCE} representation in high dimensions},
  year     = {2019},
  pages    = {186--204},
  volume   = {387},
  abstract = {Uncertainty quantification of engineering systems modeled by computationally intensive numerical models remains a challenging task, despite the increase in computer power. Efficient uncertainty propagation of such models can be performed by use of surrogate models, such as polynomial chaos expansions (PCE). A major drawback of standard PCE is that its predictive ability decreases with increase of the problem dimension for a fixed computational budget. This is related to the fact that the number of terms in the expansion increases fast with the input variable dimension. To address this issue, Tipireddy and Ghanem (2014) introduced a sparse PCE representation based on a transformation of the coordinate system in Gaussian input variable spaces. In this contribution, we propose to identify the projection operator underlying this transformation and approximate the coefficients of the resulting PCE through partial least squares (PLS) analysis. The proposed PCE-driven PLS algorithm identifies the directions with the largest predictive significance in the PCE representation based on a set of samples from the input random variables and corresponding response variable. This approach does not require gradient evaluations, which makes it efficient for high dimensional problems with black-box numerical models. We assess the proposed approach with three numerical examples in high-dimensional input spaces, comparing its performance with low-rank tensor approximations. These examples demonstrate that the PLS-based PCE method provides accurate representations even for strongly non-linear problems.},
  comment  = {Used LRA in UQLab to compare results with the proposed method of the paper.},
  doi      = {10.1016/j.jcp.2019.02.046},
  file     = {:Papaioannou2019 - PLS-based adaptation for efficient PCE representation in high dimensions.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1016/j.jcp.2019.02.046},
}

@Article{Jones2019,
  author   = {Mark Nicholas Jones and Jérôme Frutiger and Nevin Gerek Ince and Gürkan Sin},
  journal  = {Computers \& Chemical Engineering},
  title    = {The {Monte Carlo} driven and machine learning enhanced process simulator},
  year     = {2019},
  pages    = {324--338},
  volume   = {125},
  abstract = {This study presents a methodology with tools integration to apply advanced uncertainty propagation and sensitivity analysis in connection with commercial process simulation software. The methodology was applied to two processes: a heat pump system and a molecular distillation process. The input parameters of the selected thermodynamic model, namely critical temperature, critical pressure and acentric factor, were considered as a source of uncertainty and analysed using Monte Carlo sampling techniques. This enabled the process model output uncertainty to be described as an empirical distribution function with a 95\% confidence interval. Variance-based decomposition such as the Sobol method or standard regression were used to analyse the sensitivity of the respective properties. We also show that machine learning methods such as polynomial chaos expansion (PCE) can be applied to reduce the number of necessary process simulations and obtained equivalent results in comparison with the more costly full Monte Carlo based procedure.},
  comment  = {Used UQLab to construct sparse PCE with emphasis on the adaptive algorithm.},
  doi      = {10.1016/j.compchemeng.2019.03.016},
  file     = {:Jones2019 - The Monte Carlo driven and machine learning enhanced process simulator.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.03.016},
}

@Article{Zhou2019,
  author   = {Yicheng Zhou and Zhenzhou Lu and Kai Cheng and Yan Shi},
  journal  = {Mechanical Systems and Signal Processing},
  title    = {An expanded sparse {Bayesian} learning method for polynomial chaos expansion},
  year     = {2019},
  pages    = {153--171},
  volume   = {128},
  abstract = {Polynomial chaos expansion (PCE) has been proven to be a powerful tool for developing surrogate models in various engineering fields for uncertainty quantification. The computational cost of full PCE is unaffordable due to the “curse of dimensionality” of the expansion coefficients. In this paper, an expanded sparse Bayesian learning method for sparse PCE is proposed. Firstly, basis polynomials of the full PCE are partitioned into significant terms and complementary non-significant terms. The parameterized priors with distinct variance are assigned to the candidates for the significant terms. Then, the dimensionality of the parameter space is equivalent to the assumed sparsity level of the PCE. Secondly, an approximate Kashyap information criterion (KIC) rule which achieves a balance between model simplicity and goodness of fit is derived for model selection. Finally, an automatic search algorithm is proposed by minimizing the KIC objective function and using the variance contribution of each term to the model output to select significant terms. To assess the performance of the proposed method, a detailed comparison is completed with several well-established techniques. The results show that the proposed method is able to identify the most significant PC contributions with superior efficiency and accuracy.},
  comment  = {Used LAR PCE of UQLab to benchmark the proposed method of the paper.},
  doi      = {10.1016/j.ymssp.2019.03.032},
  file     = {:Zhou2019 - An expanded sparse Bayesian learning method for polynomial chaos expansion.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1016/j.ymssp.2019.03.032},
}

@Article{Tan2019,
  author   = {Fengjie Tan and Tom Lahmer},
  journal  = {Frontiers of Structural and Civil Engineering},
  title    = {Shape design of arch dams under load uncertainties with robust optimization},
  year     = {2019},
  abstract = {Due to an increased need in hydro-electricity, water storage, and flood protection, it is assumed that a series of new dams will be build throughout the world. The focus of this paper is on the non-probabilistic-based design of new arch-type dams by applying means of robust design optimization (RDO). This type of optimization takes into account uncertainties in the loads and in the material properties of the structure. As classical procedures of probabilistic-based optimization under uncertainties, such as RDO and reliability-based design optimization (RBDO), are in general computationally expensive and rely on estimates of the system’s response variance, we will not follow a full-probabilistic approach but work with predefined confidence levels. This leads to a bi-level optimization program where the volume of the dam is optimized under the worst combination of the uncertain parameters. As a result, robust and reliable designs are obtained and the result is independent from any assumptions on stochastic properties of the random variables in the model. The optimization of an arch-type dam is realized here by a robust optimization method under load uncertainty, where hydraulic and thermal loads are considered. The load uncertainty is modeled as an ellipsoidal expression. Comparing with any traditional deterministic optimization method, which only concerns the minimum objective value and offers a solution candidate close to limit-states, the RDO method provides a robust solution against uncertainty. To reduce the computational cost, a ranking strategy and an approximation model are further involved to do a preliminary screening. By this means, the robust design can generate an improved arch dam structure that ensures both safety and serviceability during its lifetime.},
  comment  = {Used Kriging tool of UQLab to construct a metamodel for predicting tensile stress and dam volume.},
  doi      = {10.1007/s11709-019-0522-x},
  file     = {:Tan2019 - Shape design of arch dams under load uncertainties with robust optimization.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://doi.org/10.1007/s11709-019-0522-x},
}

@Article{Loukrezis2019,
  author   = {Dimitrios Loukrezis and Herbert De Gersem},
  journal  = {Algorithms},
  title    = {Approximation and uncertainty quantification of stochastic systems with arbitrary input distributions using weighted {Leja} interpolation},
  year     = {2020},
  number   = {3},
  volume   = {13},
  abstract = {Approximation and uncertainty quantification methods based on Lagrange interpolation are typically abandoned in cases where the probability distributions of one or more system parameters are not normal, uniform, or closely related distributions, due to the computational issues that arise when one wishes to define interpolation nodes for general distributions. This paper examines the use of the recently introduced weighted Leja nodes for that purpose. Weighted Leja interpolation rules are presented, along with a dimension-adaptive sparse interpolation algorithm, to be employed in the case of high-dimensional input uncertainty. The performance and reliability of the suggested approach is verified by four numerical experiments, where the respective models feature extreme value and truncated normal parameter distributions. Furthermore, the suggested approach is compared with a well-established polynomial chaos method and found to be either comparable or superior in terms of approximation and statistics estimation accuracy.},
  comment  = {Used sparse PCE based on degree-adaptive algorithm (LAR) of UQLab to benchmark the proposed method of the paper.},
  doi      = {10.3390/a13030051},
  file     = {:Loukrezis2019 - Approximation and Uncertainty Quantification of Stochastic Systems with Arbitrary Input Distributions Using Weighted Leja Interpolation.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.3390/a13030051},
}

@InProceedings{Borowiec2017,
  author       = {K. Borowiec and C. Wang and Tomasz Kozlowski and C. S. Brooks},
  booktitle    = {Transactions of the American Nuclear Society},
  title        = {Uncertainty quantification for steady-state {PSBT} benchmark using surrogate models},
  year         = {2017},
  address      = {Washington, D.C., USA},
  organization = {American Nuclear Society},
  volume       = {117},
  comment      = {Used UQLab to create PCK metamodel.},
  eventdate    = {2017-10-29/2017-11-02},
  file         = {:Borowiec2017 - Uncertainty quantification for steady-state PSBT benchmark using surrogate models.pdf:PDF},
  keywords     = {Used UQLab, nuclear engineering},
  url          = {https://www.scopus.com/record/display.uri?eid=2-s2.0-85062036963&origin=resultslist},
}

@InProceedings{Wang2017,
  author    = {Chen Wang and Xu Wu and Tomasz Kozlowski},
  booktitle = {17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH) 2017},
  title     = {Surrogate-based inverse uncertainty quantification of {TRACE} physical model parameters using steady-state {PSBT} void fraction data},
  year      = {2017},
  address   = {Xi'an, Shaanxi, China},
  abstract  = {In the framework of BEPU (Best Estimate Plus Uncertainty) methodology, the uncertainties involved in simulations must be quantified to prove that the investigated design is reasonable and acceptable. The uncertainties in predictions are usually calculated by propagating input uncertainties through the simulation model, which requires prior knowledge of the model or code input uncertainties, for example, the means, variances, distribution types, etc. However, in best-estimate system thermal-hydraulics codes such as TRACE, some parameters in empirical correlations may have large uncertainties which are unknown to code users. So, the uncertainties associated these parameters are simply ignored or described by “expert opinion”. Inverse Uncertainty Quantification (UQ) is performed in the current study to replace such ad-hoc expert judgment. Inverse UQ is the process of quantifying the uncertainties in input parameters given relevant experimental measurements. The purpose of inverse UQ, and this paper, is to seek statistical descriptions of the input model parameters that are consistent with the observed data. Bayesian analysis is used to formulate the inverse UQ problem given relevant experiment data. In this study, the steady-state PSBT benchmark void fraction data is used. Within the Bayesian framework we seek the posterior distributions of the uncertain TRACE modeling parameters, which is updated from our prior knowledge given measurement data. Markov Chain Monte Carlo (MCMC) method is used to explore the posterior distributions, and surrogate models of TRACE are used to alleviate the computational burden. Gaussian Process (GP) is used to construct the surrogate model which can reduce the simulation time significantly. The outcomes will be the posterior distributions of several modeling parameters that are significant to PSBT experiment. Results of inverse UQ can be used for future forward uncertainty propagation and validation analysis, which will be presented in a companion paper.},
  comment   = {Used UQLab for sensitivity analysis (Sobol')},
  eventdate = {2017-09-3/2017-09-08},
  file      = {:Wang2017 - Surrogate-based inverse uncertainty quantification of TRACE physical model parameters using steady-state PSBT void fraction data.pdf:PDF},
  keywords  = {used UQLab, nuclear engineering},
  url       = {https://www.researchgate.net/publication/321974326_Surrogate-based_Inverse_Uncertainty_Quantification_of_TRACE_Physical_Model_Parameters_using_Steady-State_PSBT_Void_Fraction_Data},
}

@InProceedings{Wang2017a,
  author    = {Chen Wang and Xu Wu and Tomasz Kozlowski},
  booktitle = {17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH) 2017},
  title     = {Sensitivity and uncertainty analysis of {TRACE} physical model parameters based on {PSBT} benchmark using {Gaussian} process emulator},
  year      = {2017},
  address   = {Xi'an, Shaanxi, China},
  abstract  = {For best estimate thermal-hydraulics codes like TRACE and RELAP5, one major source of uncertainties is the inaccuracy of closure laws (correlations) which are used to describe transfer terms in balance equations. These closure laws were originally studied in separate-effect experiments but are implemented in thermal-hydraulics codes and used for different conditions. Thus, the model parameters involved in such closure laws may have significant influences on model outputs but are subject to considerable uncertainties. The aim of this paper is to perform sensitivity and uncertainty study with respect to selected physical model parameters in TRACE. The PSBT steady-state void fraction data is used as Quantity-of-Interest (QoI). Statistics of these physical model parameters are obtained by inverse Uncertainty Quantification (UQ) from a companion paper. Sensitivity Analysis (SA) aims at determining how uncertain input parameters contribute to the variations of outputs. In this paper, the Sobol' indices are used as a sensitivity measure, which quantify how the variation in a QoI can be apportioned to each input factor. Based on the results from SA, several input parameters that have significant effects on the QoI are selected for the UQ process. The prediction uncertainty is evaluated by considering the propagation of uncertainties in selected TRACE physical model parameters. As thousands of TRACE runs are needed for the sensitivity and uncertainty studies, the use of direct Monte Carlo sampling is not a viable solution. We developed Gaussian Process (GP) emulator as a surrogate model of TRACE to reduce the computation time substantially.},
  comment   = {Used UQLab for Sobol' sensitivity indices, PCE and GP metamodels.},
  eventdate = {2017-09-3/2017-09-08},
  file      = {:Wang2017a - Sensitivity and uncertainty analysis of TRACE physical model parameters based on PSBT benchmark using Gaussian process emulator.pdf:PDF},
  keywords  = {used UQLab, nuclear engineering},
  url       = {https://www.researchgate.net/publication/321974324_Sensitivity_and_Uncertainty_Analysis_of_TRACE_Physical_Model_Parameters_based_on_PSBT_benchmark_using_Gaussian_Process_Emulator},
}

@InProceedings{Lagouanelle2019,
  author       = {Paul Lagouanelle and Vang-Lang Krauth and Lionel Pichon},
  booktitle    = {2019 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE)},
  title        = {Uncertainty Quantification in the Assessment of Human Exposure near Wireless Power Transfer Systems in Automotive Applications},
  year         = {2019},
  address      = {Torino, Italy},
  organization = {IEEE},
  pages        = {1--5},
  abstract     = {This paper addresses the uncertainty quantification of  physical  and  geometrical  material  parameters  in  the  design  of wireless power transfer systems and in the assessment of the level  of  exposure  for  automotive  applications.  In  a  first  step,  Monte  Carlo  simulations  are  used  to  obtain  the  mean  and  confidence interval of the shielding effectiveness for conducting and composite materials over a wide frequency range in case of an  academic  shielding  configuration.  In  a  second  step,  a  non-intrusive  stochastics  technique  (Kriging  and  Polynomial  chaos  expansions)  are  combined  with  a  3D  finite  element  method  to  meta-models   in   order   to   study   a   simplified   but   realistic   configuration  of  a  power  transfer  system  at  85  kHz.  Such  an  approach  provides  fast  predictions  of  radiated  field  values  taking  into  account  the  variability  of  the  parameters  and  may  be useful to obtain the sensitivity of the model response.},
  comment      = {Used Kriging as a metamodeling tool.},
  doi          = {10.23919/EETA.2019.8804593},
  eventdate    = {2019-07-02/2019-07-04},
  file         = {:Lagouanelle2019 - Uncertainty Quantification in the Assessment of Human Exposure near Wireless Power Transfer Systems in Automotive Applications.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  url          = {https://dx.doi.org/10.23919/EETA.2019.8804593},
}

@Article{Al2019,
  author   = {Resul Al and Chitta Ranjan Behera and Alexandr Zubov and Krist V.Gernaey and GürkanSin},
  journal  = {Computers and Chemical Engineering},
  title    = {Meta-modeling based efficient global sensitivity analysis for wastewater treatment plants – An application to the {BSM2} model},
  year     = {2019},
  pages    = {223--246},
  volume   = {127},
  abstract = {Global sensitivity analysis (GSA) is a powerful tool for quantifying the effects of model parameters on the performance outputs of engineering systems, such as wastewater treatment plants (WWTP). Due to the ever-growing sophistication of such systems and their models, significantly longer processing times are required to perform a system-wide simulation, which makes the use of traditional Monte Carlo (MC) based approaches for calculation of GSA measures, such as Sobol indices, impractical. In this work, we present a systematic framework to construct and validate highly accurate meta-models to perform an efficient GSA of complex WWTP models such as the Benchmark Simulation Model No. 2 (BSM2). The robustness and the efficacy of three meta-modeling approaches, namely polynomial chaos expansion (PCE), Gaussian process regression (GPR), and artificial neural networks (ANN), are tested on four engineering scenarios. The results reveal significant computational gains of the proposed framework over the MC-based approach without compromising accuracy.},
  comment  = {Used UQLab to create a PCE metamodels.},
  doi      = {10.1016/j.compchemeng.2019.05.015},
  file     = {:Al2019 - Meta-modeling based efficient global sensitivity analysis for wastewater treatment plants – An application to the BSM2 model.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.05.015},
}

@MastersThesis{Ligeikis2019,
  author   = {Connor Ligeikis},
  school   = {University of Connecticut},
  title    = {Exploring Uncertainty in Real-Time Hybrid Substructuring},
  year     = {2019},
  address  = {USA},
  type     = {mathesis},
  abstract = {Real-time hybrid substructuring (RTHS) is a cyber-physical vibration testing method that partitions a structural system into numerical and physical substructures. RTHS incorporates sensing, actuation, and computing technologies to couple these substructures in real time. RTHS can be used to realistically examine the performance of structural systems with rate-dependent structural components that may be difficult to accurately model. However, uncertainty can be found in many aspects of RTHS testing including noise in sensor measurements, actuator performance and tracking, non-linearities in the physical test specimen, and numerical modeling assumptions. In the numerical substructure, uncertainty in material properties, stiffness, damping, or geometry may be considered random variables. Certain physical substructure characteristics may also be similarly parameterized. This thesis aims to develop and experimentally validate techniques that incorporate these parametric uncertainties into RTHS testing protocols, thereby improving the robustness of the RTHS method. This goal was accomplished through two studies.

First, a study was performed to extend and experimentally validate a proposed structural reliability method called Adaptive Kriging-Hybrid Simulation (AK-HS). The method combines a metamodeling technique known as Kriging, an adaptive learning algorithm, the Monte Carlo method, and RTHS testing to iteratively estimate a structural system’s probability of failure given random parameters in the numerical substructure. The method was validated with a series of bench-scale RTHS tests of a Taylor Devices, Inc. viscous damper connecting two adjacent 6-degree-of-freedom rigid body structures. The AK-HS method was found to accurately predict probabilities of failure for systems with up to 24 random variables using a reasonable number of RTHS tests.

The second study proposed and validated a method that can be used to develop metamodels of a system’s frequency response functions using Principal Component Analysis, Kriging, and RTHS. The proposed method was experimentally validated through a series of bench-scale RTHS tests of a Lord Corporation magnetorheological fluid damper controlling vibrations in a 2-degree-of-freedom mass-spring system subjected to an input ground acceleration. Uncertainty was introduced to the system by treating the numerical substructure spring stiffnesses and the physical damper current as random variables. It is shown that accurate, statistical metamodels can be created using a small number of RTHS tests. These metamodels may then be used to conduct Monte Carlo simulations to obtain distributions of the system’s frequency domain response behavior.},
  comment  = {Used Kriging in UQLab.},
  file     = {:Ligeikis2019 - Exploring Uncertainty in Real-Time Hybrid Substructuring.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://opencommons.uconn.edu/gs_theses/1353/},
}

@InProceedings{Maze-Merceur2019,
  author       = {G. Mazé-Merceur and B. Etchessahar and Jean-Michel Geffrin and Amélie Litman and Antoine Roueff and Philippe Besnie},
  booktitle    = {13th European Conference on Antennas and Propagation (EuCAP 2019)},
  title        = {Satisfaction Indicators Taking into Account the Measurement and Computation Uncertainties for the Comparison of Data in Electromagnetics: Motivations and Scheduled Tasks of the {French National Working Group} {CDIIS}},
  year         = {2019},
  address      = {Krakow, Poland},
  organization = {IEEE},
  abstract     = {A  national  study  of  criteria  able  to  provide  a  satisfac-tion indicator about the comparison of data from electromagnetic measurement  and  computation,  taking  into  account  their  associ-ated  uncertainties,  has  been  organized  in  the  framework  of  a French  Working  Group  of  the  GdR  Ondes,  called  CDIIS  (Com-paraison  de  Données  entachées  d’Incertitudes:  Indicateurs  de Satisfaction). This  Working  Group involves  several industrial or academic research laboratories, including laboratories depending on  governmental  organisms.  Four  tasks  have  been  defined:  1/ Identification  of  a  set  of  satisfaction  indicators.  2/  Identification of  pertinent  test  cases  in  different  application  domains  of  elec-tromagnetics.  3/  Application  of  the  above  criteria  on  the  test cases.  4/  Conclusions:  which  criterion  is  best  adapted  to  a  given electromagnetism  problem.  This  paper  deals  with  Task  1  and discusses  the  results  of  various  indicators  applied  to  a  canonical RCS (Radar Cross Section) test case.},
  comment      = {Used MCS in UQLab.},
  eventdate    = {2019-03-31/2019-04-05},
  file         = {:2019 - Satisfaction Indicators Taking into Account the Measurement and Computation Uncertainties for the Comparison of Data in Electromagnetics_ Motivations and Scheduled Tasks of the French National Working Group CDIIS.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  url          = {https://hal.archives-ouvertes.fr/hal-02133573/},
}

@InCollection{Ligeikis2019a,
  author    = {Connor Ligeikis and Richard Christenson},
  booktitle = {Model Validation and Uncertainty Quantification, Volume 3. Conference Proceedings of the Society for Experimental Mechanics Series.},
  publisher = {Springer},
  title     = {Incorporating Uncertainty in the Physical Substructure During Hybrid Substructuring},
  year      = {2019},
  chapter   = {27},
  editor    = {Robert Barthorpe},
  pages     = {237--239},
  abstract  = {In hybrid substructuring, a structural system is partitioned into a numerical substructure and a physical substructure. Typically, the physical substructure consists of a system component whose behavior is difficult to model while the numerical substructure consists of a computational model of the remainder of the system. Hybrid substructuring has previously been shown to be an effective method to quantify the effect of parametric uncertainties in the numerical substructure on the response of the system. This paper proposes and implements a methodology where the effect of parametric uncertainty can also be incorporated into the physical substructure. This idea is implemented in a series of small-scale Real-Time Hybrid Substructuring (RTHS) tests on a magneto-rheological fluid damper used to control a two degree-of-freedom mass-spring system. The physical current supplied to the damper is treated as a random variable. Using the RTHS test results, a metamodel of the system’s frequency domain behavior is developed using Principal Component Analysis and Kriging. This metamodel is then used to evaluate probabilistic system performance.},
  comment   = {Used Kriging in UQLab.},
  doi       = {10.1007/978-3-030-12075-7_27},
  file      = {:Ligeikis2019a - Incorporating Uncertainty in the Physical Substructure During Hybrid Substructuring.pdf:PDF},
  keywords  = {used UQLab, civil engineering},
  url       = {https://doi.org/10.1007/978-3-030-12075-7_27},
}

@Article{Hamdi2019,
  author   = {Hamidreza Hamdi and Christopher R. Clarkson and Ali Esmail and Mario Costa Sousa},
  journal  = {Society of Petroleum Engineers},
  title    = {A {Bayesian} Approach for Optimizing the {Huff-n-Puff} Gas Injection Performance in Shale Reservoirs Under Parametric Uncertainty: A {Duvernay Shale} Example},
  year     = {2019},
  pages    = {1--23},
  abstract = {Recent  studies  have  indicated  that  Huff-n-Puff  (HNP)  gas  injection  has  the  potential  to  recover  anadditional 30-70\% oil from multi-fractured horizontal wells in shale reservoirs. Nonetheless, this techniqueis very sensitive to production constraints and is impacted by uncertainty related to measurement quality(particularly frequency and resolution), and lack of constraining data. In this paper, a Bayesian workflow isprovided to optimize the HNP process under uncertainty using a Duvernay shale well as an example.Compositional simulations are conducted which incorporate a tuned PVT model and a set of measuredcyclic injection/compaction pressure-sensitive permeability data. Markov chain Monte Carlo (McMC) is used to estimate  the  posterior  distributions  of  the  model  uncertain  variables  by  matching  the  primaryproduction data. The McMC process is accelerated by employing an accurate proxy model (kriging) whichis updated using a highly adaptive sampling algorithm. Gaussian Processes are then used to optimize theHNP control variables by maximizing the lower confidence interval (μ-σ) of cumulative oil production(after 10 years) across a fixed ensemble of uncertain variables sampled from posterior distributions.The uncertain variable space includes several parameters representing reservoir and fracture properties.The posterior distributions for some parameters, such as primary fracture permeability and effective half-length, are narrower, while wider distributions are obtained for other parameters. The results indicate thatthe impact of uncertain variables on HNP performance is nonlinear. Some uncertain variables (such asmolecular diffusion) that do not show strong sensitivity during the primary production strongly impact gasinjection HNP performance. The results of optimization under uncertainty confirm that the lower confidenceinterval  of  cumulative  oil  production  can  be  maximized  by  an  injection  time  of  around 1.5 months,  aproduction time of around 2.5 months, and very short soaking times. In addition, a maximum injection rateand a flowing bottomhole pressure around the bubble point are required to ensure maximum incrementalrecovery. Analysis of the objective function surface highlights some other sets of production constraintswith competitive results. Finally, the optimal set of production constraints, in combination with an ensembleof uncertain variables, results in a median HNP cumulative oil production that is 30\% greater than that forprimary production. 

The application of a Bayesian framework for optimizing the HNP performance in a real shale reservoir isintroduced for the first time. This work provides practical guidelines for the efficient application of advancedmachine learning techniques for optimization under uncertainty, resulting in better decision making.},
  comment  = {Used Sobol' (PCE-based) and Cotter sensitivity indices.},
  doi      = {10.2118/195438-MS},
  file     = {:Hamdi2019 - A Bayesian Approach for Optimizing the Huff-n-Puff Gas Injection Performance in Shale Reservoirs Under Parametric Uncertainty_ A Duvernay Shale Example.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://doi.org/10.2118/195438-MS},
}

@InProceedings{Gorniak2019,
  author       = {Piotr Górniak},
  booktitle    = {13th European Conference on Antennas and Propagation (EuCAP 2019)},
  title        = {New Analytical {PCE} Coefficients for Uncertainty Quantification in Ray-Tracing Modeling},
  year         = {2019},
  address      = {Krakow, Poland},
  organization = {IEEE},
  abstract     = {Ray-tracing simulations of stochastic electromagnetic fields are considered in the paper. The author uses polynomial chaos expansion (PCE) coefficients for uncertainty quantification. The author introduces the new effective method, in terms of accuracy and calculation speed, for analytical derivation of PCE coefficients in ray-tracing modelling. The analytical PCE coefficients can be recalculated very fast when it is necessary to change probability densities of random variables of a simulation. A ray-tracing simulation of exemplary indoor scenario is used to compare the new method with general polynomial chaos (gPC) approach which requires numerical calculation of PCE coefficients for each set of probability densities of simulation random variables.},
  comment      = {Used PCE in UQLab},
  eventdate    = {2019-03-31/2019-04-05},
  file         = {:Gorniak2019 - New Analytical PCE Coefficients for Uncertainty Quantification in Ray-Tracing Modeling.pdf:PDF},
  keywords     = {used UQLab, computational science and engineering},
  url          = {https://ieeexplore.ieee.org/document/8739844},
}

@Article{Zhou2019b,
  author   = {Yicheng Zhou and Zhenzhou Lu and Kai Cheng},
  journal  = {International Journal for Numerical Methods in Engineering},
  title    = {A new surrogate modeling method combining polynomial chaos expansion and {Gaussian} kernel in a sparse {Bayesian} learning framework},
  year     = {2019},
  pages    = {1--19},
  abstract = {Surrogate modeling techniques have been increasingly developed for optimization and uncertainty quantification problems in many engineering fields. The development of surrogates requires modeling high‐dimensional and nonsmooth functions with limited information. To this end, the hybrid surrogate modeling method, where different surrogate models are combined, offers an effective solution. In this paper, a new hybrid modeling technique is proposed by combining polynomial chaos expansion and kernel function in a sparse Bayesian learning framework. The proposed hybrid model possesses both the global characteristic advantage of polynomial chaos expansion and the local characteristic advantage of the Gaussian kernel. The parameterized priors are utilized to encourage the sparsity of the model. Moreover, an optimization algorithm aiming at maximizing Bayesian evidence is proposed for parameter optimization. To assess the performance of the proposed method, a detailed comparison is made with the well‐established PC‐Kriging technique. The results show that the proposed method is superior in terms of accuracy and robustness.},
  comment  = {Used Sequential PC-Kriging and Optimal PC-Kriging in UQLab for comparison purposes.},
  doi      = {10.1002/nme.6145},
  file     = {:Zhou2019b - A new surrogate modeling method combining polynomial chaos expansion and Gaussian kernel in a sparse Bayesian learning framework.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1002/nme.6145},
}

@Article{Bernier2019,
  author   = {Carl Bernier and Jamie E. Padgett},
  journal  = {Reliability Engineering and System Safety},
  title    = {Fragility and risk assessment of aboveground storage tanks subjected to concurrent surge, wave, and wind loads},
  year     = {2019},
  pages    = {1--11},
  volume   = {191},
  abstract = {Comprehensive tools to assess the performance of aboveground storage tanks (ASTs) under multi-hazard storm conditions are currently lacking, despite the severe damage suffered by ASTs in past storms resulting in the release of hazardous substances. This paper presents a rigorous yet efficient methodology to develop fragility models and perform risk assessments of ASTs subjected to combined surge, wave, and wind loads. Parametrized fragility models are derived for buckling and dislocation from the ground. The buckling strength of ASTs is assessed using finite element analysis, while the stability against dislocation is evaluated using analytical limit state functions with surrogate modeling-based load models. Scenario and probabilistic risk assessments are then performed for a case study region by convolving the fragility models with hazard models. Results demonstrate that the derived fragility models are efficient tools to evaluate the performance of ASTs in industrial regions. Insights obtained from the fragility and risk assessments reveal that neglecting the multi-hazard nature of storms, as existing studies have done, can lead to a significant underestimation of vulnerability and risks. This paper also highlights how using surrogate model techniques can facilitate and reduce the computational complexity of fragility and risk assessments, particularly in multi-hazard settings.},
  comment  = {Used Kriging in UQLab.},
  doi      = {10.1016/j.ress.2019.106571},
  file     = {:Bernier2019 - Fragility and risk assessment of aboveground storage tanks subjected to concurrent surge, wave, and wind loads.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://doi.org/10.1016/j.ress.2019.106571},
}

@Article{Tran2019,
  author   = {Vinh Ngoc Tran and Jongho Kim},
  journal  = {Stochastic Environmental Research and Risk Assessment},
  title    = {Quantification of predictive uncertainty with a metamodel: toward more efficient hydrologic simulations},
  year     = {2019},
  number   = {7},
  pages    = {1453--1476},
  volume   = {33},
  abstract = {Hydrologic flood prediction has been a quite complex and difficult task because of various sources of inherent uncertainty. Accurately quantifying these uncertainties plays a significant role in providing flood warnings and mitigating risk, but it is time-consuming. To offset the cost of quantifying the uncertainty, we adopted a highly efficient metamodel based on polynomial chaos expansion (PCE) theory and applied it to a lumped, deterministic rainfall–runoff model (Nedb{\o}r–Afstr{\o}mnings model, NAM) combined with generalized likelihood uncertainty estimation (GLUE). The central conclusions are: (1) the subjective aspects of GLUE (e.g., the cutoff threshold values of likelihood function) are investigated for 8 flood events that occurred in the Thu bon river watershed in Vietnam, resulting that the values of 0.82 for Nash–Sutcliffe efficiency, 4.05\% for peak error, and 4.35\% for volume error are determined as the acceptance thresholds. Moreover, the number of ensemble behavioral sets required to maintain the sufficient range of uncertainty but to avoid any unnecessary computation was set to 500. (2) The number of experiment designs (N) and degree of polynomial (p) are key factors in estimating PCE coefficients, and values of N = 50 and p = 4 are preferred. (3) The results computed using a PCE model consisting of polynomial bases are as good as those given by the NAM, while the total times required for making an ensemble in the PCE model are approximately seventeen times faster. (4) Two parameters (“CQOF” and “CK12”) turned out to be most dominant based on a visual inspection of the posterior distribution and the mathematical computations of the Sobol’ and Morris sensitivity analysis. Identification of the posterior parameter distributions from the calibration process helps to find the behavioral sets even faster. The unified framework that presents the most efficient ways of predicting flow regime and quantifying the uncertainty without deteriorating accuracy will ultimately be helpful for providing warnings and mitigating flood risk in a timely manner.},
  comment  = {Used PCE in UQLab.},
  doi      = {10.1007/s00477-019-01703-0},
  file     = {:Tran2019 - Quantification of predictive uncertainty with a metamodel_ toward more efficient hydrologic simulations.pdf:PDF},
  keywords = {used UQLab, geoscience},
  url      = {https://doi.org/10.1007/s00477-019-01703-0},
}

@PhdThesis{Jones2019a,
  author   = {Mark Nicholas Jones},
  school   = {Technical University of Denmark (DTU)},
  title    = {Design and Optimisation of Oleochemical Processes},
  year     = {2019},
  address  = {Kgs. Lyngby, Denmark},
  type     = {phdthesis},
  abstract = {Process Systems Engineering (PSE) is a discipline which connects a wide range of chemical engineering topics in a systems view approach. The reason for this systematic view of this scientific field is the need of computational concepts, numerical methods and computer-aided tools which can be applied to different use cases in industry. The multi-scale framework developed in this work encompasses four levels of application: (I) A server based property prediction software prototype which quantifies the uncertainty of group contribution methods and quantitative structure property relationship models and provides the confidence bounds of the estimates. (II) A modelling development level which allows the user to develop models
in a flexible way by using common programming languages for fast prototyping (Python) and high performance computing (Fortran). (III) An interface to process simulators to analyse and optimise entire flowsheets with advanced routines. (IV) A superstructure optimisation layer where surrogate models generated from unit operations or process models can be embedded in a superstructure formulation and solved for the optimal process structure and operating point. The contributions presented in this work show how the developed framework allows to tackle research in machine learning, optimisation and Monte Carlo driven methods such as sensitivity analysis. The developed tools were applied to the oleochemical domain with selected processes. In conclusion, this work demonstrates that a modular approach to process systems engineering, combined with tools integration from various vendors, allows to gain new knowledge in a time-efficient and augmentable manner.},
  comment  = {Used PCE and Sobol' sensitivity analysis in UQLab to create a framework.},
  file     = {:Jones2019a - Design and Optimisation of Oleochemical Processes.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://orbit.dtu.dk/en/publications/design-and-optimisation-of-oleochemical-processes(f63e262f-6567-467d-bcdb-c97fbe7bde66).html},
}

@Article{Slobbe2019,
  author   = {Arthur Slobbe and Árpád Rózsás and Diego L. Allaix and Agnieszka Bigaj‐van Vliet},
  journal  = {Structural Concrete},
  title    = {On the value of a reliability‐based nonlinear finite element analysis approach in the assessment of concrete structures},
  year     = {2019},
  pages    = {1--16},
  abstract = {The objective of this paper is to explore the value of reliability‐based nonlinear finite element analysis (NLFEA) over the currently available, standardized assessment methods. To our knowledge, no studies are available on this subject, and this paper provides a first insight into the value and reliability level of these assessment methods. The exploration is illustrated through three reinforced concrete structural members: a continuous girder, a continuous deep beam, and a high‐strength deep beam. The analysis is performed gradually: step by step advancing the approximation level of the mechanical and the probabilistic models. The added value of the reliability‐based NLFEA over the semi‐probabilistic Eurocode (EC) method is found to be on average 0.60. In other words: even if according to the semi‐probabilistic EC method the design action (Ed) is 60\% higher than the design resistance (Rd), the compliance with the target reliability criterion can be demonstrated by a reliability‐based NLFEA. Furthermore, it is observed that the gain and its cause (i.e., more advanced mechanical or probabilistic models) are different for the three cases. Though this outcome is restricted to the analyzed cases and should be interpreted as an upper limit added value, it indicates that a more detailed physical representation of the problem and an explicit treatment of uncertainties may uncover substantial reserves compared with the currently available, standardized assessment methods. Hence, the reliability‐based NLFEA method offers a promising alternative in the assessment of existing structures, enabling to avoid expensive measures that might be needed based on simplified methods.},
  comment  = {Used AK-MCS in UQLab.},
  doi      = {10.1002/suco.201800344},
  file     = {:Slobbe2019 - On the value of a reliability‐based nonlinear finite element analysis approach in the assessment of concrete structures.pdf:PDF},
  keywords = {used UQLab, civil engineering},
  url      = {https://doi.org/10.1002/suco.201800344},
}

@Article{Shi2019,
  author   = {Yuhan Shi and Wei Gong and Qingyun Duan and Jackson Charles and Cunde Xiao and Heng Wang},
  journal  = {Progress in Earth and Planetary Science},
  title    = {How parameter specification of an Earth system model of intermediate complexity influences its climate simulations},
  year     = {2019},
  number   = {46},
  pages    = {1--18},
  volume   = {6},
  abstract = {Earth system models (ESMs) consist of parameterization schemes based on one’s perception of how the Earth system functions. A typical ESM contains a large number of parameters (i.e., the constants and exponents in the parameterization schemes) whose specification can have a significant impact on an ESM’s simulation capabilities. Sensitivity analyses (SA) is an important tool for assessing how parameter specification influences model simulations. In this study, we used an Earth system model of intermediate complexity (EMIC)—LOVECLIM as an example to illustrate how SA methods can be used to identify the most sensitive parameters that control the simulations of several key global water and energy cycle variables, including global annual mean absolute surface air temperature (TG), precipitation and evaporation over the land and over the oceans (PL, PO, EL, EO), and land runoff (RL). We also demonstrate how judiciously specifying model parameters can improve the simulations of those variables. Three SA methods MARS, RF, and sparse PCE-based Sobol’ method were used to evaluate a pool of 25 adjustable parameters chosen from land, atmosphere, and ocean components of LOVECLIM and their results were intercompared to ensure robustness of the results. It is found that with different parameter specification, TG can vary from 10 to 20 oC, and the values of PL, PO, EL, and EO can change by more than 100\%. An interesting observation is that the value of RL vary from 13,000 to 35,000 km3, far below the observed climatological value of 40,000 km3, indicating a model structural deficiency in representing land runoff by LOVECLIM which must be corrected to obtain more reasonable global water budgets. We also note that parameter sensitivities are significantly different at different latitudes. Finally, we showed that global water and energy cycle simulations can be significantly improved by even a crude automatic parameter tuning, indicating that parameter optimization can be a viable way to improve ESM climate simulations. The results from this study should help us to understand the parameter uncertainty of a full-scale ESM.},
  comment  = {Used Sparse PCE in UQLab.},
  doi      = {10.1186/s40645-019-0294-x},
  file     = {:Shi2019 - How parameter specification of an Earth system model of intermediate complexity influences its climate simulations.pdf:PDF},
  keywords = {used UQLab, geoscience},
  url      = {https://doi.org/10.1186/s40645-019-0294-x},
}

@InProceedings{Houret2019,
  author       = {Thomas Houret and Philippe Besnier and Stéphane Vauchamp and Philippe Pouliguen},
  booktitle    = {Joint International Symposium on Electromagnetic Compatibility and Asia-Pacific International Symposium on Electromagnetic Compatibility (EMC \& APEMC 2019)},
  title        = {Comparison of Surrogate Models for Extreme Quantile Estimation in the Context of {EMC} Risk Analysis (I)},
  year         = {2019},
  address      = {Sapporo, Japan},
  organization = {IEICE Communication Society},
  abstract     = {Various  EMC  problems   may  be  studied  from numerical  simulations  involving  3D  Maxwell  equation  solvers.  However,  the  EMC  risk  analysis,  either  from  a  susceptibility  or emissivity   point   of   view   requires   various   configurations   of coupling   paths   described   by   important   sets   of   unknown   or uncertain   parameters.   The   use   of   surrogate   models   is   very relevant  to  speed  up  the  risk  analysis  process.  More  specifically, values at  risk corresponding to extreme values of relevantfields, currents  or  voltages  are  often  the  most  important  information with  regard  to  a  possible  EMC  risk.  Specific  methods  such  as controlled stratification provide a way to sample the input space of  random  variables  in  an efficient  way  to  estimate  extreme quantiles of a distribution. However, it requires a simple (i.e. fast calculation time) companionmodel. This companion model has to be correlated to the  reference  model in  a specific  sense. Building a surrogate model would be a possible way. This paper discusses various  surrogate  models  and  provides  some conclusionsabout their abilityto  provide  either  a  direct  estimation  of  extreme quantiles  of  the  response  of  interest,  or a  companion  model  forthe controlled stratification method.},
  comment      = {Used SVM, PCE, Kriging, and PC-Kriging in UQLab.},
  eventdate    = {2019-06-03/2019-06-07},
  file         = {:Houret2019 - Comparison of Surrogate Models for Extreme Quantile Estimation in the Context of EMC Risk Analysis (I).pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  url          = {https://www.researchgate.net/publication/334131327_Comparison_of_Surrogate_Models_for_Extreme_Quantile_Estimation_in_the_Context_of_EMC_Risk_Analysis},
}

@PhdThesis{Aleksankina2019,
  author   = {Ksenia Aleksankina},
  school   = {The University of Edinbrugh},
  title    = {Application of global methods for sensitivity analysis and uncertainty assessment of atmospheric chemistry transport models},
  year     = {2019},
  address  = {Edinburgh, UK},
  type     = {phdthesis},
  abstract = {Atmospheric concentrations of air pollutants remain high, and air quality is still an issue that requires attention in many countries including the UK. Atmospheric chemistry transport models (ACTMs) are widely used to provide scientific support for policy development in relation to the mitigation of the detrimental effects of air pollution on human health and ecosystems. Hence it is important to assess the level of uncertainty associated with model predictions. In this work, the application of global sensitivity analysis and uncertainty assessment methods is investigated for two ACTMs of different complexity: the Fine Resolution Atmospheric Multi-pollutant Exchange (FRAME) model and the UK regional application of the European Monitoring and Evaluation Programme (EMEP4UK) model. For both models, the uncertainties in the outputs resulting from uncertainties in the model input emissions are quantified and apportioned. Additionally, the overall model response to variations in the input emissions within a ± 40 \% range from the baseline is investigated as this range of variation is typically used for future scenario simulations. FRAME is a Lagrangian ACTM with 5 km x 5 km horizontal resolution over the UK domain that is used to estimate annual average concentrations and deposition of sulphur and nitrogen species. In the model, air columns with 33 vertical layers of varying thickness (from 1 m at the surface to 100 m at the top of the mixing layer) move from the boundary of the domain along straight-line trajectories with different starting angles at a 1o resolution. The model utilises annually averaged meteorology to define the column trajectories and rainfall. The chemical scheme includes gaseous-and aqueous-phase reactions. FRAME supplies Source-Receptor Relationship (SRR) matrices for the UK Integrated Assessment Model, which directly underpins UK air pollution control policies. EMEP4UK is a 3-D Eulerian model with a horizontal resolution of 5 km × 5 km over the British Isles and 20 vertical levels, extending from the ground to 100 hPa, which is also extensively used to inform UK air quality assessment. The chemical scheme implemented in the model is EmChem09 with the MARS equilibrium module for gas-aerosol partitioning of secondary inorganic aerosol. In addition to the pollutants modelled by FRAME, EMEP4UK is capable of modelling ozone (O3) and speciated particulate matter (PM2.5 and PM10) concentrations, all at hourly temporal resolutions. In this study, the uncertainty ranges for the input emissions from UK anthropogenic land-based sources were assigned according to the data provided by the UK National Atmospheric Emissions Inventory. For the FRAME model, the uncertainties in the outputs were propagated from the uncertainties in the emissions of SO2, NOx, and NH3. For the EMEP4UK model, an increased number of input variables was used; the emissions of NOx, SO2, NH3, VOC, and primary PM2.5 were split into 13 model inputs based on the contributions from different emission source sectors. The optimised Latin hypercube sampling design was used for both models to construct model runs that covered a chosen range of input emission perturbations. The FRAME model was investigated using several regression techniques. The response of the model to emission perturbations within a \pm 40\% range from the baseline value was found to be substantially linear. Surface concentrations of SO2, NOx, and NH3 together with the deposition of S and N were found to be predominantly sensitive to the emissions of the respective pollutant, while sensitivities of secondary species such as HNO3 and particulate SO42−, NO3−, and NH4+ to pollutant emissions were more complex and geographically variable. Additionally, the uncertainty in the surface concentrations of NH3 and NOx and the depositions of NHx and NOy was shown to be due to uncertainty in a single input variable, NH3, and NOx respectively. In contrast, the uncertainty in concentration and deposition of sulfur containing species were affected by the uncertainties in both NH3 and SO2 emissions. Similarly, the relative uncertainties in the modelled surface concentrations of each of the secondary pollutant variables were affected by the uncertainty range of at least two input variables. An emulator-based approach was used to propagate and apportion uncertainty in EMEP4UK outputs and investigate the model response to input perturbations. A separate Gaussian process emulator was used to estimate model predictions at unsampled points in the space of the uncertain model inputs for every modelled grid cell. For the surface concentrations of O3, NO2, and PM2.5 (pollutants associated with the adverse effects on human health) the highest level of uncertainty was found to occur in the grid cells comprising urban areas, up to \pm7\%, \pm9\%, and \pm9\% respectively. However, overall uncertainty calculated for the land-based grid cells for the variables above was found to be low, which indicates that the outputs may be more sensitive to variation in other model input parameters, such as chemical or physical constants. Alternatively, UK land-based concentrations of O3, NO2, and PM2.5 may be dominated by the precursor emissions and long-range transport of pollutants from outside the UK. Investigating seasonal changes in uncertainty and sensitivity for the monthly-averaged model outputs allowed determination of the importance of the inputs that drive uncertainty changes throughout the year. For example, uncertainty in O3 was driven more by uncertainty in VOC emissions during the summer, and for PM2.5 the importance of NH3 in driving overall uncertainty increased during spring and summer. The aim of the global methods for sensitivity analysis and uncertainty assessment presented here is to quantify the confidence in model predictions associated with particular aspects of model operation. Furthermore, the model runs and emulators created for the analyses can be used to predict the ACTM response for any other combination of perturbed input emissions within the ranges set for the original Latin hypercube sampling design without the need to re-run the ACTM. This makes exploring different emission perturbation scenarios possible at a significantly reduced computational cost. The methods discussed in this study can be applied to any operational aspect of any ACTM.},
  comment  = {Use Kriging in UQLab.},
  file     = {:Aleksankina2019 - Application of global methods for sensitivity analysis and uncertainty assessment of atmospheric chemistry transport models.pdf:PDF},
  keywords = {used UQLab, geoscience},
  url      = {http://hdl.handle.net/1842/35681},
}

@Article{Gorniak2019a,
  author   = {Piotr Górniak and Wojciech Bandurski},
  journal  = {Progress In Electromagnetics Research B,},
  title    = {{PCE}-Based Approach to Worst-Case Scenario Analysis in WirelessTelecommunication System},
  year     = {2019},
  pages    = {153--170},
  volume   = {84},
  abstract = {In the paper, we present a novel PCE-based approach for the effective analysis of worst-case scenario in a wireless telecommunication system. Usually, in such analysis derivation of polynomialchaos expansion (PCE meta-model) of a considered EM field function for one precise set of probabilitydensities of random variables does not provide enough information.  Consequently, a number of PCEmeta-models of the EM field function should be derived, each for the different joint probability density ofa vector of random variables, e.g., associated with different mean (nominal) values of random variables.The general polynomial chaos (gPC) approach requires numerical calculations for each PCE meta-modelderivation. In order to significantly decrease the time required to derive all of the PCE meta-models, thenovel approach has been introduced. It utilizes the novel so-called primary approximation and the novelanalytical formulas. They significantly decrease the number of numerical calculations required to deriveall of the PCE meta-models compared with the gPC approach. In the paper, we analyze the stochasticEM fields distributions in a telecommunication system in a spatial domain. For this purpose, analysis ofuncertainties associated with a propagation channel as well as with transmitting and receiving antennaswas introduced. We take advantage of a ray theory in our analysis. This allows us to provide the novelmethod for rapid calculation of a PCE meta-model of a telecommunication system transfer function byusing the separate PCE meta-models associated with antennas and a propagation channel.},
  comment  = {Used PCE in UQLab.},
  doi      = {10.2528/PIERB19032004},
  file     = {:Gorniak2019a - PCE-Based Approach to Worst-Case Scenario Analysis in WirelessTelecommunication System.pdf:PDF},
  keywords = {used UQLab, electrical engineering},
  url      = {https://doi.org/10.2528/PIERB19032004},
}

@Article{Zhu2019,
  author   = {Bin Zhu and Huafu Pei and Qing Yang},
  journal  = {International Journal for Numerical and Analytical Methods in Geomechanics},
  title    = {An intelligent response surface method for analyzing slope reliability based on {Gaussian} process regression},
  year     = {2019},
  pages    = {1--18},
  abstract = {Problems in geotechnical engine ering inevitably involve many uncertainties in the analysis. Reliability methods are important for evaluating slope stability
and can take the uncertainties into consideration. In this paper, a novel intelligent response surface method is proposed in which a machine learning algo-
rithm, namely Gaussian process regression, is used to approximate the high‐dimensional and highly nonlinear response hypersurface. An iterative algorithm is also proposed for updating the response surface dynamically by adding the new training point nearest to the limit state surface to the initial training database at each step. The proposed Gaussian process response surface method is used to analyze three different case studies to assess its validity and efficiency. Direct Monte Carlo simulation is also carried out in each case to serve as the benchmark. Comparing with other methods confirms the accuracy and efficiency of the novel intelligent response surface method, which requires fewer performance function calls and avoids the need to normalize the correlative non‐normal variables.},
  comment  = {Used Kriging in UQLab.},
  doi      = {10.1002/nag.2988},
  file     = {:Zhu2019 - An intelligent response surface method for analyzing slope reliability based on Gaussian process regression.pdf:PDF},
  keywords = {used UQLab, geoscience},
  url      = {https://doi.org/10.1002/nag.2988},
}

@InProceedings{Yang2019,
  author       = {Huoming Yang and Hendrik Just and Sibylle Dieckerhoff},
  booktitle    = {20th Workshop on Control and Modeling for Power Electronics (COMPEL 2019)},
  title        = {Identification of Critical Parameters Affecting the Small-Signal Stability of Converter-based Microgrids},
  year         = {2019},
  address      = {Toronto, Canada},
  organization = {IEEE},
  abstract     = {The analysis of the small-signal stability of converter-based microgrids is well established, but the existing deterministic approaches cannot fully capture the impact of random parameters resulting from complex converter control structures and stochastic operation scenarios. Accurate assessment of the relevance of different parameters can facilitate efficient modeling, online monitoring and control design of the microgrids. In this paper, a global sensitivity analysis (GSA) framework is proposed to identify the most relevant parameters that affect the small-signal stability of converter-based microgrids. First, the systematic approach to derive the complete small-signal state-space model of the microgrid is introduced and the stability index is defined. Then, the system operating states, the control and the network parameters are chosen as input variables, and the priority ranking procedure based on GSA is explained. Next, the Levenberg-Marquardt based power flow calculation is adopted to determine the steady-state operation point. The polynomial chaos expansion and the low-rank approximation methods are introduced into the GSA to further improve computation efficiency. Finally, the proposed methodology is applied to an exemplary microgrid. The critical parameters influencing the system small-signal stability are identified, and the results are compared with those of the conventional local sensitivity analysis. The correctness and effectiveness of the proposed methodology are verified by real-time simulation results.},
  comment      = {Used PCE and LRA for Sobol' sensitivity analysis.},
  doi          = {10.1109/COMPEL.2019.8769621},
  eventdate    = {2019-06-17/2019-06-20},
  file         = {:Yang2019 - Identification of Critical Parameters Affecting the Small-Signal Stability of Converter-based Microgrids.pdf:PDF},
  keywords     = {used UQLab, electrical engineering},
  url          = {https://doi.org/10.1109/COMPEL.2019.8769621},
}

@Article{Wang2019,
  author   = {Han Wang and Zheng Yan and Xiaoyuan Xu and Kun He},
  journal  = {International Journal of Electrical Power and Energy Systems},
  title    = {Probabilistic power flow analysis of microgrid with renewable energy},
  year     = {2019},
  pages    = {1--10},
  volume   = {114},
  abstract = {With the development of renewable-based distributed generation (RDG), there are increasing uncertainties in the operation of microgrids (MGs), and stochastic evaluation methods are attracting more attention nowadays. In this paper, a probabilistic power flow (PPF) analysis method is proposed to evaluate the influence of uncertainties on the power flow of MGs. First, the MG PPF model is established considering different operation modes of MGs and uncertainties of RDG and load demands. Then, the Borgonovo method, which is a density-based global sensitivity analysis (GSA) method, is used to evaluate the importance of input variables in PPF calculation. To improve the computational efficiency of GSA, the sparse polynomial chaos expansion (SPCE) is used to establish the surrogate model of MG PPF, and the Borgonovo index is calculated based on the surrogate model. Finally, the procedure of applying GSA to MG power flow is established. The proposed method is tested using 33-node and 123-node MGs, and is compared with other methods to validate its effectiveness. Simulation results indicate that the proposed method identifies critical uncertainties that affect MG power flow. Based on the rankings of input variables, the influences of critical uncertainties are diminished with energy storage systems.},
  comment  = {Used Sparse PCE in UQLab.},
  doi      = {10.1016/j.ijepes.2019.105393},
  file     = {:Wang2019 - Probabilistic power flow analysis of microgrid with renewable energy.pdf:PDF},
  keywords = {used UQLab, energy engineering},
  url      = {https://doi.org/10.1016/j.ijepes.2019.105393},
}

@Article{Song2019,
  author   = {Jian Song and YunYang and Gan Chen and Xiaomin Sun and Jin Lin and Jianfeng Wu and Jichun Wu},
  journal  = {Journal of Hydrology},
  title    = {Surrogate assisted multi-objective robust optimization for groundwater monitoring network design},
  year     = {2019},
  pages    = {1--16},
  volume   = {577},
  abstract = {The robust optimization of groundwater quality monitoring network is subject to many conflicting objectives and high level of uncertainty in hydraulic conductivity. This study develops a two-stage stochastic optimization framework including the uncertainty quantification using a cheap-to-evaluate surrogate model and an improved epsilon multi-objective noisy memetic algorithm (\epsilon-MONMA) for monitoring network design. The surrogate model based on sparse polynomial chaos expansion (PCE) is constructed to replace expensive simulation model in the uncertainty quantification of concentrations at the pre-defined monitoring locations for reducing huge computational cost. Additionally, the scenario discovery strategy using sparse PCE model is applied to filter a typical scenario set and the centroid of contaminant plume is used as the diversity metric, which avoids enumerating all possible contamination plumes caused by the uncertain K-field in the optimization. The proposed algorithm is then employed to solve stochastic management model to achieve robust monitoring design, indicating the insensitivity of monitoring design to plume uncertainty no matter which of the many possible scenarios becomes the true distribution of contamination under the true K-field. A synthetic aquifer considering uncertainty in hydraulic conductivity is designed to optimize monitoring network design. The Pareto-optimal solutions to the synthetic example are achieved under three of plume scenario sets defined at deterministic scenario (Scenario A0), Monte Carlo based scenario discovery (Scenario A1) and surrogate assisted scenario discovery (Scenario A2), respectively. Comprehensive analysis demonstrates that the monitoring design based on Scenario A2 outperforms either of the two designs based on Scenarios A0 and A1 in terms of the improvement of robustness of designs evaluated against the typical scenario set. Meanwhile, the performance of monitoring network deteriorates as the uncertainty of plume (noisy strength) increases, indicating the significance of reducing parameter uncertainty in groundwater monitoring design. The research findings show that the developed stochastic optimization framework is a computationally efficient and promising tool for multi-objective design of groundwater monitoring network under uncertainty.},
  comment  = {Used Sparse PCE in UQLab.},
  doi      = {10.1016/j.jhydrol.2019.123994},
  file     = {:- Surrogate assisted multi-objective robust optimization for groundwater monitoring network design.pdf:PDF},
  keywords = {used UQLab, monitoring and remote sensing},
  url      = {https://doi.org/10.1016/j.jhydrol.2019.123994},
}

@Article{Radaideh2019,
  author   = {Majdi I. Radaideh and Tomasz Kozlowski},
  journal  = {Nuclear Engineering and Technology},
  title    = {Analyzing nuclear reactor simulation data and uncertainty with the {Group Method of Data Handling}},
  year     = {2019},
  pages    = {1--9},
  abstract = {Group Method of Data Handling (GMDH) is considered one of the earliest deep learning methods. Deep learning gained additional interest in today's applications due to its capability to handle complex and high dimensional problems. In this study, multi-layer GMDH networks are used to perform uncertainty quantification (UQ) and sensitivity analysis (SA) of nuclear reactor simulations. GMDH is utilized as a surrogate/metamodel to replace high fidelity computer models with cheap-to-evaluate surrogate models, which facilitate UQ and SA tasks (e.g. variance decomposition, uncertainty propagation, etc.). GMDH performance is validated through two UQ applications in reactor simulations: (1) low dimensional input space (two-phase flow in a reactor channel), and (2) high dimensional space (8-group homogenized cross-sections). In both applications, GMDH networks show very good performance with small mean absolute and squared errors as well as high accuracy in capturing the target variance. GMDH is utilized afterward to perform UQ tasks such as variance decomposition through Sobol indices, and GMDH-based uncertainty propagation with large number of samples. GMDH performance is also compared to other surrogates including Gaussian processes and polynomial chaos expansions. The comparison shows that GMDH has competitive performance with the other methods for the low dimensional problem, and reliable performance for the high dimensional problem.},
  comment  = {Used PCE and Kriging in UQLab.},
  doi      = {10.1016/j.net.2019.07.023},
  file     = {:Radaideh2019 - Analyzing nuclear reactor simulation data and uncertainty with the Group Method of Data Handling.pdf:PDF},
  keywords = {used UQLab, nuclear engineering},
  url      = {https://doi.org/10.1016/j.net.2019.07.023},
}

@Article{Paulson2019,
  author   = {Joel A. Paulson and Marc Martin-Casas and Ali Mesbah},
  journal  = {PLOS Computational Biology},
  title    = {Fast uncertainty quantification for dynamic flux balance analysis using non-smooth polynomial chaos expansions},
  year     = {2019},
  number   = {8},
  pages    = {1--35},
  volume   = {15},
  abstract = {We present a novel surrogate modeling method that can be used to accelerate the solution of uncertainty quantification (UQ) problems arising in nonlinear and non-smooth models of biological systems. In particular, we focus on dynamic flux balance analysis (DFBA) models that couple intracellular fluxes, found from the solution of a constrained metabolic network model of the cellular metabolism, to the time-varying nature of the extracellular substrate and product concentrations. DFBA models are generally computationally expensive and present unique challenges to UQ, as they entail dynamic simulations with discrete events that correspond to switches in the active set of the solution of the constrained intracellular model. The proposed non-smooth polynomial chaos expansion (nsPCE) method is an extension of traditional PCE that can effectively capture singularities in the DFBA model response due to the occurrence of these discrete events. The key idea in nsPCE is to use a model of the singularity time to partition the parameter space into two elements on which the model response behaves smoothly. Separate PCE models are then fit in both elements using a basis-adaptive sparse regression approach that is known to scale well with respect to the number of uncertain parameters. We demonstrate the effectiveness of nsPCE on a DFBA model of an E. coli monoculture that consists of 1075 reactions and 761 metabolites. We first illustrate how traditional PCE is unable to handle problems of this level of complexity. We demonstrate that over 800-fold savings in computational cost of uncertainty propagation and Bayesian estimation of parameters in the substrate uptake kinetics can be achieved by using the nsPCE surrogates in place of the full DFBA model simulations. We then investigate the scalability of the nsPCE method by utilizing it for global sensitivity analysis and maximum a posteriori estimation in a synthetic metabolic network problem with a larger number of parameters related to both intracellular and extracellular quantities.},
  comment  = {Used sparse PCE of UQLab.},
  doi      = {10.1371/journal.pcbi.1007308},
  file     = {:Paulson2019 - Fast uncertainty quantification for dynamic flux balance analysis using non-smooth polynomial chaos expansions.pdf:PDF},
  keywords = {used UQLab, life sciences, biology},
  url      = {https://doi.org/10.1371/journal.pcbi.1007308},
}

@Article{Li2019,
  author   = {Yangtian Li and Haibin Li and Guangmei Wei},
  journal  = {Engineering Computations},
  title    = {Dimension-adaptive algorithm-based {PCE} for models with many model parameters},
  year     = {2019},
  volume   = {In-press},
  abstract = {Purpose–To present the models with many model parameters by polynomial chaos expansion (PCE), andimprove the accuracy, this paper aims to present dimension-adaptive algorithm-based PCE technique andverify the feasibility of the proposed method through taking solid rocket motor ignition under lowtemperature as an example.Design/methodology/approach–The main approaches of this work are as follows: presenting a two-step dimension-adaptive algorithm; through computing the PCE coefficients using dimension-adaptivealgorithm, improving the accuracy of PCE surrogate model obtained; and applying the proposed method touncertainty quantification (UQ) of solid rocket motor ignition under low temperature to verify the feasibilityof the proposed method.Findings–The result indicates that by means of comparing with some conventional non-invasive method,the proposed method is able to raise the computational accuracy significantly on condition of meeting theefficiency requirement.Originality/value–This paper proposes an approach in which the optimal non-uniform grid that canavoid the issue of overfitting or underfitting is obtained.},
  comment  = {Used UQLab for sparse PCE as a building block for a more advanced method.},
  doi      = {10.1108/EC-12-2018-0595},
  file     = {:Li2019 - Dimension-adaptive algorithm-based PCE for models with many model parameters.pdf:PDF},
  keywords = {used UQLab, computational science and engineering},
  url      = {https://doi.org/10.1108/EC-12-2018-0595},
}

@InProceedings{Gorniak2019b,
  author    = {Piotr Górniak},
  title     = {The Intrusive {PCE}-Based Method for Uncertainty Calculation in Ray-Tracing Analysis of {5G} {EM} Wave Propagation},
  booktitle = {IEEE-APS Conference on Antennas and Propagation in Wireless Communications},
  year      = {2019},
  address   = {Granada, Spain},
  eventdate = {2019-09-09/2019-09-13},
  file      = {:2019 - The Intrusive PCE-Based Method for Uncertainty Calculation in Ray-Tracing Analysis of 5G EM Wave Propagation.pdf:PDF},
  keywords  = {electrical engineering, used UQLab},
  url       = {http://www.multimedia.edu.pl/?page=publication&section=The-Intrusive-PCE-Based-Method-for-Uncertainty-Calculation-in-Ray-Tracing-Analysis-of-5G-EM-Wave-Propagation},
}

@MastersThesis{Trevisi2019,
  author   = {Trevisi, Filippo},
  school   = {University of Padua},
  title    = {Configuration optimisation of kite-based wind turbines},
  year     = {2019},
  address  = {Padua, Italy},
  type     = {mathesis},
  comment  = {Used UQLab for metamodeling (PCE) and sensitivity analysis},
  file     = {:Trevisi2019 - Configuration Optimisation of Kite-based Wind Turbines.pdf:PDF},
  keywords = {Used UQLab, mechanical engineering},
  url      = {http://tesi.cab.unipd.it/62975/},
}

@Article{Razek2019,
  author   = {Razek, Adel and Pichon, Lionel and Kameni, Abelin and Makong, Ludovic and Rasm, Sahand},
  title    = {Evaluation of human exposure owing to wireless power transfer systems in electric vehicles},
  journal  = {Athens Journal of Technology \& Engineering},
  year     = {2019},
  doi      = {10.30958/ajte.6-4-3},
  file     = {:Razek2019 - Evaluation of human exposure owing to wireless power transfer systems in electric vehicles.pdf:PDF},
  keywords = {Used UQLab, electrical engineering},
  url      = {https://doi.org/10.30958/ajte.6-4-3},
}

@Article{Groenquist2019,
  author    = {Gr{\"o}nquist, Philippe and Wood, Dylan and Hassani, Mohammad M and Wittel, Falk K and Menges, Achim and R{\"u}ggeberg, Markus},
  title     = {Analysis of hygroscopic self-shaping wood at large scale for curved mass timber structures},
  journal   = {Science Advances},
  year      = {2019},
  volume    = {5},
  number    = {9},
  doi       = {10.1126/sciadv.aax1311},
  file      = {Supplemental file:aax1311_SM.pdf:PDF;Full text:Groenquist2019 - Analysis of hygroscopic self-shaping wood at large scale for curved mass timber structures.pdf:PDF},
  keywords  = {Used UQLab, civil engineering},
  publisher = {American Association for the Advancement of Science},
  url       = {https://doi.org/10.1126/sciadv.aax1311},
}

@InProceedings{Houret2019a,
  author    = {Houret, T and Besnier, P and Vauchamp, S and Pouliguen, P},
  booktitle = {2019 International Symposium on Electromagnetic Compatibility (EMC EUROPE 2019)},
  title     = {Combining {Kriging} and controlled stratification to identify extreme levels of electromagnetic interference},
  year      = {2019},
  address   = {Barcelona, Spain},
  abstract  = {EMC risk analysis requires various configurations of coupling paths described by important sets of unknown or uncertain parameters. More specifically, values at risk corresponding to extreme values of relevant fields, currents or voltages are often the most important information with regard to a possible EMC risk. Therefore, we aim at estimating extreme quantiles of the relevant field, current or voltage. Controlled stratification accelerates the standard Empirical estimation convergence to sample output extreme values, thus reducing the required number of calls to cost-expensive full-wave simulations. However, controlled stratification requires a simple (i.e. fast calculation time) model with sufficient correlation to the initial model. The main idea in this communication is to use a surrogate model as a simple model. Kriging was previously identified as a surrogate model with relevant properties. In this paper, we investigate the performance of combined kriging and control stratification. We show that this combination outperforms the stand-alone kriging surrogate model for estimating extreme quantiles. On the contrary, the latter performs better to identify less extreme quantiles.},
  eventdate = {2019-09-02/2019-09-06},
  file      = {:Houret2019a - Combining Kriging and controlled stratification to identify extreme levels of electromagnetic interference.pdf:PDF},
  groups    = {EMC risk analysis requires various configurations of coupling paths described by important sets of unknown or uncertain parameters. More specifically, values at risk corresponding to extreme values of relevant fields, currents or voltages are often the most important information with regard to a possible EMC risk. Therefore, we aim at estimating extreme quantiles of the relevant field, current or voltage. Controlled stratification accelerates the standard Empirical estimation convergence to sample output extreme values, thus reducing the required number of calls to cost-expensive full-wave simulations. However, controlled stratification requires a simple (i.e. fast calculation time) model with sufficient correlation to the initial model. The main idea in this communication is to use a surrogate model as a simple model. Kriging was previously identified as a surrogate model with relevant properties. In this paper, we investigate the performance of combined kriging and control stratification. We show that this combination outperforms the stand-alone kriging surrogate model for estimating extreme quantiles. On the contrary, the latter performs better to identify less extreme quantiles.},
  keywords  = {Used UQLab, electrical engineering},
  url       = {https://www.researchgate.net/publication/335789611_Combining_Kriging_and_Controlled_Stratification_to_Identify_Extreme_Levels_of_Electromagnetic_Interference},
}

@InProceedings{Nesterova2019,
  author       = {Mariia Nesterova and Marcel Nowak and Franziska Schmidt and Oliver Fischer},
  booktitle    = {11th International Conference on Mathematical Methods in Reliability (MMR 2019)},
  title        = {Reliability of a bridge with an orthotropic deck exposed to extreme traffic events},
  year         = {2019},
  address      = {Hong Kong, China},
  organization = {City University of Hong Kong},
  abstract     = {Predicting reliability levels for critical details of bridges based on limited statistical trafficdata is a relevant topic nowadays. That is why the comparison between results from variousstatistical approaches based on the recorded data for applied traffic actions is the main pointof interest of this work.  The object of the current study is the famous Millau viaduct, acable-stayed bridge with the steel orthotropic deck located in Southern France.  Values ofload effects that are used in analysis are derived from a finite element model of a part ofthe deck.  They are based on data from traffic monitoring that is provided from the bridgeWeigh-In-Motion system covering several months of axle loads, distances and speeds ofheavy trucks.  The methodology is based on a definition of limit state functions based onseveral statistical distributions in order to assess and compare reliability indexes for theultimate  limit  state.   It  includes  a  comparison  between  different  approaches  of  extremevalues theory, the methodology proposed in background works for European standards andthe design load model. Moreover, this work covers the influence of applied loads of a highamplitude, as global effects, onto stresses from axle loads, as local effects.},
  comment      = {Used UQLab for SORM reliability analysis.},
  eventdate    = {2016-06-03/2016-06-07},
  file         = {:Nesterova2019 - Reliability of a bridge with an orthotropic deck exposed to extreme traffic events.pdf:PDF},
  keywords     = {Used UQLab, civil engineering},
  url          = {http://infrastar.eu/fileadmin/contributeurs/Infrastar/Outreach_Dissemination/Publications/2019_06_MMR_Nesterova.pdf},
}

@InProceedings{Ahmed2019,
  author    = {Hashmi S. S. Ahmed and Siddhartha Ghosh},
  booktitle = {Proceedings of the International Colloquia on Stability and Ductility of Steel Structures (SDSS 2019)},
  title     = {Strength characterisation of a {CFS} section with initial geometric imperfections},
  year      = {2019},
  address   = {Prague, Czech Republic},
  pages     = {80},
  comment   = {used UQLab to create PCE metamodel.},
  eventdate = {2019-09-11/2019-09-13},
  keywords  = {civil engineering, used UQLab},
  url       = {https://books.google.ch/books?hl=en&lr=&id=nSysDwAAQBAJ&oi=fnd&pg=PA80&ots=a8ZaFbplk-&sig=1311sI2ka_xJ01LkqW1J1L3WVF8&redir_esc=y#v=onepage&q&f=false},
}

@Article{Wang2020,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Real-time high-fidelity reliability updating with equality information using adaptive {Kriging}},
  year     = {2020},
  pages    = {106735},
  volume   = {195},
  abstract = {Current state-of-the-art methods for reliability updating with equality information transform this challenging problem into an inequality one by introducing an auxiliary random variable. However, the joint event of information and failure in the derived conditional probabilities is typically very rare, and therefore, very challenging to estimate. Moreover, updating the reliability as new information arrives requires reevaluation of the probability of the joint event, which involves large numbers of calls to performance functions. We address these limitations by proposing a new approach to reliability updating called RUAK. One of the important contributions is the decomposition of the rare joint event of the failure and observed information into two events both with relatively high probabilities. Moreover, an adaptive Kriging-based reliability analysis method is proposed for the estimation of the prior failure probability and the conditional probability of information. This way, reliability updating for new information is conducted using the efficient Kriging meta-model, which significantly enhances the computational efficiency. Results for four examples indicate that the computational demand using RUAK is decreased by two orders of magnitude compared to the state-of-the-art methods, while achieving higher accuracy. This approach facilitates real-time reliability updating for various applications such as health monitoring and warning systems.},
  comment  = {Used UQLab for reliability analysis on an improved algorithm.},
  doi      = {10.1016/j.ress.2019.106735},
  file     = {:wang2020real - Real-time high-fidelity reliability updating with equality information using adaptive Kriging.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.ress.2019.106735},
}

@Article{Lu2020,
  author   = {Zhuoxin Lu and Xiaoyuan Xu and Zheng Yan and Han Wang},
  journal  = {Journal of Modern Power Systems and Clean Energy},
  title    = {Density-based global sensitivity analysis of islanded microgrid loadability considering distributed energy resource integration},
  year     = {2020},
  number   = {1},
  pages    = {94--101},
  volume   = {8},
  abstract = {With the proliferation of renewable energy and electric vehicles (EVs), there have been increasing uncertainties in power systems. Identifying the influencing random variables will reduce the effort in uncertainty modeling and improve the controllability of power systems. In this paper, a density-based global sensitivity analysis (GSA) method is proposed to evaluate the influence of uncertainties on islanded microgrids (IMGs). Firstly, the maximum IMG loadability evaluation model is established to assess the distance from the current operation point to the critical operation point. Secondly, the Borgonovo method, which is a density-based GSA method, is used to evaluate the influence of input variables on IMG loadability. Thirdly, to improve GSA efficiency, a modified Kriging model is used to obtain a surrogate model of IMG loadability, and Borgonovo indices are calculated based on the surrogate model. Finally, the proposed method is tested on a 38-bus IMG system. Simulation results are compared with those considering other methods to validate the effectiveness of the proposed method. Energy storage systems are considered to diminish the influence of critical uncertainties on IMG operation.},
  comment  = {Use PC-Kriging module of UQLab to construct a metamodel.},
  doi      = {10.35833/MPCE.2018.000580},
  file     = {:Lu2020 - Density-Based Global Sensitivity Analysis of Islanded Microgrid Loadability Considering Distributed Energy Resource Integration.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  url      = {https://doi.org/10.35833/MPCE.2018.000580},
}

@Article{Schneider2020,
  author   = {Aurel Schneider and Nicola Stoira and Alexandre Refregier and Andreas J. Weiss and Mischa Knabenhans and Joachim Stadel and Romain Teyssier},
  journal  = {Journal of Cosmology and Astroparticle Physics},
  title    = {Baryonic effects for weak lensing: {I.} Power spectrum and covariance matrix},
  year     = {2020},
  number   = {4},
  volume   = {2020},
  abstract = {Baryonic feedback effects lead to a suppression of the weak lensing angular power spectrum on small scales. The poorly constrained shape and amplitude of this suppression is an important source of uncertainties for upcoming cosmological weak lensing surveys such as Euclid or LSST. In this first paper in a series of two, we use simulations to build a Euclid-like tomographic mock data-set for the cosmic shear power spectrum and the corresponding covariance matrix, which are both corrected for baryonic effects following the baryonification method of [1]. In addition, we develop an emulator to obtain fast predictions of the baryonic power suppression, allowing us to perform a likelihood inference analysis for a standard ΛCDM cosmology with both cosmological and astrophysical parameters. Our main findings are the following: (i) ignoring baryonic effects leads to a greater than 5σ bias on the cosmological parameters Ωm and σ8; (ii) restricting the analysis to the largest scales, that are mostly unaffected by baryons, makes the bias disappear, but results in a blow-up of the Ωm-σ8 contour area by more than a factor of 10; (iii) ignoring baryonic effects on the covariance matrix does not significantly affect cosmological parameter estimates; (iv) while the baryonic suppression is mildly cosmology dependent, this effect does not noticeably modify the posterior contours. Overall, we conclude that including baryonic uncertainties in terms of nuisance parameters results in unbiased and surprisingly tight constraints on cosmology.},
  comment  = {Used PCE module of UQLab to construct a metamodel.},
  doi      = {10.1088/1475-7516/2020/04/019},
  file     = {:Schneider2019 - Baryonic effects for weak lensing_ I. Power spectrum and covariance matrix.pdf:PDF},
  keywords = {physics, used UQLab},
  url      = {https://doi.org/10.1088/1475-7516/2020/04/019},
}

@InProceedings{Phan2019,
  author       = {Hoang Nam Phan and Fabrizio Paolacci and Daniele Corritore and Nicola Tondini and Oreste S Bursi},
  booktitle    = {ASME 2019 Pressure Vessels \& Piping Conference},
  title        = {A {Kriging}-Based Surrogate Model for Seismic Fragility Analysis of Unanchored Storage Tanks},
  year         = {2019},
  address      = {San Antonio, Texas},
  organization = {American Society of Mechanical Engineers Digital Collection},
  abstract     = {The  seismic  vulnerability  of  aboveground  steel  storage tanks  has  been  dramatically  proved  during  the  latest  seismic events,  which  demonstrates  the  need  for  reliable  numerical models   for   vulnerability   and   risk   assessments   of   storage facilities.  While  for  anchored  aboveground  tanks,  simplified models are nowadays available and mostly used for the seismic vulnerability  assessment,  in  the  case  of  unanchored  tanks,  the scientific  community  is  still  working  on  numerical  models capable of reliably predicting  the nonlinearity due to uplift and sliding mechanisms. In this paper, a surrogate model based on a Kriging approach is proposed for a case study of an unanchored tank,  whose  calibration  is  performed  on a  three-dimensional finite   element   (3D   FE)   model   using   a   reliable   design   of experiments  (DOE)  method.  The  verification  of  the  3D  FE model  is  also  done  through  a  shaking  table  campaign.  The outcomes show the effectiveness of the proposed model to build fragility curves at a low computational cost of the critical damage state of the tank, i.e., the plastic rotation of the shell-to-bottom joint.},
  comment      = {Used Kriging module of UQLab to construct a metamodel.},
  doi          = {10.1115/PVP2019-93259},
  eventdate    = {2019-07-14/2019-07-19},
  file         = {:Phan2019 - A Kriging-Based Surrogate Model for Seismic Fragility Analysis of Unanchored Storage Tanks.pdf:PDF},
  keywords     = {civil engineering, used UQLab},
  url          = {https://doi.org/10.1115/PVP2019-93259},
}

@Article{Pan2020,
  author   = {Zhongmei Pan and Jian Liu and Huanhuan Fu and Tao Ding and Yan Xud and XiangqianTong},
  journal  = {International Journal of Electrical Power {\&} Energy Systems},
  title    = {Probabilistic voltage quality evaluation of islanded droop-regulated microgrid based on non-intrusive low rank approximation method},
  year     = {2020},
  volume   = {117},
  abstract = {Islanded microgrids (IMGs) are more vulnerable to voltage quality issues caused by the integration stochastic characteristics of intermittent distributed generations (DGs) owing to the small scale. Meanwhile, traditional load flow model is not suitable for decentralized droop-regulated IMGs considering their specific features, i.e. the absence of slack bus and variation of frequency. Therefore, a probabilistic voltage profile evaluation method for decentralized droop-regulated IMGs based on the canonical LRA method is proposed in this paper. Firstly, the deterministic load flow models of both balanced and unbalanced droop-regulated IMG are built and Newton-Raphson method is applied for solutions taking angular frequency as an unknown. Then, step size optimization is employed to improve the convergence performance of Newton-Raphson method, especially for load flow of the unbalanced IMGs. Finally, to evaluate the impact of uncertainties of the intermittent DGs and loads on voltage profiles in droop-regulated IMGs, probabilistic load flow calculations based on a non-intrusive low-rank approximation (LRA) method are performed. The response of voltage profile is represented statistically-equivalently by a sum of rank-one functions based on a small number of deterministic load flow calculations, The probabilistic characteristics of voltage quality metrics including mean value, standard deviation, limit violation probability, PDFs and CDFs of node voltage as well as three-phase voltage unbalance factor (VUF) can be evaluated analytically and accurately without evaluating a large number of samples. Test results on balanced and unbalanced IMGs demonstrate the effectiveness and accuracy of the proposed method.},
  comment  = {Used LRA module of UQLab to construct a metamodel.},
  doi      = {10.1016/j.ijepes.2019.105630},
  file     = {:Pan2020 - Probabilistic voltage quality evaluation of islanded droop-regulated microgrid based on non-intrusive low rank approximation method.pdf:PDF},
  keywords = {electrical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.ijepes.2019.105630},
}

@Article{Forero-Hernandez2020,
  author   = {Hector Forero-Hernandez and Mark Nicholas Jones and Bent Sarup and Anker Degn Jensen and Jens Abildskov and Gürkan Sin},
  journal  = {Computers {\&} Chemical Engineering},
  title    = {Comprehensive development, uncertainty and sensitivity analysis of a model for the hydrolysis of rapeseed oil},
  year     = {2020},
  volume   = {133},
  abstract = {A model describing the batch hydrolysis of rapeseed oil including kinetics and mass transfer at subcritical conditions is presented in this paper. The primary purpose of this model is to interpret experimental data collected from typical batch tests and to estimate model parameters. The developed model was further investigated using Monte Carlo simulations to statistically quantify the variability in the model outputs due to uncertainties in the parameter estimates. To understand which parameters in the model are responsible for the output uncertainty, a sensitivity analysis method was used (polynomial chaos expansions-based Sobol sensitivity indices). The results from the sensitivity analysis helped to identify what parameters in the model are influential, giving insight into the robustness and predictive capabilities of the model which form the basis for any model-based decision making for detailed process characterization, design, optimization and operation of the hydrolysis of rapeseed oil.},
  comment  = {Used UQLab to construct a PCE metamodel and to compute Sobol' sensitivity indices},
  doi      = {10.1016/j.compchemeng.2019.106631},
  file     = {:Forero-Hernandez2020 - Comprehensive development, uncertainty and sensitivity analysis of a model for the hydrolysis of rapeseed oil.pdf:PDF},
  keywords = {chemical engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.compchemeng.2019.106631},
}

@Article{Wang2020a,
  author   = {Zeyu Wang and Abdollah Shafieezadeh},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {On confidence intervals for failure probability estimates in {Kriging}-based reliability analysis},
  year     = {2020},
  volume   = {196},
  abstract = {Despite recent advancements in adaptive Kriging-based reliability analysis for complex limit states, estimation of the accuracy of extant techniques when the true failure probability is unknown remains an important challenge. The present study addresses this gap by developing analytical confidence intervals (CIs) for failure probability estimates. This is facilitated here by leveraging statistical properties of Poisson Binomial distribution for the expected number of failure points in the set of candidate design samples in adaptive Kriging as well as Lindeberg's condition for central limit theorem. Concerning computational demands involved in the computation of CIs, a simpler case where Kriging correlations are neglected is also derived. The performance of the proposed CIs is subsequently analyzed for five examples with different and varying complexities. Results indicate that the proposed CI with correlations considered offers the most accurate intervals. Additionally, whereas the CI estimated without Kriging correlation is not entirely satisfactory at early-stages of adaptive reliability analysis, it converges to accurate bounds at later stages.},
  comment  = {Used AKMCS module of UQLab to a proper derive confidence interval.},
  doi      = {10.1016/j.ress.2019.106758},
  file     = {:Wang2020a - On confidence intervals for failure probability estimates in Kriging-based reliability analysis.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.ress.2019.106758},
}

@Article{Cheng2020,
  author   = {Kai Cheng and Zhenzhou Lu},
  journal  = {Structural Safety},
  title    = {Structural reliability analysis based on ensemble learning of surrogate models},
  year     = {2020},
  volume   = {83},
  abstract = {Assessing the failure probability of complex structure is a difficult task in presence of various uncertainties. In this paper, a new adaptive approach is developed for reliability analysis by ensemble learning of multiple competitive surrogate models, including Kriging, polynomial chaos expansion and support vector regression. Ensemble of surrogates provides a more robust approximation of true performance function through a weighted average strategy, and it helps to identify regions with possible high prediction error. Starting from an initial experimental design, the ensemble model is iteratively updated by adding new sample points to regions with large prediction error as well as near the limit state through an active learning algorithm. The proposed method is validated with several benchmark examples, and the results show that the ensemble of multiple surrogate models is very efficient for estimating failure probability (>10−4) of complex system with less computational costs than the traditional single surrogate model.},
  comment  = {Used UQLab to construct sparse PCE and Kriging metamodels for an improved reliability analysis method.},
  doi      = {10.1016/j.strusafe.2019.101905},
  file     = {:Cheng2020 - Structural reliability analysis based on ensemble learning of surrogate models.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.strusafe.2019.101905},
}

@Article{Loukrezis2019a,
  author   = {Dimitrios Loukrezis and Herbert De Gersem},
  journal  = {arXiv},
  title    = {Adaptive Sparse Polynomial Chaos Expansions via {Leja} Interpolation},
  year     = {2019},
  abstract = {This work suggests an interpolation-based stochastic collocation method for the non-intrusive and adaptive construction of sparse polynomial chaos expansions (PCEs). Unlike pseudo-spectral projection and regression-based stochastic collocation methods, the proposed approach results in PCEs featuring one polynomial term per collocation point. Moreover, the resulting PCEs are interpolating, i.e., they are exact on the interpolation nodes/collocation points. Once available, an interpolating PCE can be used as an inexpensive surrogate model, or be post-processed for the purposes of uncertainty quantification and sensitivity analysis. The main idea is conceptually simple and relies on the use of Leja sequence points as interpolation nodes. Using Newton-like, hierarchical basis polynomials defined upon Leja sequences, a sparse-grid interpolation can be derived, the basis polynomials of which are unique in terms of their multivariate degrees. A dimension-adaptive scheme can be employed for the construction of an anisotropic interpolation. Due to the degree uniqueness, a one-to-one transform to orthogonal polynomials of the exact same degrees is possible and shall result in an interpolating PCE. However, since each Leja node defines a unique Newton basis polynomial, an implicit one-to-one map between Leja nodes and orthogonal basis polynomials exists as well. Therefore, the in-between steps of hierarchical interpolation and basis transform can be discarded altogether, and the interpolating PCE can be computed directly. For directly computed, adaptive, anisotropic interpolating PCEs, the dimension-adaptive algorithm is modified accordingly. A series of numerical experiments verify the suggested approach in both low and moderately high-dimensional settings, as well as for various input distributions.},
  comment  = {Used UQLab to construct sparse PCE.},
  file     = {:Loukrezis2019a - Adaptive Sparse Polynomial Chaos Expansions via Leja Interpolation.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://arxiv.org/abs/1911.08312},
}

@Article{Schneider2020a,
  author   = {Aurel Schneider and Alexandre Refregier and Sebastian Grandis and Dominique Eckert and Nicola Stoira and Tomasz Kacprzak and Mischa Knabenhans and Joachim Stadel and Romain Teyssier},
  journal  = {Journal of Cosmology and Astroparticle Physics},
  title    = {Baryonic effects for weak lensing: {II.} Combination with {X}-ray data and extended cosmologies},
  year     = {2020},
  volume   = {2020},
  abstract = {An accurate modelling of baryonic feedback effects is required to exploit the full potential of future weak-lensing surveys such as Euclid or LSST . In this second paper in a series of two, we combine Euclid-like mock data of the cosmic shear power spectrum with an eROSITA X-ray mock of the cluster gas fraction to run a combined likelihood analysis including both cosmological and baryonic parameters. Following the first paper of this series, the baryonic effects (based on the baryonic correction model of ref. [1]) are included in both the tomographic power spectrum and the covariance matrix. However, this time we assume the more realistic case of a ΛCDM cosmology with massive neutrinos and we consider several extensions of the currently favoured cosmological model. For the standard ΛCDM case, we show that including X-ray data reduces the uncertainties on the sum of the neutrino mass by ~30 percent, while there is only a mild improvement on other parameters such as Ωm and σ8. As extensions of ΛCDM, we consider the cases of a dynamical dark energy model (wCDM), a f(R) gravity model (fRCDM), and a mixed dark matter model (ΛMDM) with both a cold and a warm/hot dark matter component. We find that combining weak-lensing with X-ray data only leads to a mild improvement of the constraints on the additional parameters of wCDM, while the improvement is more substantial for both fRCDM and ΛMDM . Ignoring baryonic effects in the analysis pipeline leads to significant false-detections of either phantom dark energy or a light subdominant dark matter component. Overall we conclude that for all cosmologies considered, a general parametrisation of baryonic effects is both necessary and sufficient to obtain tight constraints on cosmological parameters.},
  comment  = {Used UQLab to create PCE metamodel.},
  doi      = {10.1088/1475-7516/2020/04/020},
  file     = {:Schneider2020a - Baryonic Effects for Weak Lensing_ II. Combination with X Ray Data and Extended Cosmologies.pdf:PDF},
  keywords = {physics, used UQLab},
  url      = {https://doi.org/10.1088/1475-7516/2020/04/020},
}

@Article{Thapa2020,
  author   = {Mishal Thapa and Sameer B. Mulani and Robert W. Walters},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Adaptive weighted least-squares polynomial chaos expansion with basis adaptivity and sequential adaptive sampling},
  year     = {2020},
  volume   = {360},
  abstract = {An efficient framework to obtain stochastic models of responses with polynomial chaos expansion (PCE) using an adaptive least-squares approach is presented in this paper. PCE is a high accuracy spectral expansion technique for uncertainty quantification; however, it is hugely affected by the curse of dimensionality with the increase in stochastic dimensions. To alleviate this effect, the basis polynomials are added in an adaptive manner, unlike selecting basis polynomials from a large predefined set in the traditional approach. Also, a refinement strategy is proposed to cull the unnecessary PCE terms based on their contribution to the variance of the response. Furthermore, a sequential optimal sampling is utilized that is capable of adding new samples based on the most recent basis polynomials and also reutilizes the old set of samples. The additional highlights of the algorithm include the implementation of weighted least-squares to reduce the effect of outliers and Kullback–Leibler Divergence to check the convergence of PCE. The algorithm has been implemented to analytical benchmark problems and a composite laminate problem. The substantial computational savings of the proposed framework compared to traditional PCE approaches and a large number of random simulations to achieve similar accuracy were demonstrated by the results.},
  comment  = {Used sparse PCE in UQLab for benchmarking to own's method.},
  doi      = {10.1016/j.cma.2019.112759},
  file     = {:Thapa2020 - Adaptive Weighted Least Squares Polynomial Chaos Expansion with Basis Adaptivity and Sequential Adaptive Sampling.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1016/j.cma.2019.112759},
}

@Article{Zhang2019,
  author   = {Jiangjiang Zhang and Qiang Zheng and Dingjiang Chen and Laosheng Wu and Lingzao Zeng},
  journal  = {Water Resources Research},
  title    = {Surrogate‐Based {Bayesian} Inverse Modeling of the Hydrological System: An Adaptive Approach Considering Surrogate Approximation Error},
  year     = {2019},
  number   = {1},
  volume   = {56},
  abstract = {Bayesian inverse modeling is important for a better understanding of hydrological processes. However, this approach can be computationally demanding, as it usually requires a large number of model evaluations. To address this issue, one can take advantage of surrogate modeling techniques. Nevertheless, when approximation error of the surrogate model is neglected, the inversion result will be biased. In this paper, we develop a surrogate‐based Bayesian inversion framework that explicitly quantifies and gradually reduces the approximation error of the surrogate. Specifically, two strategies are proposed to quantify the surrogate error. The first strategy works by quantifying the surrogate prediction uncertainty with a Bayesian method, while the second strategy uses another surrogate to simulate and correct the approximation error of the primary surrogate. By adaptively refining the surrogate over the posterior distribution, we can gradually reduce the surrogate approximation error to a small level. Demonstrated with three case studies involving high dimensionality, multimodality, and a real‐world application, it is found that both strategies can reduce the bias introduced by surrogate approximation error, while the second strategy that integrates two methods (i.e., polynomial chaos expansion and Gaussian process in this work) that complement each other shows the best performance.},
  comment  = {Used UQLab for LARS-PCE},
  doi      = {10.1029/2019WR025721},
  file     = {:Zhang2019 - Surrogate‐Based Bayesian Inverse Modeling of the Hydrological System_ An Adaptive Approach Considering Surrogate Approximation Error.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.1029/2019WR025721},
}

@Article{Wang2020b,
  author   = {Heng Wang and Wei Gong and Qingyun Duan and Zhenhua Dia},
  journal  = {Environmental Modelling {\&} Software},
  title    = {Evaluation of parameter interaction effect of hydrological models using the sparse polynomial chaos ({SPC}) method},
  year     = {2020},
  volume   = {125},
  abstract = {Most of the commonly available sensitivity analysis methods cannot reliably compute the interaction effect. Even though the Sobol’ type methods that use Monte Carlo simulation can evaluate the interaction effect, the result is either inaccurate or requires an extraordinary number of model runs to obtain a reasonable estimate. In this study, we evaluate the sparse polynomial chaos (SPC) method as a reasonable way to estimate the interaction effect. This method is evaluated on two mathematical test functions (Ishigami and Sobol’ G) and two hydrologic models (HBV-SASK and SAC-SMA). Our results show the SPC method needs about a sample size of 30 to 70 times the number of dimensions of the parameter space to evaluate the interaction effects of hydrologic models. Our findings are significant for hydrologic simulation and model calibration, as we aim to improve the understanding of complex interactions among model components and to reduce model uncertainty.},
  doi      = {10.1016/j.envsoft.2019.104612},
  file     = {:Wang2020b - Evaluation of parameter interaction effect of hydrological models using the sparse polynomial chaos (SPC) method.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.1016/j.envsoft.2019.104612},
}

@Article{Braun2019,
  author   = {Mathias Braun and Olivier Piller and Angelo Iollo and Iraj Mortazavi},
  journal  = {Journal of Water Resources Planning and Management},
  title    = {A Spectral Approach to Uncertainty Quantificationin Water Distribution Network},
  year     = {2019},
  number   = {3},
  volume   = {146},
  abstract = {To date, the hydraulics of water distribution networks are calculated using deterministic models. Because many of the parameters in these models are not known exactly, it is important to evaluate the effects of their uncertainties on the results through uncertainty analysis. For the propagation of uncertain parameters, this article for the first time applies the polynomial chaos expansion to a hydraulic model and compares the results with those from classical approaches like the first-order second-moment method and Monte Carlo simulations. Results presented in this article show that the accuracy of the polynomial chaos expansion is on the same level as the Monte Carlo simulation. Further, it is concluded that due to its computational efficiency, polynomial chaos expansion is superior to the Monte Carlo simulation.},
  comment  = {Used UQLab for uncertainty propagation.},
  doi      = {10.1061/(ASCE)WR.1943-5452.0001138},
  file     = {:Braun2019 - A Spectral Approach to Uncertainty Quantificationin Water Distribution Network.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.1061/(ASCE)WR.1943-5452.0001138},
}

@Article{Lee2019,
  author   = {Gil-Yong Lee and Yong-Hwa Park},
  journal  = {Applied Sciences},
  title    = {A Combined Nonstationary {Kriging} and Support Vector Machine Method for Stochastic Eigenvalue Analysis of Brake Systems},
  year     = {2019},
  number   = {245},
  volume   = {10},
  abstract = {This paper presents a new metamodel approach based on nonstationary kriging and a support vector machine to efficiently predict the stochastic eigenvalue of brake systems. One of the difficulties in the mode-coupling instability induced by friction is that stochastic eigenvalues represent heterogeneous behavior due to the bifurcation phenomenon. Therefore, the stationarity assumption in kriging, where the response is correlated over the entire random input space, may not remain valid. In this paper, to address this issue, Gibb’s nonstationary kernel with step-wise hyperparameters was adopted to reflect the heterogeneity of the stochastic eigenvalues. In predicting the response for unsampled input, the support vector machine-based classification is utilized. To validate the performance, a simplified finite element model of the brake system is considered. Under various types of uncertainties, including different friction coefficients and material properties, stochastic eigenvalue problems are investigated. Through numerical studies, it is seen that the proposed method improves accuracy and robustness compared to conventional stationary kriging.},
  comment  = {Used UQLab to generate Sobol' sample.},
  doi      = {10.3390/app10010245},
  file     = {:Lee2019 - A Combined Nonstationary Kriging and Support Vector Machine Method for Stochastic Eigenvalue Analysis of Brake Systems.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.3390/app10010245},
}

@Article{Guo2020,
  author   = {Xiangfeng Guo and Daniel Dias},
  journal  = {Computers and Geotechnics},
  title    = {Kriging based reliability and sensitivity analysis – application to the stability of an earth dam},
  year     = {2020},
  volume   = {120},
  abstract = {This article presents a Kriging-based probabilistic analysis of an earth dam. The dam failure probability with respect to the sliding stability is investigated by considering the influence of various factors: the filter drain length, the full reservoir water level location and the correlation between the input parameters. A procedure which combines the Kriging surrogate model with the Monte Carlo Simulation (MCS), the Global Sensitivity Analysis (GSA) and the First Order Reliability Method (FORM) is proposed. It aims at benefiting from the computational efficiency of a Kriging surrogate model to provide as much as possible results such as the failure probability, the sensitivity index of each input parameter and the design point. Having more useful results in a probabilistic analysis can help engineers to make more rational decisions. The proposed procedure is compared with the direct MCS, GSA and FORM, and shows a good accuracy and efficiency. In addition, two commonly used slope stability analysis methods (strength reduction method (SRM) and limit equilibrium method (LEM)) are compared in a probabilistic framework. The comparison shows that the two methods can lead to similar estimates of the failure probability for most cases, except when the pore water pressure is important for the determination of the critical slip surface. This kind of results can help engineers to judge when LEM is accurate enough and when SRM is required for a probabilistic analysis.},
  comment  = {Used UQLab to create Kriging metamodels.},
  doi      = {10.1016/j.compgeo.2019.103411},
  file     = {:Guo2020 - Kriging based reliability and sensitivity analysis – Application to the stability of an earth dam.pdf:PDF},
  keywords = {geoscience, used UQLab},
  url      = {https://doi.org/10.1016/j.compgeo.2019.103411},
}

@Article{Bonato2019,
  author   = {Marta Bonato and Emma Chiaramello and Serena Fiocchi and Gabriella Tognola and Paolo Ravazzani and Marta Parazzini},
  journal  = {IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology},
  title    = {Influence of low frequency Near-Field Sources Position on the Assessment of Children Exposure Variability using Stochastic Dosimetry},
  year     = {2019},
  abstract = {The  objective  of the  present  work  was to  assess  the  children exposure  variability due  to  low  frequency  near-fieldsources using an approach based on stochastic dosimetry. These scenarios represent a topic of high interest, because it was found that domestic appliances could be relevant for children exposure level. In details, in this paper the exposure of two child modelsto  a  hairdryer  model  was  evaluated.  Following  the  ICNIRP  guidelines,  the  electric  field  amplitudes  induced  in  specific  tissuescomposing the central nervous system and the peripheral nervous system were analyzed. The analysis of the results permitted to highlight a high exposure variability depending on the near-field source position and to individuate the regions where the sourcecould cause the highest levels of exposure, not limiting the analysis only to some worst-case exposure scenarios.},
  comment  = {Used UQLab to create a surrogate model.},
  doi      = {10.1109/JERM.2019.2958549},
  file     = {:Bonato2019 - Influence of low frequency Near-Field Sources Position on the Assessment of Children Exposure Variability using Stochastic Dosimetry.pdf:PDF},
  keywords = {biomedical science, used UQLab},
  url      = {https://doi.org/10.1109/JERM.2019.2958549},
}

@Article{Lu2020a,
  author   = {Xuefei Lu and Alessandro Rudi and Emanuele Borgonovo and Lorenzo Rosasco},
  journal  = {Operations Research},
  title    = {Faster {Kriging}: facing high-dimensional simulators},
  year     = {2020},
  number   = {1},
  pages    = {233--249},
  volume   = {68},
  abstract = {Kriging is one of the most widely used emulation methods in simulation. However, memory and time requirements potentially hinder its application to data sets generated by high-dimensional simulators. We borrow from the machine learning literature to propose a new algorithmic implementation of kriging that, while preserving prediction accuracy, notably reduces time and memory requirements. The theoretical and computational foundations of the algorithm are provided. The work then reports results of extensive numerical experiments to compare the performance of the proposed algorithm against current kriging implementations, on simulators of increasing dimensionality. Findings show notable savings in time and memory requirements that allow one to handle inputs across more that 10,000 dimensions.},
  comment  = {Used Kriging module in UQLab for benchmarking purposes.},
  doi      = {10.1287/opre.2019.1860},
  file     = {:Lu2020a - Faster Kriging_ Facing High Dimensional Simulators.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://doi.org/10.1287/opre.2019.1860},
}

@InProceedings{Slobbe2018,
  author    = {Arthur Slobbe and Árpád Rózsás and Diego L. Allaix and Agnieszka Bigaj-van Vliet},
  booktitle = {The International Federation for Structural Concrete 5th International fib Congress},
  title     = {What can we gain from advanced modeling of reinforced concrete structures? The added value of nonlinear finite element analysis and reliability based assessment},
  year      = {2018},
  abstract  = {The state-of-the-practice (SoP) assessment methods for reinforced concrete (RC) structures rely on many simplifications and assumptions, both in the modelling of the structural behaviour and the treatment of uncertainties, making them suitable for relatively fast design calculations. Examples of those can be found e.g. in Eurocodes or fib Model Code 2010, where resistance models are formulated with symbolic (analytical) expressions that can be used for evaluating the performance by semi-probabilistic methods (e.g. partial factor safety formats). On the other hand, the assessment of concrete structures may also be conducted by applying a full-probabilistic nonlinear finite element analysis (NLFEA) method that includes more realistic descriptions of material behaviour and enables to account explicitly for relevant uncertainties. Hence, the full-probabilistic NLFEA method is able to provide a better representation of the actual behaviour and reliability of structures. In this paper, the added value of moving from these SoP assessment methods to full-probabilistic NLFEA is explored. As an illustration, two RC structural members are assessed-a continuous girder and a continuous deep beam-for which the approximation level of mechanical and the probabilistic models is gradually increased. The most advanced assessment method uses NLFEA coupled with full-probabilistic analysis (reliability analysis), where uncertain parameters are represented by random variables. To discuss the added value of various methods, a recently introduced quantitative measure of added value is used. This measure expresses the largest value of relative deficit (non-compliance) that can be compensated by a more advanced method. For both cases, a comparison is made between the outcomes of the semi-probabilistic Eurocode method, the semi-probabilistic NLFEA methods according to Eurocode and fib Model Code 2010, and the full-probabilistic NLFEA method. For the RC continuous girder example, the measure of added value of the full-probabilistic NLFEA over the semi-probabilistic Eurocode method is found to be 0.48. In other words: even if according to the semi-probabilistic Eurocode method the design action is 48% higher than the design resistance, the compliance with the target criteria can successfully be demonstrated by full-probabilistic NLFEA. For the RC continuous deep beam example, this measure of added value is 0.76, hence also in this case reserves can be uncovered when applying the full-probabilistic NLFEA. It is observed that origin of the gain can differ: in case of the continuous girder the gain is almost solely attributed to the use of the probabilistic models, while for the continuous deep beam both the probabilistic models and especially the higher approximation level of NLFEA contribute to the gain. Keeping in mind that the added values and their origin are clearly case dependent, the results indicate that a more detailed physical representation of the problem and an explicit treatment of the uncertainties may uncover substantial reserves, compared to simplified assessment methods. Hence, the full-3589 f i b © fédération internationale du béton (fib). This document may not be copied or distributed without prior permission from fib. probabilistic NLFEA method offer a promising alternative in the assessment of existing structures, enabling to avoid expensive measures that might be needed based on simplified methods.},
  comment   = {Used UQLab for AK-MCS.},
  eventdate = {2018-10-07/2018-10-11},
  file      = {:Slobbe2018 - What can we gain from advanced modeling of reinforced concrete structures_ The added value of nonlinear finite element analysis and reliability based assessment.pdf:PDF},
  keywords  = {civil engineering, used UQLab},
  url       = {https://www.researchgate.net/publication/338677461_What_can_we_gain_from_advanced_modeling_of_reinforced_concrete_structures_The_added_value_of_nonlinear_finite_element_analysis_and_reliability_based_assessment},
}

@Article{Zhang2020,
  author   = {Chi Zhang and Zeyu Wang and Abdollah Shafieezadeh},
  journal  = {arXiv},
  title    = {Value of information analysis via active learning and knowledge sharing in error-controlled adaptive {Kriging}},
  year     = {2020},
  abstract = {Large uncertainties in many phenomena of interest have challenged the reliability of pertaining decisions. Collecting additional information to better characterize involved uncertainties is among decision alternatives. Value of information (VoI) analysis is a mathematical decision framework that quantifies expected potential benefits of new data and assists with optimal allocation of resources for information collection. However, a primary  challenge  facing  VoI  analysis  is  the  very  high  computational  cost  of  the  underlying  Bayesian  inference especially for equality-type information. This paper proposes the first surrogate-based framework for  VoI  analysis.  Instead  of  modeling  the  limit  state  functions  describing  events  of  interest  for  decision  making, which is commonly pursued in surrogate model-based reliability methods, the proposed framework models system responses. This approach affords sharing equality-type information from observations among surrogate  models  to  update  likelihoods  of  multiple  events  of  interest.  Moreover,  two  knowledge  sharing  schemes  called  model  and  training  points  sharing  are  proposed  to  most  effectively  take  advantage  of  the  knowledge  offered  by  costly  model  evaluations.  Both  schemes  are  integrated  with  an  error  rate-based adaptive  training  approach  to  efficiently  generate  accurate  Kriging  surrogate  models.  The  proposed  VoI  analysis  framework  is  applied  for  an  optimal  decision-making  problem  involving  load  testing  of  a  truss  bridge.  While  state-of-the-art  methods  based  on  importance  sampling  and  adaptive  Kriging  Monte  Carlo  simulation  are  unable  to  solve  this  problem,  the  proposed  method  is  shown  to  offer  accurate  and  robust  estimates of VoI with a limited number of model evaluations. Therefore, the proposed method facilitates the application of VoI for complex decision problems.},
  comment  = {Large uncertainties in many phenomena of interest have challenged the reliability of pertaining decisions. Collecting additional information to better characterize involved uncertainties is among decision alternatives. Value of information (VoI) analysis is a mathematical decision framework that quantifies expected potential benefits of new data and assists with optimal allocation of resources for information collection. However, a primary challenge facing VoI analysis is the very high computational cost of the underlying Bayesian inference especially for equality-type information. This paper proposes the first surrogate-based framework for VoI analysis. Instead of modeling the limit state functions describing events of interest for decision making, which is commonly pursued in surrogate model-based reliability methods, the proposed framework models system responses. This approach affords sharing equality-type information from observations among surrogate models to update likelihoods of multiple events of interest. Moreover, two knowledge sharing schemes called model and training points sharing are proposed to most effectively take advantage of the knowledge offered by costly model evaluations. Both schemes are integrated with an error rate-based adaptive training approach to efficiently generate accurate Kriging surrogate models. The proposed VoI analysis framework is applied for an optimal decision-making problem involving load testing of a truss bridge. While state-of-the-art methods based on importance sampling and adaptive Kriging Monte Carlo simulation are unable to solve this problem, the proposed method is shown to offer accurate and robust estimates of VoI with a limited number of model evaluations. Therefore, the proposed method facilitates the application of VoI for complex decision problems.},
  file     = {:Zhang2020 - Value of Information Analysis via Active Learning and Knowledge Sharing in Error-Controlled Adaptive Kriging.pdf:PDF},
  keywords = {computational science and engineering, used UQLab},
  url      = {https://arxiv.org/abs/2002.02354},
}

@InProceedings{Bilicz2018,
  author    = {Sándor Bilicz and Arnold Bingler},
  booktitle = {23rd International Workshop on Electromagnetic Nondestructive Evaluation (ENDE2018)},
  title     = {Low-Rank Approximations in Sensitivity Analysis Applied to Electromagnetic Nondestructive Evaluation},
  year      = {2018},
  address   = {Detroit, Michigan},
  abstract  = {The global sensitivity analysis of electromagnetic nondestructive evaluation (ENDE) is dealt with in this work. The goal is to calculate the Sobol’ indices, which quantitatively describe the sensitivity of the output(s) with respect to the input variability of the ENDE model. To reduce the computational burden, a recent surrogate modeling technique, the canonical low-rank approximation is applied. Studies are presented via representative examples from eddy-current and magnetic flux leakage nondestructive evaluation.},
  comment   = {Used UQLab for LRA metamodel.},
  eventdate = {2018-09-09/2018-09-13},
  file      = {:Bilicz2018 - Low-Rank Approximations in Sensitivity Analysis Applied to Electromagnetic Nondestructive Evaluation.pdf:PDF},
  keywords  = {used UQLab, monitoring and remote sensing},
  url       = {http://ebooks.iospress.nl/volumearticle/53612},
}

@MastersThesis{Sariguel2019,
  author   = {Murat Bari\cs Sarig\"ul},
  school   = {Middle East Technical University},
  title    = {Mechanical reliability analysis using stress-strength interference model and engineering applications of reliability: system safety assessment},
  year     = {2019},
  address  = {Turkey},
  type     = {mathesis},
  abstract = {Mechanical  reliability  is  the  main  concern  of  this  thesis.  Reliability  prediction  of mechanical components is commonly performed via four different techniques which are Component Failure Data Analysis, Empirical Reliability Analysis, Stress-Strength Interference Model and Reliability Database and Handbook Usage. However, Stress-Strength Reliability Model, which has a great popularity in these reliability estimation methods in literature, is the main focus of this research. Five  methods  are  used  in  calculations  modelled  with  Stress-Strength  Interference. These  are  analytical  methods,  which  are  First  Order  Reliability  Method  (FORM), Second Order Reliability Method (SORM), and simulation methods, which are Monte Carlo Simulation (MCS), Importance Sampling (IS), Adaptive Kriging Monte Carlo Simulation  (AK-MCS).  Landing  Gear  Emergency  Extension  System  is  taken  into account and analyzed in details. Venting valve, which is a part of this subsystem, is chosen as an example of mechanical component. While stress random variables of the venting valve are assumed to be uniformly distributed, strength random variables are considered  as  both  normal  and  lognormal  random  variables  in  calculations.  Five different reliability  methods are performed in this manner and  results are compared 
viextensively. Number of evaluation is also assessed as an indication of the strength of the method.System safety assessment is an application area of reliability values. “SAE ARP 4761 Aerospace Recommended Practice” defines the process of safety assessment of the systems. A reliability requirement for venting valve is created in this manner and an example of validation is performed which shows the relation between system safety engineering and reliability engineering disciplines.},
  comment  = {Used UQLab to conduct MCS for reliability analysis.},
  file     = {:Sariguel2019 - Mechanical reliability analysis using stress-strength interference model and engineering applications of reliability_ system safety assessment.pdf:PDF},
  keywords = {mechanical engineering, used UQLab},
  url      = {http://etd.lib.metu.edu.tr/upload/12624628/index.pdf},
}

@Article{Tran2020,
  author   = {Vinh Ngoc Tran and M. Chase Dwelle and Khachic Sargsyan and Valeriy Y. Ivanov and Jongho Kim},
  journal  = {Water Resource Research},
  title    = {A novel modeling framework for computationally efficient and accurate real-time ensemble flood forecasting with uncertainty quantification},
  year     = {2020},
  number   = {3},
  volume   = {56},
  abstract = {A novel modeling framework that simultaneously improves accuracy, predictability, and computational efficiency is presented. It embraces the benefits of three modeling techniques integrated together for the first time: surrogate modeling, parameter inference, and data assimilation. The use of polynomial chaos expansion (PCE) surrogates significantly decreases computational time. Parameter inference allows for model faster convergence, reduced uncertainty, and superior accuracy of simulated results. Ensemble Kalman filters assimilate errors that occur during forecasting. To examine the applicability and effectiveness of the integrated framework, we developed 18 approaches according to how surrogate models are constructed, what type of parameter distributions are used as model inputs, and whether model parameters are updated during the data assimilation procedure. We conclude that (1) PCE must be built over various forcing and flow conditions, and in contrast to previous studies, it does not need to be rebuilt at each time step; (2) model parameter specification that relies on constrained, posterior information of parameters (so‐called Selected specification) can significantly improve forecasting performance and reduce uncertainty bounds compared to Random specification using prior information of parameters; and (3) no substantial differences in results exist between single and dual ensemble Kalman filters, but the latter better simulates flood peaks. The use of PCE effectively compensates for the computational load added by the parameter inference and data assimilation (up to ~80 times faster). Therefore, the presented approach contributes to a shift in modeling paradigm arguing that complex, high‐fidelity hydrologic and hydraulic models should be increasingly adopted for real‐time and ensemble flood forecasting.},
  doi      = {10.1029/2019WR025727},
  file     = {:Tran2020 - A Novel Modeling Framework for Computationally Efficient and Accurate Real Time Ensemble Flood Forecasting with Uncertainty Quantification.pdf:PDF},
  keywords = {used PCE in UQLab, Geoscience},
  url      = {https://doi.org/10.1029/2019WR025727},
}

@Article{Das2020,
  author   = {Sourav Das and Solomon Tesfamariam and Yangyang Chen and Zhichao Qian and Ping Tan and Fulin Zhou},
  journal  = {Journal of Sound and Vibration},
  title    = {Reliability-based optimization of nonlinear energy sink with negative stiffness and sliding friction},
  year     = {2020},
  volume   = {485},
  abstract = {This paper presents implementation of a reliability-based design optimization (RBDO) for Nonlinear Energy Sink (NES) system. The RBDO is implemented with consideration of uncertainties in structural system's parameters and ground acceleration. Negative stiffness based NES and sliding friction are utilized to enhance passive targeted energy transfer from the primary structure to the proposed control device. For a one-story steel moment resistant frame, shake table tests were carried out to show effectiveness of the NES, and the results are used to validate numerical models. Sensitivity analysis is carried out to demonstrate the performance envelops of the proposed control strategy for different stiffness ratios and peak ground acceleration values. Reliable performance of the proposed controller, through tuning parameters of the proposed NES, are determined through the RBDO framework. To reduce computational time, polynomial-chaos-based Kriging surrogate model is used in the RBDO analysis. The numerical results demonstrate efficiency of finding optimal design parameters of the proposed NES system under uncertainties with reasonable computational effort.},
  doi      = {10.1016/j.jsv.2020.115560},
  file     = {:Das2020 - Reliability Based Optimization of Nonlinear Energy Sink with Negative Stiffness and Sliding Friction.pdf:PDF},
  keywords = {used PCK in UQLab, civil engineering},
  url      = {https://doi.org/10.1016/j.jsv.2020.115560},
}

@Article{Ceravolo2020,
  author   = {Rosario Ceravolo and Alessio Faraci and Gaetano Miraglia},
  journal  = {Applied Sciences},
  title    = {Bayesian calibration of hysteretic parameters with consideration of the model discrepancy for use in seismic structural health monitoring},
  year     = {2020},
  number   = {17},
  volume   = {10},
  abstract = {Bayesian model calibration techniques are commonly employed in the characterization of nonlinear dynamic systems, as they provide a conceptual and effective framework to deal with model uncertainties, experimental errors and procedure assumptions. This understanding has resulted in the need to introduce a model discrepancy term to account for the differences between model-based predictions and real observations. Indeed, the goal of this work is to investigate model-driven seismic structural health monitoring procedures based on a Bayesian uncertainty quantification framework, and thus make relevant considerations for its use in the seismic structural health monitoring, focusing on masonry structures. Specifically, the Bayesian inference has been applied to the calibration of nonlinear hysteretic systems to both provide: (i) most probable values (MPV) of the parameters following the calibration; and (ii) estimates of the model discrepancy posterior distribution. The effect of the model discrepancy in the calibration is first illustrated recurring to a single degree of freedom using a Bouc–Wen type oscillator as a numerical benchmark. The model discrepancy is then introduced for calibrating a reference nonlinear Bouc–Wen model derived from real data acquired on a monitored masonry building. The main novelty of this study is the application of the framework of uncertainty quantification on models representing data measured directly on masonry structures during seismic events.},
  doi      = {10.3390/app10175813},
  file     = {:Ceravolo2020 - Bayesian Calibration of Hysteretic Parameters with Consideration of the Model Discrepancy for Use in Seismic Structural Health Monitoring.pdf:PDF},
  keywords = {used Inversion in UQLab, civil engineering},
  url      = {https://doi.org/10.3390/app10175813},
}

@InProceedings{Jia2020,
  author    = {Bin Jia and Ming Xin},
  booktitle = {2020 American Control Conference ({ACC})},
  title     = {Orbital uncertainty propagation with {PC-Kriging}},
  year      = {2020},
  address   = {Denver, Colorado},
  publisher = {IEEE},
  abstract  = {In this paper, the polynomial chaos based Kriging (PC-Kriging) is utilized as a surrogate model for orbital uncertainty propagation. The polynomial chaos can represent the global trend of the uncertainty distribution whereas the Kriging captures the local uncertainty variations. A new learning strategy is proposed to incrementally build and improve the PC-Kriging model. This new PC-Kriging scheme only requires a small number of sampling points while achieving close performance to the Monte-Carlo based propagation. It is also more accurate than the random sampling based Kriging model. An orbital uncertainty propagation example is used to demonstrate the effectiveness of the proposed algorithm.},
  doi       = {10.23919/ACC45564.2020.9147914},
  eventdate = {2020-07-01/2020-07-03},
  file      = {:Jia2020 - Orbital Uncertainty Propagation with PC Kriging.pdf:PDF},
  keywords  = {used PCK in UQLab, mechanical engineering},
  url       = {https://doi.org/10.23919/ACC45564.2020.9147914},
}

@Article{HernandezBecerro2020,
  author   = {Pablo Hernández-Becerro and Joel Purtschert and Jan Konvicka and Christian Buesser and David Schranz and Josef Mayr and Konrad Wegener},
  journal  = {Journal of Manufacturing Science and Engineering},
  title    = {Reduced-order model of the environmental variation error of a precision five-axis machine tool},
  year     = {2020},
  number   = {2},
  volume   = {143},
  abstract = {Thermo-mechanical models, based on the discretization of the heat transfer and elasticity equations, enable the analysis and optimization of the thermal design of machine tools. This work investigates the thermo-mechanical response of a five-axis precision machine tool to fluctuations of the environmental temperature. To increase the computational efficiency of the thermo-mechanical model, a surrogate model by means of projection-based model order reduction (MOR) is created. This article uses the parametric Krylov Modal Subspace (KMS) method, which enables the evaluation of the thermo-mechanical response of the machine tool for different values of the parameters describing the convective boundary conditions. The thermo-mechanical model is validated comparing the simulated and measured response of the machine tool to a step in the environmental temperature. The validation process uses the global sensitivity analysis (GSA) to determine the convective boundary conditions with the largest impact on the thermally induced deviations. The reduced-order model ensures the computational tractability of the Monte Carlo simulation associated with the sensitivity analysis and parameter identification. The validated thermo-mechanical model is used to investigate the thermo-mechanical design of the machine tool.},
  doi      = {10.1115/1.4047739},
  file     = {:HernandezBecerro2020 - Reduced Order Model of the Environmental Variation Error of a Precision Five Axis Machine Tool.pdf:PDF},
  keywords = {used MC and PCE in UQLab, mechanical engineering},
  url      = {https://doi.org/10.1115/1.4047739},
}

@Article{Zhang2020a,
  author   = {Hongbo Zhang and Younes Aoues and Didier Lemosse and Eduardo Souza de Cursi},
  journal  = {Engineering Optimization},
  title    = {A single-loop approach with adaptive sampling and surrogatekriging for reliability-based design optimization},
  year     = {2020},
  abstract = {Surrogate models have been widely used for Reliability-Based Design Optimization (RBDO) to solve complex engineering problems. However, the accuracy and efficiency of surrogate-based RBDO largely rely on the sample size and sampling methods. For this reason, successive sampling methods that update the surrogate successively are more promising. Nowadays, several Kriging-based RBDO approaches have been proposed with different successive sampling techniques. However, these approaches are based on Monte Carlo simulations and double-loop approaches such that most of them would be time consuming for high target reliability levels or high dimensional problems. To improve the efficiency of surrogate-based RBDO, this article proposes a Single-Loop Approach (SLA) combined with the Kriging surrogate. This Kriging model is updated efficiently by using the Most Probable Points (MPPs) from the last SLA iteration. A very simple and effective stopping criterion is proposed. Compared with other sampling methods, the initial Kriging can be started with very few training points and converges to the right optimum very efficiently. Three mathematical examples and a practical engineering problem are used to demonstrate the effectiveness, the advantages and also the limitations of this method.},
  doi      = {10.1080/0305215X.2020.1800664},
  file     = {:Zhang2020a - A Single Loop Approach with Adaptive Sampling and SurrogateKriging for Reliability Based Design Optimization.pdf:PDF},
  keywords = {used AKMCS in UQLab, mechanical engineering},
  url      = {https://doi.org/10.1080/0305215X.2020.1800664},
}

@Article{Zhu2020,
  author   = {Bin Zhu and Tetsuya Hiraishi and Huafu Pei and Qing Yang},
  journal  = {International Journal for Numerical and Analytical Methods in Geomechanics},
  title    = {Efficient reliability analysis of slopes integrating the random field method and a gaussian process regression-based surrogate model},
  year     = {2020},
  abstract = {Efficient evaluation of slope stability is a frontier in geo‐disaster prevention fields. While many slope stability evaluation methods, ranging from deterministic to probabilistic, have been proposed, reliability methods are particularly advantageous as they can account for uncertainties during slope stability evaluation. To address the problem of the low efficiency of direct Monte Carlo simulations and overcome the defects of the traditional response surface method, in this work, a novel probabilistic procedure that integrates a Gaussian process regression‐based surrogate model and the random limit equilibrium method for slope stability evaluation while accounting for the spatial variability of soil strength is proposed. The novel Gaussian process regression‐based surrogate model is efficiently used in the Monte Carlo simulation to reduce the number of calls for direct stability analysis of a slope with spatially varied soil strength. To verify the accuracy and efficiency of the proposed procedure, it was applied to three case studies: the first one is a slope in saturated clay under undrained conditions; the second one is a slope for which the friction angle and cohesion values are cross‐correlated; and the third one is a real slope with multiple soil layers. The results obtained from the comparisons with other methods confirmed the precision and feasibility of the proposed procedure.},
  doi      = {10.1002/nag.3169},
  file     = {:Zhu2020 - Efficient Reliability Analysis of Slopes Integrating the Random Field Method and a Gaussian Process Regression Based Surrogate Model.pdf:PDF},
  keywords = {used GPR in UQLab, geomechanics},
  url      = {https://doi.org/10.1002/nag.3169},
}

@Article{Wang2020c,
  author   = {Binglin Wang and Xiaojun Duan and Liang Yan and Juan Deng and Jiangtao Chen},
  journal  = {Entropy},
  title    = {Rapidly tuning the {PID} controller based on the regional surrogate model technique in the {UAV} formation},
  year     = {2020},
  number   = {5},
  volume   = {22},
  abstract = {The leader–follower structure is widely used in unmanned aerial vehicle formation. This paper adopts the proportional-integral-derivative (PID) and the linear quadratic regulator controllers to construct the leader–follower formation. Tuning the PID controllers is generally empirical; hence, various surrogate models have been introduced to identify more refined parameters with relatively lower cost. However, the construction of surrogate models faces the problem that the singular points may affect the accuracy, such that the global surrogate models may be invalid. Thus, to tune controllers quickly and accurately, the regional surrogate model technique (RSMT), based on analyzing the regional information entropy, is proposed. The proposed RSMT cooperates only with the successful samples to mitigate the effect of singular points along with a classifier screening failed samples. Implementing the RSMT with various kinds of surrogate models, this study evaluates the Pareto fronts of the original simulation model and the RSMT to compare their effectiveness. The results show that the RSMT can accurately reconstruct the simulation model. Compared with the global surrogate models, the RSMT reduces the run time of tuning PID controllers by one order of magnitude, and it improves the accuracy of surrogate models by dozens of orders of magnitude.},
  doi      = {10.3390/e22050527},
  file     = {:Wang2020c - Rapidly Tuning the PID Controller Based on the Regional Surrogate Model Technique in the UAV Formation.pdf:PDF},
  keywords = {used PCK/Kriging/PCE in UQLab, mechanical engineering},
  url      = {https://doi.org/10.3390/e22050527},
}

@PhdThesis{Slot2019,
  author   = {René Meklenborg Miltersen Slot},
  school   = {Aalborg Universitet},
  title    = {From Wind Climate to Wind Turbine Loads: efficient and accurate decision support and reliability analysis},
  year     = {2019},
  address  = {Aalborg, Denmark},
  type     = {phdthesis},
  abstract = {Wind energy plays a critical role in reaching the climate goals set out by the Paris
Agreement. To keep wind turbines a main pillar of sustainable energy, improved
decision support is required to fully exploit the best wind resources avaliable. This
thesis aims to improve accuracy and efficiency in onshore wind turbine site suitability
evaluations. It quantifies how critical and uncertain assumptions on wind climate
measurements propagate to uncertainties in the estimated fatigue loads on wind
turbine components and affect their structural reliability.

The IEC 61400-1 design standard accounts for variability in turbulence for fatigue
loads by a wind speed dependent 90% quantile of the observed turbulence including
wake contributions. However, for wind shear the overall mean value is considered,
accounting neither for the wind speed dependence of shear nor for the significant shear
variability around the mean. Using wind data from 99 real sites, this thesis shows how
this leads to inconsistent fatigue load assessment of wind turbine blades. Instead, a
60% quantile of wind shear conditioned on wind speed is proposed, which reduces
the inconsistency across real sites by a factor of approximately two.
The effective turbulence approximation is widely used to significantly reduce the
required aeroelastic simulations in site-suitability assessment. The approximation is
slightly conservative and leads to fatigue load predictions within 4% on average when
compared to full sector-wise simulations. An inevitable consequence of the effective
turbulence is that directional information of turbulence is lost, which is a
simplification for components below the yaw bearing. This leads to fatigue load overpredictions of 14% on average across the 99 available sites. A simple method is
proposed to avoid this over-prediction and thereby significantly reduce material
consumption of steel towers.

Traditional engineering methods for wind turbine structural evaluations are based on
deterministic design approaches. This is a necessity to limit the computational
requirement and the design efforts for the engineers. Deterministic methods are
designed to ensure a low probability of structural failure, but this probability of failure
cannot be quantified or divided into individual contributions from climate modelling,
model uncertainties or material uncertainties. In this thesis, a fast and efficient
probabilistic framework is developed by utilizing a Kriging surrogate model to
approximate fatigue loads. The framework quantifies the relative importance of the
site-specific climate conditions, from the model uncertainties as well as the
uncertainty related to the strength and material models. Analyses of the relevant
uncertainties lead to insight into the relative importance of different wind farm
planning decisions such as mast equipment and height. This improved knowledge
provides a sound basis for rational decision-making by identifying the most important
uncertainties that can be reduced to increase structural reliability of wind turbines.},
  file     = {:Slot2019 - From Wind Climate to Wind Turbine Loads_ Efficient and Accurate Decision Support and Reliability Analysis.pdf:PDF},
  keywords = {used UQLab, mechanical engineering},
  url      = {https://vbn.aau.dk/en/publications/from-wind-climate-to-wind-turbine-loads-efficient-and-accurate-de},
}

@InCollection{Roemer2020,
  author    = {Ulrich Römer and Shanza Ali Zafar and Nicolas Fezans},
  booktitle = {Fundamentals of High Lift for Future Civil Aircraft. Notes on Numerical Fluid Mechanics and Multidisciplinary Design 145},
  publisher = {Springer},
  title     = {A multifidelity approach for uncertaintypropagation in flight dynamics},
  year      = {2020},
  editor    = {R. Radespiel and R. Semaan},
  abstract  = {This paper is concerned with the estimation of failure probabilities and statistical moments in a flight dynamics context with multifidelity Monte Carlo techniques. A propulsion-based emergency autoland scenario for the ATTAS research aircraft is considered, where stochastic wind disturbances complicate the landing. All statistical quantities of interest are inferred from samples of the wind shear and the corresponding aircraft touchdown parameters. The underlying Monte Carlo methodology ensures un-biased and reliable estimation of statistical quantities of interest. The advantages and limitations of using black-box surrogates as low-fidelity models are revealed through a series of numerical examples. In particular, signifficant computational gains can be achieved in some cases, however, advanced surrogate modeling may be required for some solution parameters.},
  doi       = {10.1007/978-3-030-52429-6_28},
  file      = {:Roemer2020 - A Multifidelity Approach for UncertaintyPropagation in Flight Dynamics.pdf:PDF},
  keywords  = {used Kriging in UQLab, mechanical engineering},
  url       = {https://doi.org/10.1007/978-3-030-52429-6_28},
}

@Article{Voulpiotis2021,
  author   = {Konstantinos Voulpiotis and Jochen Köhler and Robert Jockwerc and Andrea Frangi},
  journal  = {Engineering Structures},
  title    = {A holistic framework for designing for structural robustness in tall timber buildings},
  year     = {2021},
  volume   = {227},
  abstract = {With the ever-increasing popularity of engineered wood products, larger and more complex structures made of timber have been built, such as new tall timber buildings of unprecedented height. Designing for structural robustness in tall timber buildings is still not well understood due the complex properties of timber and the difficulty in testing large assemblies, making the prediction of tall timber building behaviour under damage very difficult. This paper discusses briefly the existing state-of-the-art and suggests the next step in considering robustness holistically. Qualitatively, this is done by introducing the concept of scale, that is to consider robustness at multiple levels within a structure: in the whole structure, compartments, components, connections, connectors, and material. Additionally, considering both local and global exposures is key in coming up with a sound conceptual design. Quantitatively, the method to calculate the robustness index in a building is presented. A novel framework to quantify robustness and find the optimal structural solution is presented, based on the calculation of the scenario probability-weighted average robustness indices of various design options of a building. A case study example is also presented in the end.},
  doi      = {10.1016/j.engstruct.2020.111432},
  file     = {:Voulpiotis2021 - A Holistic Framework for Designing for Structural Robustness in Tall Timber Buildings.pdf:PDF},
  keywords = {used UQ propagation in UQLab, civil engineering},
  url      = {https://doi.org/10.1016/j.engstruct.2020.111432},
}

@Article{Wang2020d,
  author   = {Xiaoting Wang and Xiaozhe Wang and Hao Sheng and Xi Lin},
  journal  = {arXiv},
  title    = {A data-driven sparse polynomial chaos expansion method to assess probabilistic total transfer capability for power systems with renewables},
  year     = {2020},
  abstract = {The increasing uncertainty level caused by growing renewable energy sources (RES) and aging transmission networks poses a great challenge in the assessment of total transfer capability (TTC) and available transfer capability (ATC). In this paper, a novel data-driven sparse polynomial chaos expansion (DDSPCE) method is proposed for estimating the probabilistic characteristics (e.g., mean, variance, probability distribution) of probabilistic TTC (PTTC). Specifically, the proposed method, requiring no pre-assumed probabilistic distributions of random inputs, exploits data sets directly in estimating the PTTC. Besides, a sparse scheme is integrated to improve the computational efficiency. Numerical studies on the modified IEEE 118-bus system demonstrate that the proposed DDSPCE method can achieve accurate estimation for the probabilistic characteristics of PTTC with a high efficiency. Moreover, numerical results reveal the great significance of incorporating discrete random inputs in PTTC and ATC assessment, which nevertheless was not given sufficient attention.},
  file     = {:Wang2020d - A Data Driven Sparse Polynomial Chaos Expansion Method to Assess Probabilistic Total Transfer Capability for Power Systems with Renewables.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://arxiv.org/abs/2010.14358},
}

@PhdThesis{Groenquist2020,
  author   = {Philippe Grönquist},
  school   = {ETH Zürich},
  title    = {Smart manufacturing of curved mass timber components by self-shaping},
  year     = {2020},
  address  = {Zürich, Switzerland},
  type     = {phdthesis},
  abstract = {With the rise of complex and free-form timber architecture enabled by
digital design and fabrication, timber manufacturing companies increasingly need to produce curved components. In this thesis, a novel approach
for the manufacturing of curved timber building components is proposed
and analyzed. Following biological role models such as the bending of
pine cone scales, a smart way to curve wood at large-scale is given by
the biomimetic concept of bi-layered laminated wood. This principle enables large programmed material deformations upon controlled moisture
content change. The main objectives of this thesis are the in-depth understanding of the mechanics of self-shaping wood bilayers and the up-scaling
of the already known principle from the laboratory to the industrial scale
in order to enable an application as form-stable curved elements in architecture. Hereby, the main challenges addressed are the accurate prediction
of shape-change in terms of the natural variability in wood material parameters, the scale-dependent impact of moisture gradients on mechanical
behavior, and the influence of wood-specific time- and moisture-dependent
deformation mechanisms such as creep or mechano-sorption in the shaping process. Major impacts of these aspects on the shaping behavior could
be demonstrated by the use of continuum-mechanical material models
adapted to wood, both in the form of analytical and numerical models.
Based on the gained insight, the up-scaling process to industrial manufacturing was successfully made possible. A collaborative project realized in
2019, the 14 m high Urbach tower, is presented as a proof of concept for application and competitiveness of the novel biomimetic method for production of curved mass timber components. Furthermore, next to self-shaping
by bending to single-curved components, possibilities and limitations for
achieving double-curved structures using wood bilayers in a gridshell configuration are analyzed and discussed.},
  file     = {:Groenquist2020 - Smart Manufacturing of Curved Mass Timber Components by Self Shaping.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://www.research-collection.ethz.ch/handle/20.500.11850/405617},
}

@Article{Guo2020a,
  author   = {Shuai Guo and Camilo F. Silva and Kah Joon Yong and Wolfgang Polifke},
  journal  = {Proceedings of the Combustion Institute},
  title    = {A {Gaussian-process-based} framework for high-dimensional uncertainty quantification analysis in thermoacoustic instability predictions},
  year     = {2020},
  abstract = {When combining a flame model with acoustic tools to predict thermoacoustic instability, uncertainties embedded in the flame model and acoustic system parameters propagate through the thermoacoustic model, inducing variations in calculation results. Therefore, uncertainty quantification (UQ) analysis is essential for delivering a reliable prediction of thermoacoustic instability. The present paper proposes a general, surrogate-based framework to efficiently perform UQ analysis in thermoacoustic instability predictions that (1) can handle large variational ranges and flexible statistical descriptions of the uncertain parameters, (2) takes into account uncertainties from both acoustic system parameters and high-dimensional flame response models (e.g. the finite impulse response model (FIR), the flame describing function (FDF), etc.), (3) quantifies uncertainties in modal frequency and linear growth rate for linear thermoacoustic analysis, or (4) quantifies uncertainties in limit cycle frequency and amplitude for nonlinear thermoacoustic analysis. The framework is built upon Gaussian process (GP) surrogate models. An active learning strategy from the machine learning community has been adopted to significantly enhance the efficiency of GP model training, thus achieving a significant reduction in computational cost. The effectiveness of the proposed UQ framework is demonstrated by two case studies: one linear case with an uncertain FIR model and acoustic system parameters, and one nonlinear case with an uncertain FDF dataset and acoustic system parameters. Compared with reference Monte Carlo simulations, the case studies reveal UQ analyses that are, respectively, 20 and 15 times faster, but nevertheless highly accurate. The proposed GP-based framework also forms an efficient foundation on which to address other types of studies, in which repetitive thermoacoustic calculations are required, such as parametric investigations, sensitivity analyses, nonlinear bifurcation studies and robust design.},
  doi      = {10.1016/j.proci.2020.06.229},
  file     = {:Guo2020a - A Gaussian Process Based Framework for High Dimensional Uncertainty Quantification Analysis in Thermoacoustic Instability Predictions.pdf:PDF},
  keywords = {used Kriging in UQLab, mechanical engineering},
  url      = {https://doi.org/10.1016/j.proci.2020.06.229},
}

@MastersThesis{Paixao2020,
  author   = {Paixão, Jessé Augusto dos Santos},
  school   = {Universidade Estadual Paulista},
  title    = {Damage quantification in laminated composites using gaussian process regression model},
  year     = {2020},
  address  = {São Paulo, Brazil},
  type     = {mathesis},
  abstract = {After detecting initial damage in a composite structure through a data-driven approach, the user needs to decide if there is an imminent structural failure or if the system can be kept in operation under monitoring to track the damage progression and its impact on the structural safety condition. In this scenario, it is essential to quantify the extension and the damage's size to decide about these points. Therefore, this work proposes a damage quantification based on the application of the Gaussian Process Regression (GPR) model to capture the trend and uncertainties associated with the damage index progression according to damage extension. The GPR model trained in a supervised approach is used to quantify the damage by the stochastic interpolation of the damage indices. The central methodology is proposed for delamination area quantification in laminated composite plates using damage indices based on Lamb wave signals. Autoregressive models are applied to extract damage-sensitive features from Lamb waves signals, and the Mahalanobis squared distance is used to compute damage indices, although any damage features extraction technique could be used and adapted to the proposed methodology. A modified version of the central methodology is proposed to demonstrate the methodology's versatility, using damage indices based on singular spectrum analysis of vibration signals, a well-established technique in the literature. Three sets of tests are used to demonstrate the effectiveness of this approach — one in carbon-epoxy laminate with simulated damage under temperature changes to show the general steps of the procedure; a second test involving a set of carbon fiber reinforced polymer coupons with actual delamination caused by repeated fatigue loads, for which the central methodology is applied; and finally an industrial example involving a wind turbine blade with damage caused by debonding in the trailing edge and using traditional vibration-based damage indices. Various damage progression levels are measured during the tests and monitored using the sensors bonded to these structural surfaces. The GPR proved to be capable of capturing the trend and accommodating the uncertainties related to the damage indices versus the damage size in the simulated spots in the tests. The results manifest a smooth and adequate prediction of the size area of the simulated and real delamination damage in the two first application cases and the debonding size in the last application case.},
  file     = {:Paixao2020 - Damage Quantification in Laminated Composites Using Gaussian Process Regression Model.pdf:PDF},
  keywords = {used Kriging in UQLab, mechanical engineering},
  url      = {https://repositorio.unesp.br/handle/11449/193430},
}

@Article{Memon2020,
  author   = {Zain A. Memon and Riccardo Trinchero and Paolo Manfredi and Flavio Canavero and Igor S. Stievano and Yanzhao Xie},
  journal  = {{IEEE} Letters on Electromagnetic Compatibility Practice and Applications},
  title    = {Machine learning for the uncertainty quantification of power networks},
  year     = {2020},
  abstract = {This paper addresses the uncertainty quantification of a power network and is based on surrogate models built via Machine Learning techniques. Specifically, the least-square support vector machine regression is combined with the principal component analysis to generate a compressed surrogate model capable of predicting all the nodal voltages of the network as a function of the uncertain electrical parameters of the transmission lines. The surrogate model is built from a limited number of system responses provided by the computational model. The power flow analysis of the benchmark IEEE-118 bus system with 250 parameters is considered as a test case. The performance of the proposed modeling strategy in terms of accuracy, efficiency and convergence, are assessed and compared with those of an alternative surrogate model based on a sparse implementation of the polynomial chaos expansion. The results of a Monte Carlo simulation are used as reference in the above comparison.},
  doi      = {10.1109/LEMCPA.2020.3042122},
  file     = {:Memon2020 - Machine Learning for the Uncertainty Quantification of Power Networks.pdf:PDF},
  keywords = {used PCE in UQLab, electrical engineering},
  url      = {https://doi.org/10.1109/LEMCPA.2020.3042122},
}

@Article{Das2020a,
  author   = {Sourav Das and Arunasis Chakraborty and Ipshita Barua},
  journal  = {Smart Materials and Structures},
  title    = {Optimal tuning of {SMA} inerter for simultaneous wind induced vibration control of high-rise building and energy harvesting},
  year     = {2020},
  abstract = {This study proposes shape memory alloy based inerter combined with electromagnetic transducer for both vibration control of buildings and energy harvesting. The proposed device has non-linear spring made of shape memory alloy for its excellent load-deformation characteristics. The hysteretic behaviour of this smart material is capable of dissipating significant amount of energy. The conventional viscous damping is also replaced by a motor, which offers flexibility in damping while converting the mechanical energy into power. The optimal performance of this device demands precise tuning of its parameters for vibration control of the building, which is exposed to random wind load. This, in turn, advocates for the solution of stochastic non-linear optimization problem, which is the main aim of this study. It is proposed in two steps i.e. adopt equivalent linearization for efficient input-output characterization followed by an ensemble surrogate analysis for stochastic response quantification. A seventy six storied benchmark building is used for numerical demonstration, which clearly establishes the superiority of the passive device for simultaneous vibration control and energy harvesting over the possible range of wind speeds. The results show that the ensemble surrogate model is very efficient to predict the responses compared to a single surrogate model. Overall the performance of the controller is impressive and can be adopted for further experimental investigation prior to its use in prototype buildings.},
  doi      = {10.1088/1361-665X/abd42a},
  file     = {:Das2020a - Optimal Tuning of SMA Inerter for Simultaneous Wind Induced Vibration Control of High Rise Building and Energy Harvesting.pdf:PDF},
  keywords = {used SVR in UQLab, civil engineering},
  url      = {https://doi.org/10.1088/1361-665X/abd42a},
}

@InProceedings{Allahvirdizadeh2020,
  author       = {Reza Allahvirdizadeh and Andreas Andersson and Raid Karoumi},
  booktitle    = {{XI} International Conference on Structural Dynamics ({EURODYN 2020})},
  title        = {Reliability assessment of the dynamic behavior of high-speed railways bridges using first order reliability method},
  year         = {2020},
  address      = {Athens, Greece},
  organization = {EASD},
  abstract     = {The operational speed of the trains is intended to be significantly increased forthcoming; which consequently questions the safety level of the current design concepts concerning different performance limits. Thus, the reliability of the simply supported single-span bridges is assessed in the current article adopting the first-order reliability method (FORM) approach. In this regard, the dynamic response of the aimed bridges is investigated under the passage of a series of moving loads using available closed-form solutions in the literature. Hereof, axle load, car body/train configuration, flexural rigidity, damping, mass and model uncertainties are considered as random variables; while traintrack-bridge interactions are neglected and the procedure is repeated for a wide range of span lengths and train velocities. Then, the safety index corresponding to each case is evaluated by considering running safety as the limit state function; where, the bridge deck vertical acceleration is taken as the capacity of the system. The outcomes are presented as average probability of exceeding the limit state versus train speed and categorized based on span lengths.},
  doi          = {10.47964/1120.9282.18654},
  eventdate    = {2019-11-23/2019-11-26},
  file         = {:Allahvirdizadeh2020 - Reliability Assessment of the Dynamic Behavior of High Speed Railways Bridges Using First Order Reliability Method.pdf:PDF},
  keywords     = {used FORM in UQLab, civil engineering},
  url          = {https://doi.org/10.47964/1120.9282.18654},
}

@Article{Melito2020,
  author   = {Gian Marco Melito and Alireza Jafarinia and Thomas Hochrainer and Katrin Ellermann},
  journal  = {Journal of Biomedical Engineering and Biosciences},
  title    = {Sensitivity analysis of a phenomenological thrombosis model and growth rate characterisation},
  year     = {2020},
  pages    = {31--40},
  volume   = {7},
  abstract = {Aortic dissection is a severe cardiovascular disease caused by the occurrence of a tear in the aortic wall. As a result, the blood penetrates the wall and makes a new blood channel called the false lumen. The haemodynamic conditions in the false lumen may contribute to the formation of thrombi, which influence the patient's diagnosis and outcomes. In this study, the focus is on a haemodynamic-based model of thrombus formation. Since the model construction entails uncertainties in the model parameters, a variance-based sensitivity analysis is performed. Thrombus formation at a backward-facing step is considered as a benchmark for the numerical simulations and sensitivity analysis. This geometry is capable of representing the main contributions of the model in thrombus formation. The study aims at improving the understanding of the model's structure and at preparing model simplifications to enable efficient patient-specific simulations in the future. A polynomial chaos expansion is employed as a surrogate model, from which the quantitative sensitivity indices are derived. In this study, nine model parameters are selected, whose proper values are not well known. The model responses taken into account are the maximum volume fraction of thrombus, its time development, and the thrombus growth rate. The results show that the model lends itself to model reduction since some of the model parameters show little to no influence on the model's outputs. A threshold value related to the concentration of bounded platelets and the bounded platelets reaction rate are identified as the key input parameters dominating the thrombus model predictions in the current geometry. Furthermore, the introduced thrombus characteristic growth time is driven by both the aforementioned variables.},
  file     = {:Melito2020 - Sensitivity Analysis of a Phenomenological Thrombosis Model and Growth Rate Characterisation.pdf:PDF},
  keywords = {used PCE in UQLab, biomedical science},
  url      = {https://jbeb.avestia.com/2020/004.html},
}

@Article{Andrew2020,
  author   = {Dallen L. Andrew and Hai‐Chao Han and Juan Ocampo and Adel Alaeddini and Mark Thomsen},
  journal  = {Fatigue {\&} Fracture of Engineering Materials {\&} Structures},
  title    = {Characterization of residual stresses from cold expansion using spatial statistics},
  year     = {2020},
  abstract = {Residual stress fields from cold expansion have been widely used to extend the fatigue life of aircraft structures. However, the spatial statistical character of these residual stress fields has not been established and has not been incorporated in current analysis methods. The objective of this study was to establish a spatial statistical method to quantify the residual stress field around a cold expanded hole. A framework called the Spatial Analysis of Residual Stress (SpARS) was developed utilizing spatial correlation, response surface modelling techniques and statistical resampling methods to characterize the residual stress field. Our results showed that tolerance bounds on residual stress can be quantified using this method. We also demonstrated the SpARS method using recently published round robin case studies. The newly developed model will be useful for aircraft structural fatigue crack growth analyses to incorporate residual stress fields for extending inspection intervals for fatigue and fracture critical structures.},
  doi      = {10.1111/ffe.13334},
  file     = {:Andrew2020 - Characterization of Residual Stresses from Cold Expansion Using Spatial Statistics.pdf:PDF},
  keywords = {used UQLib/Kriging in UQLab, mechanical engineering},
  url      = {https://doi.org/10.1111/ffe.13334},
}

@Article{Hamdi2020,
  author   = {Hamidreza Hamdi and Christopher R. Clarkson and Ali Esmail and Mario Costa Sousa},
  journal  = {{SPE} Reservoir Evaluation & Engineering},
  title    = {Optimizing the {Huff ‘n’ Puff} gas injection performance in shale reservoirs considering the uncertainty: {A} {Duvernay Shale} example},
  year     = {2020},
  pages    = {1--19},
  doi      = {10.2118/195438-PA},
  file     = {:Hamdi2020 - Optimizing the Huff ‘n’ Puff Gas Injection Performance in Shale Reservoirs Considering the Uncertainty_ a Duvernay Shale Example.pdf:PDF},
  keywords = {used Kriging/Sensitivity in UQLab, chemical engineering},
  url      = {https://doi.org/10.2118/195438-PA},
}

@PhdThesis{Andrew2020a,
  author   = {Darren L. Andrew},
  school   = {The University of Texas at San Antonio},
  title    = {Characterization of residual stress fields from cold expansion using spatial statistics and incorporation into multi-point fracture mechanics analyses},
  year     = {2020},
  address  = {San Antonio, Texas},
  type     = {phdthesis},
  abstract = {Residual stress fields from cold expansion have been widely used to extend the fatigue life of aircraft structures. However, the spatial statistical character of these residual stress fields has not been established and has not been incorporated in current analysis methods. The objective of this study was to establish a spatial statistical method to quantify the residual stress field around a cold expanded hole and validate the method using experimental fatigue test data. A framework called the Spatial Analysis of Residual Stress (SpARS) was developed utilizing spatial correlation, response surface modeling techniques, and statistical resampling methods to characterize the residual stress field. Our results showed that tolerance bounds on residual stress can be quantified using this method and demonstrated that the use of the residual stress tolerance bounds provides a range of estimations of fatigue crack growth life. Comparison with fatigue test data demonstrated that the less compressive 95% upper bound from the mean of the residual stress field provides reasonable estimations of the fatigue life. These results are consistent with the results of the recent round robin fatigue analyses associated with the experimental data. The use of SpARS to incorporate a statistically representative residual stress field in fatigue crack growth analyses will be useful to provide guidance to aircraft structural management and allow for extending inspection intervals for fatigue and fracture critical structure.},
  url      = {https://search.proquest.com/docview/2444855805?pq-origsite=gscholar&fromopenview=true},
}

@Article{Makrygiorgos2020,
  author   = {Georgios Makrygiorgos and Giovanni Maria Maggioni and Ali Mesbaha},
  journal  = {Computers {\&} Chemical Engineering},
  title    = {Surrogate modeling for fast uncertainty quantification: Application to {2D} population balance models},
  year     = {2020},
  volume   = {138},
  abstract = {Surrogate modeling is a useful tool for enabling uncertainty quantification (UQ) tasks that require many expensive model evaluations, as it replaces expensive high-fidelity models with cheap-to-evaluate surrogates. This paper investigates sparse polynomial chaos and Kriging methods for surrogate modeling of first-principles models with probabilistic uncertainty in parameters and initial conditions. The surrogate modeling methods are demonstrated on a 2-dimensional population balance (2D-PB) model for batch cooling crystallization of ibuprofen with 20 uncertain parameters. Our analysis indicates that not only sparse polynomial chaos expansions are powerful for probabilistic UQ, but also the approximation accuracy of Kriging surrogate models can be significantly improved when polynomial chaos expansions are used to describe their trend. A basis-adaptive least-angle-regression strategy is shown to be particularly useful for inducing sparsity in polynomial chaos expansions, allowing for dealing with problems with a relatively large number of uncertain inputs. The utility of sparse polynomial chaos- and Kriging-based surrogate models is illustrated for various forward and inverse UQ problems, including global sensitivity analysis as well as Bayesian and maximum a posteriori parameter estimation of the 2D-PB model, where massive savings in computational cost (up to 30,000-fold) are observed.},
  comment  = {used PCE in UQLab.},
  doi      = {10.1016/j.compchemeng.2020.106814},
  file     = {:Makrygiorgos2020 - Surrogate Modeling for Fast Uncertainty Quantification_ Application to 2D Population Balance Models.pdf:PDF},
  keywords = {used UQLab, chemical engineering},
  url      = {https://doi.org/10.1016/j.compchemeng.2020.106814},
}

@Article{Ramsay2020,
  author   = {Travis Ramsay},
  journal  = {Society of Petroleum ({SPE}) Journal},
  title    = {Uncertainty quantification of an explicitly coupled multiphysics simulation of in-situ pyrolysis by radio frequency heating in oil shale},
  year     = {2020},
  pages    = {1443--1461},
  volume   = {25},
  abstract = {In‐situ pyrolysis provides an enhanced oil recovery (EOR) technique for exploiting oil and gas from oil shale by converting in‐place solid kerogen into liquid oil and gas. Radio‐frequency (RF) heating of the in‐place oil shale has previously been proposed as a method by which the electromagnetic energy gets converted to thermal energy, thereby heating in-situ kerogen so that it converts to oil and gas. In order to numerically model the RF heating of the in‐situ oil shale, a novel explicitly coupled thermal, phase field, mechanical, and electromagnetic (TPME) framework is devised using the finite element method in a 2D domain. Contemporaneous efforts in the commercial development of oil shale by in‐situ pyrolysis have largely focused on pilot methodologies intended to validate specific corporate or esoteric EOR strategies. This work focuses on addressing efficient epistemic uncertainty quantification (UQ) of select thermal, oil shale distribution, electromagnetic, and mechanical characteristics of oil shale in the RF heating process, comparing a spectral methodology to a Monte Carlo (MC) simulation for validation. Attempts were made to parameterize the stochastic simulation models using the characteristic properties of Green River oil shale. The geologic environment being investigated is devised as a kerogen‐poor under‐ and overburden separated by a layer of heterogeneous yet kerogen‐rich oil shale in a target formation. The objective of this work is the quantification of plausible oil shale conversion using TPME simulation under parametric uncertainty; this, while considering a referenced conversion timeline of 1.0 × 107 seconds. Nonintrusive polynomial chaos (NIPC) and MC simulation were used to evaluate complex stochastically driven TPME simulations of RF heating. The least angle regression (LAR) method was specifically used to determine a sparse set of polynomial chaos coefficients leading to the determination of summary statistics that describe the TPME results. Given the existing broad use of MC simulation methods for UQ in the oil and gas industry, the combined LAR and NIPC is suggested to provide a distinguishable performance improvement to UQ compared to MC methods.},
  comment  = {used PCE in UQLab},
  doi      = {10.2118/200476-PA},
  file     = {:Ramsay2020 - Uncertainty Quantification of an Explicitly Coupled Multiphysics Simulation of in Situ Pyrolysis by Radio Frequency Heating in Oil Shale.pdf:PDF},
  keywords = {monitoring and remote sensing},
  url      = {https://doi.org/10.2118/200476-PA},
}

@Article{Wang2020e,
  author   = {Ziqi Wang and Marco Broccardo},
  journal  = {Structural Safety},
  title    = {A novel active learning-based {Gaussian} process metamodelling strategy for estimating the full probability distribution in forward {UQ} analysis},
  year     = {2020},
  volume   = {84},
  abstract = {This paper proposes an active learning-based Gaussian process (AL-GP) metamodelling method to estimate the cumulative as well as complementary cumulative distribution function (CDF/CCDF) for forward uncertainty quantification (UQ) problems. Within the field of UQ, previous studies focused on developing AL-GP approaches for reliability (rare event probability) analysis of expensive black-box solvers. A naive iteration of these algorithms with respect to different CDF/CCDF threshold values would yield a discretized CDF/CCDF. However, this approach inevitably leads to a trade off between accuracy and computational efficiency since both depend (in opposite way) on the selected discretization. In this study, a specialized error measure and a learning function are developed such that the resulting AL-GP method is able to efficiently estimate the CDF/CCDF for a specified range of interest without an explicit dependency on discretization. Particularly, the proposed AL-GP method is able to simultaneously provide accurate CDF and CCDF estimation in their median-low probability regions. Three numerical examples are introduced to test and verify the proposed method.},
  comment  = {used Kriging in UQLab},
  doi      = {j.strusafe.2020.101937},
  file     = {:Wang2020e - A Novel Active Learning Based Gaussian Process Metamodelling Strategy for Estimating the Full Probability Distribution in Forward UQ Analysis.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.strusafe.2020.101937},
}

@Article{Koker2020,
  author   = {N. De Koker and C. Viljoen and R. Lenner and S. W.Jacobsz},
  journal  = {Structural Safety},
  title    = {Updating structural reliability efficiently using load measurement},
  year     = {2020},
  volume   = {84},
  abstract = {The observation of an existing structure supporting a particular maximal load provides a direct constraint on the possible range of values its resistance capacity may take. The implied update of structural reliability allows monitoring and maintenance planning to be done from a risk optimal perspective. Existing proof load-based reliability updating techniques require multiple numerical computations which are often too cumbersome for routine use. By building on the assumptions of the first order reliability method, this study develops and validates a first order reliability updating approach which is computationally efficient. The resulting formulation is shown to be applicable to reliability problems tractably considered using the first order reliability method. This method is illustrated for two example structures: a reinforced concrete beam forming part of a highway bridge to which traffic loading data is applied, and a granular embankment forming a seawall on a shoreline mining operation for which the phreatic surface level is monitored.},
  comment  = {used Reliability module in UQLab},
  doi      = {10.1016/j.strusafe.2020.101939},
  file     = {:Koker2020 - Updating Structural Reliability Efficiently Using Load Measurement.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1016/j.strusafe.2020.101939},
}

@Article{Pellizzer2020,
  author   = {Giovanni Pais Pellizzer and Henrique Machado Kroetz and Edson Denner Leonel and André Teófilo Beck},
  journal  = {Latin American Journal of Solids and Structures},
  title    = {Time-dependent reliability of reinforced concrete considering chloride penetration via boundary element method},
  year     = {2020},
  number   = {8},
  volume   = {17},
  abstract = {Strength degradation of structural materials is an inevitable process, due to deleterious actions such as corrosion and fatigue. These phenomena are also typically random, with degradation rates and starting time of degradation process largely uncertain. In reinforced concrete structures, corrosion of reinforcing bars caused by chloride ions is one of the main pathological manifestations. Past studies on the time-variant reliability of reinforced concrete structures subject to corrosion have relied on simplified analytical models for estimating the depassivation time. This study contributes with an accurate modelling of chloride diffusion through concrete using the boundary element method, which is employed for the first time within a timevariant reliability framework. Cumulative failure probabilities are evaluated in time by considering random depassivation times, random corrosion evolution, and random load processes. The time-variant reliability problem is solved using Monte Carlo simulation. An application example is presented, demonstrating the capabilities of the proposed framework.},
  comment  = {used UQLab to discretize random field},
  doi      = {10.1590/1679-78255885},
  file     = {:Pellizzer2020 - Time Dependent Reliability of Reinforced Concrete ConsideringChloride Penetration Via Boundary Element Method.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1590/1679-78255885},
}

@Article{Zhang2021,
  author   = {Chi Zhang and Zeyu Wang and Abdollah Shafieezadeh},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Error quantification and control for adaptive {Kriging}-based reliability updating with equality information},
  year     = {2021},
  volume   = {207},
  abstract = {New information obtained through measurements provide an opportunity to update estimates of a system's reliability. For equality type information, reliability updating is a daunting task. The current state-of-the-art method, reliability updating with equality information using adaptive Kriging (RUAK), integrates an adaptive Kriging process with a transformation of equality information into inequality information. The stopping criterion for training the Kriging model relies on the estimated error for prior failure probability, thus leaving the potential for the true error in posterior failure probability to exceed acceptable thresholds. This study presents an approach to estimate the maximum error in posterior failure probability for a given confidence level . Moreover, a two-phase approach is proposed for active learning and adaptive training of Kriging models in reliability updating problems. The new stopping criterion based on the maximum error of posterior failure probability ensures the accuracy of Kriging and thus the reliability estimates, while the two-phase scheme avoids unnecessary training hence improving the efficiency of reliability updating. Four numerical examples are considered to investigate the performance of the proposed approach. It is demonstrated that this method offers the ability to set and meet target accuracies for reliability updating, which is critical for applications where the consequences of decisions are significant.},
  comment  = {used Kriging in UQLab},
  doi      = {j.ress.2020.107323},
  file     = {:Zhang2021 - Error Quantification and Control for Adaptive Kriging Based Reliability Updating with Equality Information.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.ress.2020.107323},
}

@PhdThesis{Blomfors2020,
  author   = {Mattias Blomfors},
  school   = {Chalmers University of Technology},
  title    = {Assessment of concrete structures including corrosion and cracks},
  year     = {2020},
  address  = {Gothenburg, Sweden},
  type     = {phdthesis},
  abstract = {Reinforced concrete (RC) structures constitute a major proportion of the built environment and society relies continuously on their service. Many of these structures were built in the era following the Second World War and are thus approaching the end of their intended service life. The likelihood of deterioration increases with time and so damage caused by, say, corrosion is not uncommon. Also, increased demands are often laid on the load-carrying capacity of existing bridges, aimed at increasing utilisation of the road network by allowing heavier vehicles. Simply dismantling and re-constructing all bridges at the end of their designed service life, or taking needless strengthening measures, is unsustainable. Rather, improved methods of assessing the capacity of existing infrastructure are needed.

The current work has aimed to develop improved, reliable assessment methods. Its focus areas were structures with reinforcement corrosion and structures with cracks from previous loading. Both simplified and advanced methods of evaluating anchorage capacity were developed for concrete structures with corroded reinforcement. The simplified method modifies the bond stress-slip relationship and is calibrated against a large database of bond tests, with the safety margin ensured by deriving partial safety factors. The advanced method is based on finite element (FE) analysis, with tensile material properties altered for elements positioned at the splitting cracks along the reinforcement. The latter method was also investigated for RC without corrosion damage but with cracks from previous loading. Design results from advanced nonlinear FE analyses (meaning results with a proper safety margin) are obtained by applying a “safety format”. The current work investigated whether safety formats available in fib Model Code 2010 also ensured reliable design capacities for structures with somewhat complicated load application and geometry; in this case, a concrete frame subjected to vertical and horizontal loads.

The results indicate that the anchorage capacity may be reasonably well estimated by using the simplified method. The proposed partial safety factors also provided sufficient safety margin. Furthermore, in the advanced anchorage assessment, the capacity could be estimated solely from weakened tensile properties located at the position of the splitting cracks and without input concerning the corrosion level. Moreover, by including cracks from previous loading in advanced modelling, improved predictions of the failure mode, ultimate capacity and ductility were demonstrated. Lastly, in the investigation of safety formats for nonlinear FE analysis, the method of estimating a coefficient of variance of resistance (ECOV), did not reach the intended safety level. However, the global resistance factor method (GRF) and partial factor method (PSF) did. This work has the potential to improve both simplified and advanced assessment methods, providing more sustainable infrastructure management in the future.},
  comment  = {used FORM in UQLab},
  file     = {:Blomfors2020 - Assessment of Concrete Structures Including Corrosion and Cracks.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://research.chalmers.se/en/publication/518615},
}

@Article{Xu2020,
  author   = {Can Xu and Zhao Liu and Ping Zhu and Mushi Li},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Sensitivity-based adaptive sequential sampling for metamodel uncertainty reduction in multilevel systems},
  year     = {2020},
  pages    = {1473--1496},
  volume   = {62},
  abstract = {Decomposition-based technique is often used in the analysis and design of complex engineering systems for reducing the computational complexity by studying the subsystems decomposed from multilevel systems. Metamodels, as a replacement of original simulation models, can further alleviate the computational burden. However, discrepancy between the simulation models and metamodels, which is defined as metamodel uncertainty, may be introduced in the analysis process of multilevel systems owing to the lack of data. The metamodel uncertainties of sub-models will be further amplified because of the hierarchical uncertainty propagation and interaction between uncertainties, which will have a great impact on the system results. An adaptive sequential sampling strategy based on sensitivity is proposed in this paper so as to improve the prediction accuracy of system response. In this strategy, polynomial-chaos expansion is used to realize the forward propagation of metamodel uncertainty quantified by the Kriging model. The forward propagation is combined with optimization based on maximum variance criterion for searching the input locations that results in the largest variance of system response. Then, the indices of subsystems are obtained to make decisions about which subsystem needs extra samples by combining Karhunen-Loeve expansion and sensitivity analysis. The effectiveness of the proposed sequential sampling strategy method is verified by two mathematical examples and a multiscale composite material.},
  comment  = {used PCE in UQLab},
  doi      = {10.1007/s00158-020-02673-6},
  file     = {:Xu2020 - Sensitivity Based Adaptive Sequential Sampling for Metamodel Uncertainty Reduction in Multilevel Systems.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1007/s00158-020-02673-6},
}

@Article{Melillo2020,
  author   = {Nicola Melillo and Adam S. Darwich},
  journal  = {arXiv},
  title    = {A latent variable approach to account for correlated inputs in global sensitivity analysis with cases from pharmacological systems modelling},
  year     = {2020},
  abstract = {In pharmaceutical research and development decision-making related to drug candidate selection, efficacy and safety is commonly supported through modelling and simulation (M\&S). Among others, physiologically-based pharmacokinetic models are used to describe drug absorption, distribution and metabolism in human. Global sensitivity analysis (GSA) is gaining interest in the pharmacological M\&S community as an important element for quality assessment of model-based inference. Physiological models often present inter-correlated parameters. The inclusion of correlated factors in GSA and the sensitivity indices interpretation has proven an issue for these models. Here we devise and evaluate a latent variable approach for dealing with correlated factors in GSA. This approach describes the correlation between two model inputs through the causal relationship of three independent factors: the latent variable and the unique variances of the two correlated parameters. Then, GSA is performed with the classical variance-based method. We applied the latent variable approach to a set of algebraic models and a case from physiologically-based pharmacokinetics. Then, we compared our approach to Sobol's GSA assuming no correlations, Sobol's GSA with groups and the Kucherenko approach. The relative ease of implementation and interpretation makes this a simple approach for carrying out GSA for models with correlated input factors.},
  comment  = {used UQLab for GSA},
  file     = {:- A Latent Variable Approach to Account for Correlated Inputs in Global Sensitivity Analysis with Cases from Pharmacological Systems Modelling.pdf:PDF},
  keywords = {biomedical science},
  url      = {https://arxiv.org/abs/2012.02500},
}

@Article{Phan2020,
  author   = {Hoang Nam Phan and Fabrizio Paolacci and Rocco Di Filippo and Oreste S. Bursi},
  journal  = {Bulletin of Earthquake Engineering},
  title    = {Seismic vulnerability of above-ground storage tanks with unanchored support conditions for {Na-tech} risks based on {Gaussian} process regression},
  year     = {2020},
  pages    = {6883--6906},
  volume   = {18},
  abstract = {This paper aims to investigate the seismic vulnerability of an existing unanchored steel storage tank ideally installed in a refinery in Sicily (Italy), along the lines of performance-based earthquake engineering. Tank performance is estimated by means of component-level fragility curves for specific limit states. The assessment is based on a framework that relies on a three-dimensional finite element (3D FE) model and a low-fidelity demand model based on Gaussian process regression, which allows for cheaper simulations. Moreover, to approximate the system response corresponding to the random variation of both peak ground acceleration and liquid filling level, a second-order design of experiments method is adopted. Hence, a parametric investigation is conducted on a specific existing unanchored steel storage tank. The relevant 3D FE model is validated with an experimental campaign carried out on a shaking table test. Special attention is paid to the base uplift due to significant inelastic deformations that occur at the baseplate close to the welded baseplate-to-wall connection, offering extensive information on both capacity and demand. As a result, the tank performance is estimated by means of component-level fragility curves for the aforementioned limit state which are derived through Monte Carlo simulations. The flexibility of the proposed framework allows fragility curves to be derived considering both deterministic and random filling levels. The comparison of the seismic vulnerability of the tank obtained with probabilistic and deterministic mechanical models demonstrates the conservatism of the latter. The same trend is also exhibited in terms of risk assessment.},
  comment  = {used Kriging in UQLab},
  doi      = {10.1007/s10518-020-00960-7},
  file     = {:Phan2020 - Seismic Vulnerability of above Ground Storage Tanks with Unanchored Support Conditions for Na Tech Risks Based on Gaussian Process Regression.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1007/s10518-020-00960-7},
}

@Article{Fahrni2021,
  author   = {Reto Fahrni and Gianluca De Sanctis and Andrea Frangi},
  journal  = {Structural Safety},
  title    = {Comparison of reliability- and design-based code calibrations},
  year     = {2021},
  volume   = {88},
  abstract = {Some of the early probabilistic code calibrations minimized an error term defined as difference between the optimal or target design and the design resulting with the trial partial factors and the code format. In contrast to these design-based calibrations, in recent years, the error was defined in terms of a reliability deviation. Aiming at minimizing societal costs, reliability-based calibrations probably provide more accurate results than design-based code calibrations, however at the cost of a significantly lower computational efficiency.

The long duration of reliability-based code calibrations impedes repeated code calibrations, which are needed in the development of a code format or for a more profound understanding of it. This paper compares the reliability-based with the design-based code calibration procedure theoretically and by using an example. The results showed that the design-based code calibration is efficient for the development of a code format. However, the differences between both calibration procedures regarding the calibrated partial factors are significant. As the reliability-based calibration has a more solid theoretical background, it is preferred for the final calibration of the code format.},
  comment  = {used Reliability module of UQLab},
  doi      = {10.1016/j.strusafe.2020.102005},
  file     = {:Fahrni2021 - Comparison of Reliability and Design Based Code Calibrations.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1016/j.strusafe.2020.102005},
}

@Article{Zhang2021a,
  author   = {Yu Zhang and Jun Xua},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Efficient reliability analysis with a {CDA}-based dimension-reduction model and polynomial chaos expansion},
  year     = {2021},
  volume   = {373},
  abstract = {Polynomial chaos expansion (PCE) is a versatile tool for building a meta-model in various engineering fields. Unfortunately, it is largely affected by the curse of dimensionality and its application for reliability analysis is usually hindered unless high truncated degrees are used. To alleviate this problem, this paper presents a new method based on contribution-degree analysis (CDA), an exact dimension-reduction model (DRM) and polynomial chaos expansion for efficient reliability analysis. First, the original performance function is decomposed as a summation of several component functions including one lower-dimensional interacting component function and several one-dimensional non-interacting component functions via a CDA-based DRM. Then, the full PCE or sparse PCE is employed to reconstruct the lower-dimensional interacting component function instead of the original complicated multi-dimensional performance function, whereas one-dimensional full PCEs are utilized for approximating the non-interacting component functions. In this way, we avoid building a PCE meta-model for the original multivariate performance function, thus the number of unknown coefficients is significantly reduced and the computational burden for reliability analysis is eased accordingly. Pertinent examples including both analytical performance functions and finite-element models are investigated, which demonstrates that the proposed method achieves a good trade-off of efficiency and accuracy for reliability analysis.},
  comment  = {used PCE (LARS, OLS) of UQLab},
  doi      = {10.1016/j.cma.2020.113467},
  file     = {:- Efficient Reliability Analysis with a CDA Based Dimension Reduction Model and Polynomial Chaos Expansion.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.cma.2020.113467},
}

@InProceedings{Filippo2019,
  author       = {Rocco di Filippo and Giuseppe Abbiati and Osman Sayginer and Patrick Covi and Oreste S. Bursi and Fabrizio Paolacci},
  booktitle    = {ASME 2019 Pressure Vessels \& Piping Conference},
  title        = {Numerical surrogate model of a coupled tank-piping system for seismic fragility analysis with synthetic ground motions},
  year         = {2019},
  address      = {San Antonio, Texas},
  organization = {ASME},
  abstract     = {Seismic risk evaluation of coupled systems of industrial plants often needs the implementation of complex finite element models to consider their multicomponent nature. These models typically rely on significant computational resources. Moreover, the relationships between seismic action, system response and relevant damage levels are often characterized by a high level of nonlinearity, thus requiring a solid background of experimental data. Furthermore, fragility analyses depend on the adoption of a significant number of seismic waveforms generally not available when the analysis is site-specific. To propose a methodology able to manage these issues, we present a possible approach for a seismic reliability analysis of a coupled tank-piping system. The novelty of this approach lies in the adoption of artificial accelerograms, FE models and experimental hybrid simulations to evaluate a surrogate meta-model of our system. First, to obtain the necessary input for a stochastic ground motion model able to generate synthetic ground motions, a disaggregation analysis of the seismic hazard is performed. Hereafter, we reduce the space of parameters of the stochastic ground motion model by means of a global sensitivity analysis upon the seismic response of our system. Hence, we generate a large set of synthetic ground motions and select, among them, a few signals for experimental hybrid simulations. In detail, the hybrid simulator is composed by a numerical substructure to predict the sliding response of a steel tank, and a physical substructure made of a realistic piping network. Furthermore, we use these experimental results to calibrate a refined ANSYS FEM. More precisely, we focus on tensile hoop strains in elbow pipes as a leading cause for leakage, monitoring them with strain gauges. Thus, we present the procedure to evaluate a numerical Kriging meta-model of the coupled system based on both experimental and finite element model results. This model will be adopted in a future development to carry out a seismic fragility analysis.},
  comment      = {used PCE and Sobol analysis of UQLab},
  doi          = {10.1115/PVP2019-93685},
  eventdate    = {2019-07-14/2019-07-19},
  file         = {:Filippo2019 - Numerical Surrogate Model of a Coupled Tank Piping System for Seismic Fragility Analysis with Synthetic Ground Motions.pdf:PDF},
  keywords     = {civil engineering},
  url          = {https://doi.org/10.1115/PVP2019-93685},
}

@InProceedings{Ye2019,
  author    = {Yu Ye and Zhang Lizhong and Guo Jingwei and Liu Siyao},
  booktitle = {Proceedings of the 2019 International Conference on Precision Machining, Non-Traditional Machining and Intelligent Manufacturing (PNTIM 2019)},
  title     = {Sensitivity analysis of island micro-grid based on probabilistic load flow model},
  year      = {2019},
  address   = {Xi' an, China},
  abstract  = {In the global energy crisis and environmental pollution problems, vigorous development of clean energy is a common initiative of all countries in the world. As an important means of new energy consumption and reliability improvement of power supply, micro-grid has a very broad development prospect. In order to overcome the problem of low efficiency of traditional MCS calculation, an micro-grid GSA method based on sparse polynomial chaotic expansion in the paper is proposed, which is used to accurately and quickly identify key input random variables that affect the operating state of the system. Furthermore, the accuracy and efficiency of the proposed method are verified by the example analysis of the island micro-grid.},
  doi       = {10.2991/pntim-19.2019.92},
  eventdate = {2019-11-22/2019-11-24},
  file      = {:Ye2019 - Sensitivity Analysis of Island Micro Grid Based on Probabilistic Load Flow Model.pdf:PDF},
  keywords  = {electrical engineering},
  url       = {https://dx.doi.org/10.2991/pntim-19.2019.92},
}

@Article{Chen2020,
  author   = {M. Chen and T. Guo and C. Chen and W. Xu},
  journal  = {Experimental Techniques},
  title    = {Data-driven arbitrary polynomial chaos expansion on uncertainty quantification for real-time hybrid simulation under stochastic ground motions},
  year     = {2020},
  pages    = {751--762},
  volume   = {44},
  abstract = {Uncertainties in real-time hybrid simulation include structural parameters and ground motion. Uncertain parameters often do not follow common distribution types. Data-driven arbitrary polynomial chaos constructs optimal orthogonal polynomial basis based on the sample data without distribution assumption. In this study, the data-driven polynomial chaos is compared with other generalized polynomial chaos from the aspects of the rate of error convergence when applied for uncertainty quantification of real-time hybrid simulation. Moreover, uncertainties of ground motion are considered in the RTHS problem to represent the scenarios with more complex input variables. Different statistical indicators are utilized to evaluate the accuracy of the alternative model in comparison with the Monte Carlo simulation results. Compared with generalized polynomial chaos, the data-driven arbitrary polynomial chaos presents potential for uncertainty quantification of real-time hybrid simulation with approximate or better accuracy. Actuator delay in RTHS could change the sensitivity of model output to the random variables.},
  comment  = {used PCE of UQLab (Stieltjes'  procedure for generalized PCE)},
  doi      = {10.1007/s40799-020-00381-w},
  file     = {:Chen2020 - Data Driven Arbitrary Polynomial Chaos Expansion on Uncertainty Quantification for Real Time Hybrid Simulation under Stochastic Ground Motions.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1007/s40799-020-00381-w},
}

@InProceedings{Wu2020,
  author       = {Raphael Wu and Giovanni Sansavini},
  booktitle    = {2020 International Conference on Probabilistic Methods Applied to Power Systems ({PMAPS})},
  title        = {Parameter estimation for distribution grid reliability assessment},
  year         = {2020},
  address      = {Liege, Belgium},
  organization = {IEEE},
  abstract     = {Strengthening distribution grids reliability and resilience against technical and natural hazards is a costly endeavor including equipment upgrades and distributed energy resources. Therefore, using accurate data when assessing grid reliability is key to identify effective solutions. As literature parameters can be inaccurate for specific locations, tuning and validating reliability models against real-world data is key for accurate assessments. In this paper, distribution grid reliability is modelled by considering three failure mechanisms in a Monte Carlo simulation: bus and line failures within the distribution grid, blackouts of the surrounding grid, and dependent failures due to extreme events. Ten parameters governing the frequency and duration distributions of the three failure mechanisms are tuned using metaheuristic optimization. A subsequent global sensitivity analysis quantifies the importance of the estimated parameters.},
  comment      = {used PCE and Sobol analysis of UQLab},
  doi          = {10.1109/PMAPS47429.2020.9183408},
  eventdate    = {2019-08-18/2019-08-21},
  file         = {:Wu2020 - Parameter Estimation for Distribution Grid Reliability Assessment.pdf:PDF},
  keywords     = {electrical engineering},
  url          = {https://doi.org/10.1109/PMAPS47429.2020.9183408},
}

@InProceedings{Veloz2020,
  author    = {Wilson J. Veloz and Hongbo Zhang and Hao Bai and Younes Aoues and Didier Lemosse},
  booktitle = {The 5th International Symposium on Uncertainty Quantification and Stochastic Modelling (Uncertainties 2020)},
  title     = {Uncertainty propagation in wind turbine blade loads},
  year      = {2020},
  address   = {Rouen, France},
  pages     = {337--346},
  abstract  = {Minimizing the cost and enhancing the lifespan of wind turbines entails the optimization of the material distribution of wind turbine components (blades, tower, etc.) without compromising their structural safety. Wind turbines are often design using the IEC 61400-1 standard to provide an appropriate level of protection against damage from all hazards during the planned lifetime. Typically, aero-elastic simulations codes are used to determine loads and displacements time history in the wind turbine. To predict the fatigue damage limit of the wind turbine blade, it is important to quantify and model all relevant uncertainties but it requires a considered amount of simulation time, and a surrogate model can substitute this simulation, to decrease this time consuming part of the problem. In this study, Monte Carlo simulation and FAST code are used to simulate different wind conditions. Here, 10-min of effective simulations generate a time history for all forces and moments acting in 10 selected gages of the blade. Subsequently, we quantify the uncertainty of their maximum value using a Gaussian process (Kriging) and Deep Neural Network (DNN), fitting this maximum output values with their correspondent input values. For Kriging and DNN a good fitting was found for almost all output variables.},
  doi       = {10.1007/978-3-030-53669-5_24},
  eventdate = {2020-06-29/2020-07-03},
  file      = {:Veloz2020 - Uncertainty Propagation in Wind Turbine Blade Loads.pdf:PDF},
  keywords  = {mechanical engineering},
  url       = {https://doi.org/10.1007/978-3-030-53669-5_24},
}

@Article{Ebiwonjumi2020,
  author   = {Bamidele Ebiwonjumi and Chidong Kong and Peng Zhang and Alexey Cherezov and Deokjung Lee},
  journal  = {Nuclear Engineering and Technology},
  title    = {Uncertainty quantification of {PWR} spent fuel due to nuclear data and modeling parameters},
  year     = {2020},
  abstract = {Uncertainties are calculated for pressurized water reactor (PWR) spent nuclear fuel (SNF) characteristics. The deterministic code STREAM is currently being used as an SNF analysis tool to obtain isotopic inventory, radioactivity, decay heat, neutron and gamma source strengths. The SNF analysis capability of STREAM was recently validated. However, the uncertainty analysis is yet to be conducted. To estimate the uncertainty due to nuclear data, STREAM is used to perturb nuclear cross section (XS) and resonance integral (RI) libraries produced by NJOY99. The perturbation of XS and RI involves the stochastic sampling of ENDF/B-VII.1 covariance data. To estimate the uncertainty due to modeling parameters (fuel design and irradiation history), surrogate models are built based on polynomial chaos expansion (PCE) and variance-based sensitivity indices (i.e., Sobol’ indices) are employed to perform global sensitivity analysis (GSA). The calculation results indicate that uncertainty of SNF due to modeling parameters are also very important and as a result can contribute significantly to the difference of uncertainties due to nuclear data and modeling parameters. In addition, the surrogate model offers a computationally efficient approach with significantly reduced computation time, to accurately evaluate uncertainties of SNF integral characteristics.},
  comment  = {used PCE and Sobol analysis of UQLab},
  doi      = {10.1016/j.net.2020.07.012},
  file     = {:Ebiwonjumi2020 - Uncertainty Quantification of PWR Spent Fuel Due to Nuclear Data and Modeling Parameters.pdf:PDF},
  keywords = {nuclear engineering},
  url      = {https://doi.org/10.1016/j.net.2020.07.012},
}

@Article{Guo2020b,
  author   = {Xiangfeng Guo and Qiangqiang Sun and Daniel Dias and Eric Antoinet},
  journal  = {Bulletin of Engineering Geology and the Environment},
  title    = {Probabilistic assessment of an earth dam stability design using the adaptive polynomial chaos expansion},
  year     = {2020},
  pages    = {4639--4655},
  volume   = {79},
  abstract = {Assessment of the stability for an earth dam under seismic loadings is presented in a probabilistic framework. The geotechnical parameters measured at the dam site permit a realistic description of the soil variability for the dam and the superficial foundation. An active learning sparse polynomial chaos expansions (A-bSPCE) combined with Monte Carlo simulation is employed to quantify the uncertainty propagation. The proposed dam design is assessed within two seismic scenarios: the first one considers a constant pseudo-static acceleration while the second one accounts for the uncertainties in seismic loadings by modeling the pseudo-static acceleration as a random variable. The probabilistic assessments for variation of the amplitude and standard deviation of the seismic loading are performed. In addition, the effects of the correlation coefficient between the soil’s shear strength parameters and the soil spatial variability are discussed. It is found that these effects are more significant in the first scenario. The obtained results permit an evaluation of the preliminary dam design and to provide guidance on seismic safety enhancement for the final design phase. The present study also highlights the accuracy and efficiency of the employed reliability method. It is shown that the total computational time of a probabilistic analysis can be reduced from several days to only 20 min by using the A-bSPCE.},
  comment  = {used sparse PCE of UQLab.},
  doi      = {10.1007/s10064-020-01847-2},
  file     = {:Guo2020b - Probabilistic Assessment of an Earth Dam Stability Design Using the Adaptive Polynomial Chaos Expansion.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1007/s10064-020-01847-2},
}

@InProceedings{Wang2020f,
  author    = {Xiaolong Wang and Xiang Fang and Lukas Beller and Florian Holzapfel},
  booktitle = {The 30th European Safety and Reliability Conference and the 15th Probabilistic Safety Assessment and Management Conference ({ESREL 2020 PSAM 15})},
  title     = {Calibration of contributing factors for model-based predictive analysis algorithm using polynomial chaos expansion methods},
  year      = {2020},
  address   = {Venice, Italy},
  abstract  = {To improve operational flight safety, a model-based predictive analysis algorithm to quantify the occurrence probabilities of airlines' incidents or accidents has been developed at the Institute of Flight System Dynamics. An incident model based on physics is built to map the relationship between the incident metric and corresponding contributing factors, which are recorded directly or identified indirectly from the operational flight data. The incident metric distribution can be obtained via propagating uncertainties of the contributing factors through the incident model. A calibration step is necessary to ensure that the generated incident metric distribution does represent the reality before prediction. Except for the model simplification error, the deviation between simulation and reality also arises from the measurement error, identification error or distribution fitting error of contributing factors and the incident metric. Therefore, the basic strategy of calibration is to reduce those errors via solving an optimization problem and then justify if the deviation between the calibrated and recorded incident metric is acceptable for the subsequent prediction. A calibration framework using the polynomial chaos expansion method and optimization method is proposed in the paper. Meanwhile, the dependence structure of contributing factors has been considered and integrated into this framework based on Copula. This approach has been shown to be convergent and greatly improve efficiency because it is not required to run the Monte Carlo simulation again during each iteration of optimization. Finally, this calibration algorithm is implemented for a runway overrun incident model.},
  comment   = {used PCE of UQLab},
  doi       = {10.3850/978-981-14-8593-0},
  eventdate = {2020-11-01/2020-11-05},
  file      = {:Wang2020f - Calibration of Contributing Factors for Model Based Predictive Analysis Algorithm Using Polynomial Chaos Expansion Methods.pdf:PDF},
  keywords  = {computational science and engineering},
  url       = {https://doi.org/10.3850/978-981-14-8593-0},
}

@Article{Lu2020b,
  author   = {Guang Lu and Adrian Ringenbach and Andrin Caviezel and Miguel Sanchez and Marc Christen and Perry Bartelt},
  journal  = {Landslides},
  title    = {Mitigation effects of trees on rockfall hazards: does rock shape matter?},
  year     = {2020},
  abstract = {Does rock shape matter to the mitigation effects of trees on rockfall hazards? This question must be resolved in order to better quantify the protective role of mountain forests against rockfall. To probe this question, we investigate a single rock-tree interaction using non-smooth, hard-contact mechanics that allows us to consider rock shape at impact. The interaction of equant shaped rocks with cylinder-like tree stems is modeled. The equant shaped rocks are close to spherical but have a certain shape variability governed by the rock’s surface area ratio and aspect ratio. This work serves as an important follow-up study to the existing investigations from Toe et al. (Landslides 14: 1603-1614, 2017), where the effects of trees on block propagation are numerically investigated using spherical shaped rocks. The objective of our simulations is to understand how and to what extent, shape will influence energy dissipation and trajectory change. The primary results include: surface area ratio plays a more important role than aspect ratio in determining the rock’s post-impact dynamics. The primary parameters governing the rock kinematics after impact (i.e., block’s energy reduction, reflected rotational speed, and trajectory change) are impact velocity, impact eccentricity, and the tree stem diameter. The latter observation aligns well with previous findings and suggests that the shape factors, at least for nearly spherical rocks, can be integrated into the current block propagation models. However, from a statistical viewpoint, the anisotropic distribution of mass and hence the asymmetric moment of inertia of non-spherical rocks leads to stronger or weaker spin effects compared to mass- and volume-equivalent spheres. Apparently, the rotational motion of an irregular object serves as a kinetic energy reservoir leading to subsequent rock-tree impacts, and therefore significant differences in energy loss and trajectory in comparison to spherical shaped rocks. This effect must be further investigated using elongated and flattened blocks and underscores the importance of measuring rockfall rotation in experimental investigations.},
  comment  = {used PCE of UQLab},
  doi      = {10.1007/s10346-020-01418-2},
  file     = {:Lu2020b - Mitigation Effects of Trees on Rockfall Hazards_ Does Rock Shape Matter_.pdf:PDF},
  keywords = {geomechanics},
  url      = {https://doi.org/10.1007/s10346-020-01418-2},
}

@InProceedings{Bonato2020,
  author    = {Marta Bonato and Emma Chiaramello and Serena Fiocchi and Silvia Galluccia and Laura Dossi and Gabriella Tognola and Paolo Ravazzani and Marta Parazzini},
  booktitle = {2020 {IEEE} 20th Mediterranean Electrotechnical Conference ({MELECON})},
  title     = {Stochastic dosimetry applied on a low frequency near-field source scenario},
  year      = {2020},
  address   = {Palermo, Italy},
  abstract  = {The present work aims to prove the validity of the well know stochastic dosimetry method for a low frequency near-field source exposure scenario. The interest in this thematic has grown since it was discovered that house electric devices could be significant for children ELF-MF levels of exposure. More in details, it was conducted the assessment of children exposure variability caused by 10000 different positions of a hairdryer as near-field source, in order to test the goodness of the method.},
  comment   = {used PCE of UQLab.},
  doi       = {10.1109/MELECON48756.2020.9140631},
  eventdate = {2020-06-16/2020-06-18},
  file      = {:Bonato2020 - Stochastic Dosimetry Applied on a Low Frequency near Field Source Scenario.pdf:PDF},
  keywords  = {biomedical science},
  url       = {https://doi.org/10.1109/MELECON48756.2020.9140631},
}

@Article{Dang2020,
  author   = {Chao Dang and Jun Xua},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Unified reliability assessment for problems with low- to high-dimensional random inputs using the {Laplace} transform and a mixture distribution},
  year     = {2020},
  volume   = {204},
  abstract = {Efficient evaluation of the failure probability of systems with low- to high-dimensional random inputs in a unified way is still a challenging task since the methods that work in low dimensions usually become inefficient in high dimensions and vice versa. In this paper, a unified method is proposed to address this challenge. First, the Laplace transform (LT) is introduced to characterize the output variable of the limit state function (LSF). Two fifth-degree cubature formulae are employed to numerically approximate the LT when the input parameter space is low/moderate-dimensional, whereas a low-discrepancy sampling technique is adopted for high-dimensional problems. A mixture of skew normal distributions, is then developed to recover the probability distribution of the LSF from the knowledge of its LT. By matching with discrete values of the LT, the parameters of the mixture distribution are identified and the probability distribution of the LSF can be reconstructed. Five numerical examples are investigated to verify and exemplify the proposed method, where some standard reliability analysis methods are also conducted for comparison. The results indicate that the proposed method can efficiently recover the probability distribution of the LSF and estimate the failure probability for problems with low- to high-dimensional random inputs within a unified framework.},
  comment  = {used SS/AK-MCS of UQLab to compare own' s method.},
  doi      = {10.1016/j.ress.2020.107124},
  file     = {:Dang2020 - Unified Reliability Assessment for Problems with Low to High Dimensional Random Inputs Using the Laplace Transform and a Mixture Distribution.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.ress.2020.107124},
}

@Article{Xu2021,
  author   = {Can Xu and Ping Zhu and Zhao Liu and Wei Tao},
  journal  = {Journal of Mechanical Design},
  title    = {Mapping-based hierarchical sensitivity analysis for multilevel systems with multidimensional correlations},
  year     = {2021},
  number   = {1},
  volume   = {143},
  abstract = {Hierarchical sensitivity analysis (HSA) of multilevel systems is to assess the effect of system’s input uncertainties on the variations of system’s performance through integrating the sensitivity indices of subsystems. However, it is difficult to deal with the engineering systems with complicated correlations among various variables across levels by using the existing hierarchical sensitivity analysis method based on variance decomposition. To overcome this limitation, a mapping-based hierarchical sensitivity analysis method is proposed to obtain sensitivity indices of multilevel systems with multidimensional correlations. For subsystems with dependent variables, a mapping-based sensitivity analysis, consisting of vine copula theory, Rosenblatt transformation, and polynomial chaos expansion (PCE) technique, is provided for obtaining the marginal sensitivity indices. The marginal sensitivity indices can allow us to distinguish between the mutual depend contribution and the independent contribution of an input to the response variance. Then, extended aggregation formulations for local variables and shared variables are developed to integrate the sensitivity indices of subsystems at each level so as to estimate the global effect of inputs on the response. Finally, this paper presents a computational framework that combines related techniques step by step. The effectiveness of the proposed mapping-based hierarchical sensitivity analysis (MHSA) method is verified by a mathematical example and a multiscale composite material.},
  comment  = {used PCE of UQLab.},
  doi      = {10.1115/1.4047689},
  file     = {:Xu2021 - Mapping Based Hierarchical Sensitivity Analysis for Multilevel Systems with Multidimensional Correlations.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1115/1.4047689},
}

@Article{Deshmukh2020,
  author   = {Sahil Deshmukh and Paul Lagouanelle and Lionel Pichon},
  journal  = {{ACES} Journal},
  title    = {Assessment of human exposure in case of wireless power transfer for automotive applications using stochastic models},
  year     = {2020},
  number   = {3},
  pages    = {338--343},
  volume   = {33},
  abstract = {In this paper, different non-intrusive stochastic approaches are compared in view of human exposure assessment from an inductive power transfer system at 85 kHz, dedicated to automotive applications. The stochastic approaches are combined with a 3D finite element method to build adequate meta-models based on Kriging and Polynomial Chaos Expansion. These models are used to consider the uncertainty and variability of several parameters defining the electromagnetic problem. Such fast predictions of uncertainties may help to improve the design of shields for inductive power transfer systems considering health and safety standards.},
  comment  = {used PCE of UQLab.},
  file     = {:Deshmukh2020 - Assessment of Human Exposure in Case of Wireless Power Transfer for Automotive Applications Using Stochastic Models.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://hal.archives-ouvertes.fr/hal-02568258},
}

@PhdThesis{Lin2020,
  author   = {Nan Lin},
  school   = {Technischen Universität Carolo-Wilhelmina zu Braunschweig},
  title    = {Multiphysics modelling of electrochemical energy storage devices: lithium-ion batteries and supercapacitors},
  year     = {2020},
  address  = {Braunschweig, Germany},
  type     = {phdthesis},
  abstract = {As one of the current and next-generation energy storage devices, lithium-ion batteries and supercapacitors have potential for improved design for lower cost, higher safety and efficiency, compared with recent status. The major development of lithium-ion batteries and supercapacitors is currently driven by the experimental approaches, which cost much time and lack deep insight into interacted multiphysics processes. Multiphysics modelling approach can overcome such drawbacks, and thus be used for investigation and discovery of new materials and device design. For lithium-ion batteries, a novel 3D multiphysics model was developed to discover such heterogeneously physical interactions in a 12Ah pouch cell based on operation principles. The model analyzes the internal interactions of local thermal effects, electrochemical and electric performance on electro-active layers in the pouch cell. It illustrates that the poor heat dissipation and high heat generation causes strong thermal heterogeneity at large currents, which finally results in intense local interactions of electric and electrochemical performance. This 3D multiphysics model is applied into a global sensitivity analysis of the same cell for the utilizable discharge capacity and local maximum temperature at 1C. The sensitivity analysis is efficiently executed for 46 parameters in this 3D model. Consequently, it reveals that the capacity and the local maximum temperature are both most sensitive to geometric parameters of electrodes and their pore structures. Following these results, a large-format prismatic battery was optimized with the detailed 3D geometries and components for upgrading safety prevention of thermal-runaway and maximizing the capacity. For supercapacitors, a novel multiphysics pseudo-2D model is developed to interpret complex faradaic reaction mechanisms and investigate effects of different pore structures on capacitive performance. Symmetric cells of cuprous oxide and hierarchical porous carbon are parameterized respectively, with good agreement between experimental data and simulated results at the room temperature. The simulation results imply that the effective ion size and micropore volume can strongly affect the double layer capacitance due to the change of ion transport in double layers and pores. This multiphysics model is the first to interpret the capacitive behavior in hierarchical pore structures based on attractive force theory and modified Donnan concepts, to my best knowledge.},
  comment  = {used PCE and Sobol' analysis of UQLab.},
  doi      = {10.24355/dbbs.084-202005181341-0},
  file     = {:Lin2020 - Multiphysics Modelling of Electrochemical Energy Storage Devices_ Lithium Ion Batteries and Supercapacitors.pdf:PDF},
  keywords = {chemical engineering},
  url      = {https://doi.org/10.24355/dbbs.084-202005181341-0},
}

@Article{Paixao2020a,
  author   = {Jessé Paixão and Samuel da Silva and Eloi Figueiredo and Lucian Radu and Gyuhae Park},
  journal  = {Journal of Vibration and Control},
  title    = {Delamination area quantification in composite structures using {Gaussian} process regression and auto-regressive models},
  year     = {2020},
  abstract = {After detecting initial delamination damage in a hotspot region of a composite structure monitored through a data-driven approach, the user needs to decide if there is an imminent structural failure or if the system can be kept in operation under monitoring to track the damage progression and its impact on the structural safety condition. Therefore, this study proposes delamination area quantification by stochastically interpolating global damage indices based on Gaussian process regression and taking into account uncertainty. Auto-regressive models are applied to extract damage-sensitive features from Lamb wave signals, and the Mahalanobis squared distance is used to compute damage indices. Two sets of laboratory tests are used to demonstrate the effectiveness of this methodology—one in carbon–epoxy laminate with simulated damage under temperature changes to show the general steps of the procedure, and a second test involving a set of carbon fiber–reinforced polymer coupons with actual delamination caused by repeated fatigue loads. Various levels of progression damage, measured by the covered area of delamination, are monitored using piezoelectric lead zirconate titanate patches bonded to the structural surfaces of these setups. The Gaussian process regression proved to be capable of accommodating the uncertainties to relate the damage indices versus the damaged area. The results exhibit a smooth and adequate prediction of the damaged area for both simulated damage and actual delamination.},
  comment  = {used Kriging of UQLab.},
  doi      = {10.1177/1077546320966183},
  file     = {:Paixao2020a - Delamination Area Quantification in Composite Structures Using Gaussian Process Regression and Auto Regressive Models.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://doi.org/10.1177/1077546320966183},
}

@Article{Okpokparoro2021,
  author   = {Salem Okpokparoro and Srinivas Sriramula},
  journal  = {Renewable Energy},
  title    = {Uncertainty modeling in reliability analysis of floating wind turbine support structures},
  year     = {2021},
  number   = {1},
  pages    = {88--108},
  volume   = {165},
  abstract = {Accurate structural reliability assessment of floating wind turbine (FWT) systems is a desideratum for achieving consistent optimal reliability levels and cost-effective design. Such reliability assessment should consider relevant system uncertainties—a nontrivial task. Formulation of the reliability problem requires structural demand in form of load and load effect. Support structure loads are predicted with time-domain dynamic simulations. This represents a challenge when thousands of such simulations are required to capture the uncertainty associated with design variables. Finite element analysis (FEA) is commonly used to evaluate load effects such as stresses, strains etc. This can be computationally expensive if not prohibitive when such evaluation is carried out for every time step. To tackle these issues, a framework for expeditious load effect computation and robust reliability analysis of FWT support structures under ultimate limit state design is presented. The framework employs linear elastic FEA and Kriging surrogate models. The adequacy of Kriging as applied in this study is investigated using high fidelity simulation data. The results highlight the importance of incorporating the Kriging uncertainty in the formulation of the limit state function. With the framework presented, FWT support structures can be designed at consistent reliability levels leading to cost reductions.},
  comment  = {used Kriging of UQLab},
  doi      = {10.1016/j.renene.2020.10.068},
  file     = {:Okpokparoro2021 - Uncertainty Modeling in Reliability Analysis of Floating Wind Turbine Support Structures.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://doi.org/10.1016/j.renene.2020.10.068},
}

@InProceedings{Yang2020,
  author    = {Jianping Yang and Shu Liu and Zhuoxin Lu and Zheng Yan and Xiaoyuan Xu},
  booktitle = {2020 12th {IEEE} {PES} Asia-Pacific Power and Energy Engineering Conference ({APPEEC})},
  title     = {{Source-Grid-Load} combined security assessment of {PV}-penetrated distribution network},
  year      = {2020},
  address   = {Nanjing, China},
  abstract  = {With the increasing penetration of renewable energy, distribution network operation is threatened by uncertainties. The proposed security assessment of distribution network combining system operation risk with uncertain PV power and network topology robustness. It helps evaluate system operation status objectively and provide suggestions for renewable energy planning. First, node voltage deviation and branch overload rate are defined as system operation risks. The sparse polynomial chaos expansion (SPCE)-based stress testing is proposed to evaluate the operation risk including voltage deviation and overload. Second, the definition of topology robustness is introduced. Then, the source-grid-load combined assessment method is applied in modified IEEE 33-bus systems with different PV penetration rates. Weights for different indicators are calculated based on the improved analytic hierarchy process (IAHP). The efficiency of the method is significantly improved compared to the Monte Carlo simulation (MCS). The evaluation results also indicate that proper PV penetration improves distribution system operation security.},
  comment   = {used sparse PCE of UQLab},
  doi       = {10.1109/APPEEC48164.2020.9220690},
  eventdate = {2018-09-20/2018-09-23},
  file      = {:Yang2020 - Source Grid Load Combined Security Assessment of PV Penetrated Distribution Network.pdf:PDF},
  keywords  = {electrical engineering},
  url       = {https://doi.org/10.1109/APPEEC48164.2020.9220690},
}

@Article{Sime2020,
  author   = {Jacqueline E. Sime and Pascal Morel and Mohamad Younes and Igor Simone Stievano and Mihai Telescu and Noël Tanguy and Stéphane Azou},
  journal  = {{IEEE} Photonics Technology Letters},
  title    = {The effects of digital predistortion in a {CO-OFDM} system --- {A} stochastic approach},
  year     = {2020},
  number   = {13},
  pages    = {763--766},
  volume   = {32},
  abstract = {Digital predistortion is topic of significant interest in telecommunications - both in the wireless radio field and, more recently, in photonics. In the present letter, the authors undertake a sensitivity analysis of various digital predistortion algorithms. Using recent metamodeling techniques designed for efficient stochastic analysis, the authors show that using predistortion not only leads to a reduction of the error vector magnitude in general but can also make the system less sensitive to uncertainties.},
  comment  = {used PCE of UQLab.},
  doi      = {10.1109/LPT.2020.2994892},
  file     = {:Sime2020 - The Effects of Digital Predistortion in a CO OFDM System a Stochastic Approach.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://doi.org/10.1109/LPT.2020.2994892},
}

@Article{Sauder2020,
  author   = {T. Sauder},
  journal  = {Experimental Techniques},
  title    = {Fidelity of cyber-physical empirical methods: {A} control system perspective},
  year     = {2020},
  pages    = {669--685},
  volume   = {44},
  abstract = {Cyber-physical empirical methods enable to address problems that classical empirical methods alone, or models alone, cannot address in a satisfactory way. In CPEMs, the substructures are interconnected through a control system that includes sensors and actuators, having their own dynamics. The present paper addresses how the fidelity of CPEMs, that is the degree to which they reproduce the behaviour of the real system under study, is affected by the presence of this control system. We describe an analysis method that enables the designer of a CPEM to (1) identify the artefacts (such as biases, noise, or delays) that play a significant role for the fidelity, (2) define bounds for the describing parameter of these artefacts ensuring high-fidelity of the CPEM, and (3) evaluate whether probabilistic robust fidelity is achieved. The proposed method is illustrated by considering a substructured slender structure subjected to dynamic loading.},
  comment  = {used PCK, PCE, Sensitivity Analysis of UQLab.},
  doi      = {10.1007/s40799-020-00372-x},
  file     = {:Sauder2020 - Fidelity of Cyber Physical Empirical Methods_ a Control System Perspective.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://doi.org/10.1007/s40799-020-00372-x},
}

@Article{GarciaMartin2020,
  author   = {Roberto García-Martin and Jorge López-Rebollo and Luis Javier Sánchez-Aparicio and José G. Fueyo and Javier Pisonero and Diego González-Aguilera},
  journal  = {Construction and Building Materials},
  title    = {Digital image correlation and reliability-based methods for the design and repair of pressure pipes through composite solutions},
  year     = {2020},
  volume   = {248},
  abstract = {This work aims to develop an approach for the reliability-based analysis for the design and repair of pressurized pipes by means of composite solutions. To this end, the approach uses a simulation method to estimate the failure probability of the solution based on the Monte Carlo approach and a Polynomial Chaos Expansion surrogate metamodeling strategy. This combination allows us to reduce the computational time required for evaluating the system’s probability of failure as well as extracting the Sobol’ indices during the sensitivity analysis stage. The uncertainties related with the composite solution were obtained by means of the Digital Image Correlation approach, allowing us to extract the Probabilistic Distribution Functions (PDF) of its main mechanical parameters. This methodology is validated through the design and repair of a pressurized pipe using a carbon fiber solution and roll wrapping technology. The results show the strong potential of the proposed methodology for the safety evaluation of pressurized composite pipes.},
  comment  = {used Reliability module of UQLab.},
  doi      = {10.1016/j.conbuildmat.2020.118625},
  file     = {:GarciaMartin2020 - Digital Image Correlation and Reliability Based Methods for the Design and Repair of Pressure Pipes through Composite Solutions.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1016/j.conbuildmat.2020.118625},
}

@Article{Ligeikis2020,
  author   = {Connor Ligeikis and Richard Christenson},
  journal  = {Experimental Techniques},
  title    = {Identifying stochastic frequency response functions using real-time hybrid substructuring, principal component analysis, and {Kriging} metamodeling},
  year     = {2020},
  pages    = {763--786},
  volume   = {44},
  abstract = {Real-time hybrid substructuring (RTHS) has previously been shown to be an effective tool to quantify the effect of parametric uncertainties on the response of a structural system. Proposed and implemented in this paper is a method that combines RTHS, Principal Component Analysis, and Kriging to metamodel the frequency response functions of a structure. The proposed method can be used to account for parametric variation in both the numerical and physical substructures. This approach is demonstrated using a series of bench-scale RTHS tests of a magnetorheological (MR) fluid damper used to control a 2 degree-of-freedom mass-spring system. The numerical system spring stiffnesses and the physical current supplied to the MR damper are each treated as uniformly distributed random variables. The RTHS test data is used to train computationally fast metamodels, which can then be used to conduct Monte Carlo simulations to determine distributions of the system response. The proposed methodology is shown to be both efficient and accurate.},
  comment  = {used Kriging of UQLab.},
  doi      = {10.1007/s40799-020-00389-2},
  file     = {:Ligeikis2020 - Identifying Stochastic Frequency Response Functions Using Real Time Hybrid Substructuring, Principal Component Analysis, and Kriging Metamodeling.pdf:PDF},
  url      = {https://doi.org/10.1007/s40799-020-00389-2},
}

@Article{Zhou2020,
  author   = {Yicheng Zhou and Zhenzhou Lu and Jinghan Hu and Yingshi Hu},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Surrogate modeling of high-dimensional problems via data-driven polynomial chaos expansions and sparse partial least square},
  year     = {2020},
  volume   = {364},
  abstract = {Surrogate modeling techniques such as polynomial chaos expansion (PCE) are widely used to simulate the behavior of manufactured and physical systems for uncertainty quantification. An inherent limitation of many surrogate modeling methods is their susceptibility to the curse of dimensionality, that is, the computational cost becomes intractable for problems involving a high-dimensionality of the uncertain input parameters. In the paper, we address the issue by proposing a novel surrogate modeling method that enables the solution of high dimensional problems. The proposed surrogate model relies on a dimension reduction technique, called sparse partial least squares (SPLS), to identify the projection directions with largest predictive significance in the PCE surrogate. Moreover, the method does not require (or even assume the existence of) a functional form of the distribution of input variables, since a data-driven construction, which can ensure that the polynomial basis maintains the orthogonality for arbitrary mutually dependent randomness, is applied to surrogate modeling. To assess the performance of the method, a detailed comparison is made with several well-established surrogate modeling methods. The results show that the proposed method can provide an accurate representation of the response of high-dimensional problems.},
  comment  = {used PCE (LAR) and LRA of UQLab.},
  doi      = {10.1016/j.cma.2020.112906},
  file     = {:Zhou2020 - Surrogate Modeling of High Dimensional Problems Via Data Driven Polynomial Chaos Expansions and Sparse Partial Least Square.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.cma.2020.112906},
}

@Article{Paredes2020,
  author   = {Guilherme Moura Paredes and Claes Eskilsson and Allan P. Engsig-Karup},
  journal  = {Journal of Marine Science and Engineering},
  title    = {Uncertainty quantification in mooring cable dynamics using polynomial chaos expansions},
  year     = {2020},
  number   = {3},
  volume   = {8},
  abstract = {Mooring systems exhibit high failure rates. This is especially problematic for offshore renewable energy systems, like wave and floating wind, where the mooring system can be an active component and the redundancy in the design must be kept low. Here we investigate how uncertainty in input parameters propagates through the mooring system and affects the design and dynamic response of mooring and floaters. The method used is a nonintrusive surrogate based uncertainty quantification (UQ) approach based on generalized Polynomial Chaos (gPC). We investigate the importance of the added mass, tangential drag, and normal drag coefficient of a catenary mooring cable on the peak tension in the cable. It is found that the normal drag coefficient has the greatest influence. However, the uncertainty in the coefficients plays a minor role for snap loads. Using the same methodology we analyze how deviations in anchor placement impact the dynamics of a floating axi-symmetric point-absorber. It is shown that heave and pitch are largely unaffected but surge and cable tension can be significantly altered. Our results are important towards streamlining the analysis and design of floating structures. Improving the analysis to take into account uncertainties is especially relevant for offshore renewable energy systems where the mooring system is a considerable portion of the investment.},
  comment  = {used PCE of UQLab.},
  doi      = {10.3390/jmse8030162},
  file     = {:Paredes2020 - Uncertainty Quantification in Mooring Cable Dynamics Using Polynomial Chaos Expansions.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.3390/jmse8030162},
}

@InProceedings{Aerts2019,
  author    = {Sam Aerts and Yuanyuan Huang and Luc Martens and Wout Joseph and Joe Wiart},
  booktitle = {Proceedings of the Joint Annual Meeting of the Bioelectromagnetics Society and the European BioElectromagnetics Association (BioEM2019)},
  title     = {Measurement-based modeling of {RF-EMF} exposure in urban environments using artificial intelligence techniques},
  year      = {2019},
  address   = {Montpellier, France},
  pages     = {254--258},
  abstract  = {This study explores the use of artificial intelligence (AI) techniques to model the exposure to nvironmental radiofrequency (RF) electromagnetic fields (EMF) in an urban environment. For this, a spatial RF-EMF distribution was simulated in the 14th district of Paris, France, based on the existing base station infrastructure and a simple path loss model. Then, a number of sample measurements were used to build and compare an artificial neural network (ANN) model and a conventional kriging interpolation model. The ability to use additional information (such as distance to base stations) in the NN model results in a promising approach – especially when measurement data are scarce – and more research is encouraged.},
  comment   = {used Kriging of UQLab},
  eventdate = {2019-06-23/2019-06-28},
  file      = {:Aerts2019 - Measurement Based Modeling of RF EMF Exposure in Urban Environments Using Artificial Intelligence Techniques.pdf:PDF},
  url       = {https://biblio.ugent.be/publication/8647488},
}

@InCollection{Florian2020,
  author    = {Francesco Florian and Rossana Vermiglio},
  booktitle = {Current Trends in Dynamical Systems in Biology and Natural Sciences},
  publisher = {Springer},
  title     = {{PC}-based sensitivity analysis of the basic reproduction number of population and epidemic models},
  year      = {2020},
  editor    = {M. Aguiar and C. Braumann and B. Kooi and A. Pugliese and N. Stollenwerk and E. Venturino},
  pages     = {205--222},
  series    = {SEMA SIMAI Springer Series},
  volume    = {21},
  abstract  = {The basic reproduction number, simply denoted by R0, plays a fundamental role in the analysis of population and epidemic models. However in mathematical modelling the specification of the input parameters can be crucial since, due to some limitations in experimental data available, they can be uncertain and often represented as random quantities in a suitable probabilistic framework. In this context the Polynomial Chaos Expansions (PCEs), coupled with suitable numerical methods, furnish important tools for the sensitivity analysis and the uncertainty quantification of the random model response. The aim of this paper is to describe how the variability of R0 is affected by the variability of the input parameters, through the evaluation of Sobol’ indices by PC-based methods. The use of a suitable and new computational model of R0 allows also to consider more complex epidemic models, where R0 is defined as the spectral radius of the infinite-diminensional next generation operator. The efficiency and versatility of the numerical approach are confirmed by the experimental analysis of two examples of increasing complexity.},
  comment   = {used PCE of UQLab.},
  doi       = {10.1007/978-3-030-41120-6_11},
  file      = {:Florian2020 - PC Based Sensitivity Analysis of the Basic Reproduction Number of Population and Epidemic Models.pdf:PDF},
  keywords  = {biology},
  url       = {https://doi.org/10.1007/978-3-030-41120-6_11},
}

@Article{Yun2020,
  author   = {XiangSun Yun and Young Choi and Jung-Il Choi},
  journal  = {Applied Mathematical Modelling},
  title    = {Global sensitivity analysis for multivariate outputs using polynomial chaos-based surrogate models},
  year     = {2020},
  volume   = {82},
  abstract = {We propose an efficient global sensitivity analysis method for multivariate outputs that applies polynomial chaos-based surrogate models to vector projection-based sensitivity indices. These projection-based sensitivity indices, which are powerful measures of the comprehensive effects of model inputs on multiple outputs, are conventionally estimated by the Monte Carlo simulations that incur prohibitive computational costs for many practical problems. Here, the projection-based sensitivity indices are efficiently estimated via two polynomial chaos-based surrogates: polynomial chaos expansion and a proper orthogonal decomposition-based polynomial chaos expansion. Several numerical examples with various types of outputs are tested to validate the proposed method; the results demonstrate that the polynomial chaos-based surrogates are more efficient than Monte Carlo simulations at estimating the sensitivity indices, even for models with a large number of outputs. Furthermore, for models with only a few outputs, polynomial chaos expansion alone is preferable, whereas for models with a large number of outputs, implementation with proper orthogonal decomposition is the best approach.},
  comment  = {used PCE of UQLab.},
  doi      = {10.1016/j.apm.2020.02.005},
  file     = {:Yun2020 - Global Sensitivity Analysis for Multivariate Outputs Using Polynomial Chaos Based Surrogate Models.pdf:PDF},
  keywords = {computational science and engineering},
  url      = {https://doi.org/10.1016/j.apm.2020.02.005},
}

@Article{Zhou2020a,
  author   = {Siyang Zhou and Xiangfeng Guo and Qian Zhang and Daniel Dias and Qiujing Pan},
  journal  = {Computers and Geotechnics},
  title    = {Influence of a weak layer on the tunnel face stability -- {Reliability} and sensitivity analysis},
  year     = {2020},
  volume   = {122},
  abstract = {In this study, reliability and sensitivity analyses were employed to investigate the effects of a weak interlayer on tunnel face stability. Due to the irregular geology process and the movement of tunnelling machine, the situations of weak soils encountered during tunnel construction are complex and variable. To address this problem, this paper adopted reliability and sensitivity analysis to evaluate the face stability of a tunnel considering a weak soil layer. An active learning method which combines Kriging and Monte Carlo Simulation (AK-MCS) was used for the reliability analysis. Then, the importance of each stochastic soil property was estimated by coupling the surrogate model constructed in the AK-MCS with the global sensitivity analysis (GSA). Compared with traditional approaches, the employed technique (termed as AK-MCS-GSA) is able to provide a variety of interesting results (failure probability, model response statistics and sensitivity index) but with a greatly reduced computational time. The obtained results permitted to show that a weak soil layer over the tunnel cross-section can lead to a significant increase of failure probability. Additionally, the sensitivity indices in GSA revealed that the soil's frictional angle of the weak layer has the greatest influence on the face stability. Moreover, the impacts of varying the weak layer’s position and thickness are discussed. The critical position for the weak layer which leads to the biggest failure probability is remarked.},
  comment  = {used AK-MCS of UQLab.},
  doi      = {10.1016/j.compgeo.2020.103507},
  file     = {:Zhou2020a - Influence of a Weak Layer on the Tunnel Face Stability Reliability and Sensitivity Analysis.pdf:PDF},
  url      = {https://doi.org/10.1016/j.compgeo.2020.103507},
}

@Article{Strand2020,
  author   = {Andreas Strand and Ivar Eskerud Smith and Tor Erling Unander and Ingelin Steinsland and Leif Rune Hellevik},
  journal  = {Algorithms},
  title    = {Uncertainty propagation through a point model for steady-state two-phase pipe flow},
  year     = {2020},
  number   = {3},
  volume   = {13},
  abstract = {Uncertainty propagation is used to quantify the uncertainty in model predictions in the presence of uncertain input variables. In this study, we analyze a steady-state point-model for two-phase gas-liquid flow. We present prediction intervals for holdup and pressure drop that are obtained from knowledge of the measurement error in the variables provided to the model. The analysis also uncovers which variables the predictions are most sensitive to. Sensitivity indices and prediction intervals are calculated by two different methods, Monte Carlo and polynomial chaos. The methods give similar prediction intervals, and they agree that the predictions are most sensitive to the pipe diameter and the liquid viscosity. However, the Monte Carlo simulations require fewer model evaluations and less computational time. The model predictions are also compared to experiments while accounting for uncertainty, and the holdup predictions are accurate, but there is bias in the pressure drop estimates.},
  comment  = {used PCE and Sobol' Analysis of UQLab.},
  doi      = {10.3390/a13030053},
  file     = {:Strand2020 - Uncertainty Propagation through a Point Model for Steady State Two Phase Pipe Flow.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://doi.org/10.3390/a13030053},
}

@Article{Wang2020g,
  author   = {Han Wang and Zheng Yan and Mohammad Shahidehpour and Xiaoyuan Xu and Quan Zhou},
  journal  = {{IEEE} Transactions on Smart Grid},
  title    = {Quantitative evaluations of uncertainties in multivariate operations of microgrids},
  year     = {2020},
  number   = {4},
  pages    = {2892--2903},
  volume   = {11},
  abstract = {Microgrid (MG) provides a promising solution to managing various distributed energy resources and loads within defined electrical boundaries. However, variable renewable energy generation and loads as MG inputs would inject complex uncertainties into MG operations, which have significant impacts on MG outputs (e.g., system frequency, bus voltages, and power flows). It is imperative to evaluate the impacts of pertinent uncertainties on MG multivariate outputs for securing MG operations. In this paper, a multivariate global sensitivity analysis (MV-GSA) with three MV-indices is proposed for quantifying the impacts of correlated renewable energy generation on MG multivariate outputs. The proposed MV-GSA overcomes the shortcomings of conventional univariate GSA (UV-GSA) in which the correlations among multivariate outputs are neglected. Also, analytical representations of indices for the proposed MV-GSA are derived by which the computation burdens are significantly reduced as compared with those of using the Monte Carlo simulation. The effectiveness of the proposed method is verified by using the modified IEEE 33-bus and IEEE 123-bus MG systems.},
  comment  = {used sparse PCE of UQLab.},
  doi      = {10.1109/TSG.2020.2971689},
  file     = {:Wang2020g - Quantitative Evaluations of Uncertainties in Multivariate Operations of Microgrids.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://doi.org/10.1109/TSG.2020.2971689},
}

@Article{Chen2020a,
  author   = {Yangli Chen and Weimin Ma},
  journal  = {Annals of Nuclear Energy},
  title    = {Uncertainty quantification for {TRACE} simulation of {FIX-II} {No.} 5052 test},
  year     = {2020},
  volume   = {143},
  abstract = {The Best Estimate Plus Uncertainty approach requires the knowledge of input uncertainties for the uncertainty propagation with best-estimate codes. Inaccurate judgement of some model parameter uncertainties related to the dominant physical phenomena may result in misestimation of the safety margin. This paper presents a framework of inverse uncertainty quantification (UQ) to quantify model parameter uncertainties in order to address this issue. It is applied to TRACE simulation of a large break loss of coolant accident conducted on the FIX-II facility, and peak cladding temperature (PCT) is the simulation output. Sensitivity analysis identifies the parameters of the critical flow model as the most influential to the PCT. The inverse UQ is performed based on Bayesian framework, which adopts Markov Chain Monte Carlo sampling and surrogate modelling algorithms. The quantified uncertainties of the model parameters are the desired results from the inverse UQ process, which are useful in BEPU studies.},
  comment  = {used PCK of UQLab.},
  doi      = {10.1016/j.anucene.2020.107490},
  file     = {:Chen2020a - Uncertainty Quantification for TRACE Simulation of FIX II No. 5052 Test.pdf:PDF},
  keywords = {nuclear engineering},
  url      = {https://doi.org/10.1016/j.anucene.2020.107490},
}

@Article{Lebloub2020,
  author   = {Moussa Lebloub and Sami W. Tabsh and Samer Barakat},
  journal  = {Journal of Constructional Steel Research},
  title    = {Reliability-based design of corrugated web steel girders in shear as per {AASHTO} {LRFD}},
  year     = {2020},
  volume   = {169},
  abstract = {Despite their successful use in many bridges across several countries around the world, corrugated web steel girders (CWSGs) haven't been yet recognized formally by mainstream design codes and standards. CWSGs can be an economical alternative to conventional welded plate girders; they can achieve higher shear strengths with lesser material usage. Corrugated webs have been demonstrated to carry about half of their ultimate shear strength after buckling, which is an important reserve strength to overcome sudden collapses of bridge structures. The current design practice combines the latest research in the field with engineering judgment with no regard for reliability-based design. In light of the new developments in the field of bridge design and reliability analysis techniques, this paper revisits first the reliability of welded plate girders in shear and verifies their target reliability index and the corresponding resistance factor as per AASHTO LRFD. Then, and in an attempt to prepare the inclusion of CWSGs shear design in AASHTO LRFD, a probabilistic-based calibration procedure is applied to calibrate the resistance factor for bridges having a range of span lengths and girder spacings, with consideration of different average daily truck traffic. A series of sensitivity analyses were carried out in order to quantify the relative contribution of each design parameter to the overall reliability of CWSGs in shear. The findings of this study suggest that, for CWSGs in shear, a resistance factor of 0.95 is appropriate to ensure a reliability level that is consistently close to the target. The sensitivity analysis showed that the web thickness, web depth, and the yield strength of the web's steel material are the most influential parameters on the reliability index of CWSGs.},
  comment  = {used Reliability module of UQLab.},
  doi      = {10.1016/j.jcsr.2020.106013},
  file     = {:Lebloub2020 - Reliability Based Design of Corrugated Web Steel Girders in Shear As Per AASHTO LRFD.pdf:PDF},
  keywords = {civil engineering},
  url      = {https://doi.org/10.1016/j.jcsr.2020.106013},
}

@Article{Lebloub2020a,
  author   = {Moussa Lebloub and Sami W. Tabsh},
  journal  = {Engineering Structures},
  title    = {Reliability-based shear design of corrugated web steel beams for {AISC} 360 specification and {CSA-S16} standard},
  year     = {2020},
  volume   = {215},
  abstract = {The available literature on the shear strength of corrugated web steel beams (CWSBs) includes a wealth of methods that specify the nominal buckling capacity, but with no consideration of reliability-based design. In the framework of Load and Resistance Factor Design (LRFD) codes, the design of steel members requires load factors to amplify the load effect and a resistance factor to scale-down the resistance based on a target reliability index. Therefore, for full integration of the shear design of CWSBs into the current design codes, resistance factors must be developed and calibrated for use within the LRFD context. This is crucial since the use of existing resistance factors that were specifically developed for welded plate girders for the design of CWSBs may lead to a level of safety that is not consistent with the philosophy of the code. Using a curated database of test results, the present paper attempts to address the lack of a reliability-based design method specific to CWSBs. Two North American design standards were selected in this study, namely, AISC 360 and CSA-S16. A series of reliability analyses are performed first to verify the consistency of reliability indices for welded plate girders designed following the two standards. Then, a probabilistic-based approach is used to calibrate the resistance factors for the shear limit state of CWSBs in accordance with the AISC 360 and CSA-S16 considering dead, live, wind, and snow loads. Finally, an iterative step-by-step procedure is then developed within the LRFD framework to aid in the design of corrugated webs to resist shear loads at ultimate. Findings of the study showed that the current AISC 360 resistance factor for welded plate girders when used for the shear design for CWSB, does not yield a safety level that is consistent with the target reliability; thus, a new factor equal to 0.85 is proposed. For CSA-S16, the current resistance factor of 0.90 for welded plate girders is found to be appropriate for corrugated web steel beams.},
  comment  = {used Reliability module of UQLab.},
  doi      = {10.1016/j.engstruct.2020.110617},
  file     = {:Lebloub2020a - Reliability Based Shear Design of Corrugated Web Steel Beams for AISC 360 Specification and CSA S16 Standard.pdf:PDF},
  url      = {https://doi.org/10.1016/j.engstruct.2020.110617},
}

@InProceedings{Nispel2020,
  author       = {Abraham Nispel and Stephen Ekwaro-Osire and João Paul Dias},
  booktitle    = {Proceedings of {ASME} Turbo Expo 2020 Turbomachinery Technical Conference and Exposition {GT2020}},
  title        = {Probabilistic analysis of the fatigure life of offshore wind turbine structures},
  year         = {2020},
  address      = {London, UK},
  organization = {ASME},
  abstract     = {The structural response of the main components of offshore wind turbines (OWTs) is considerably sensitive to amplification as their excitation frequencies approach the natural frequency of the structure. Furthermore, uncertainties present in the loading conditions, soil and structural properties highly influence the dynamic response of the OWT. In most cases, the cost of the structure reaches around 30% of the entire OWT because conservative design approaches are employed to ensure its reliability. As a result, this study aims to address the following research question: can the structural reliability of OWT under fatigue loading conditions be predicted more consistently? The specific aims are to (1) establish the design parameters that most impact the fatigue life, (2) determine the probability distributions of the design parameters, and (3) predict the structural reliability. An analytical model to determine the fatigue life of the structure under 15 different loading conditions and two different locations were developed. Global sensitivity analysis was used to establish the more important design parameters. Also, a systematic uncertainty quantification (UQ) scheme was employed to model the uncertainties of model input parameters based on their available information. Finally, the framework used reliability analysis to consistently determine the system probability of failure of the structure based on the fatigue limit state design criterion. The results show high sensitivity for parameters usually considered as deterministic values in design standards. Additionally, it is shown that applying systematic UQ produces a better approximation of the fatigue life under uncertainty and more accurate estimations of the structural reliability. Consequently, more reliable and robust structural designs may be achieved without the need for overestimating the offshore wind turbine response.},
  comment      = {used MCS of UQLab},
  eventdate    = {2020-06-22/2020-06-26},
  file         = {:Nispel2020 - Probabilistic Analysis of the Fatigure Life of Offshore Wind Turbine Structures.pdf:PDF},
  keywords     = {civil engineering},
  url          = {https://www.researchgate.net/publication/340935365_PROBABILISTIC_ANALYSIS_OF_THE_FATIGUE_LIFE_OF_OFFSHORE_WIND_TURBINE_STRUCTURES},
}

@Article{Joshi2020,
  author   = {Kavita Joshi and Gaurav Tembhurnikar and Rajesh C. Patil},
  journal  = {Resincap Journal of Science and Engineering},
  title    = {A load management of islanded micro-grids with renewable energy generation},
  year     = {2020},
  number   = {6},
  pages    = {1063--1067},
  volume   = {4},
  abstract = {microgrid (MG) is a small-scale power system which is fed by constrained distributed generation (DG) units and its continuous operation is affected by the variability of available generation resources. In this paper a global sensitivity analysis (GSA) method is proposed to evaluate the impact of variable energy resources on the maximum load ability of islanded
MGs (IMGs). First, a probabilistic optimization problem is formulated to calculate the IMG load margin considering the droop characteristics of DG units and uncertainties of renewable generation, loads and distribution feeder parameters. Then, the global sensitivity (GS) is introduced that can identify the impact of independent and correlated variables on the IMG load ability. Next, the sparse polynomial
chaos expansion (SPCE) method is used to obtain the
probabilistic models for IMG load margins, and an efficient GSA method is proposed to calculate the GS of IMG load ability to prevailing variables. The probabilistic models are considered for IMG input variables and the impact of variable correlations on IMG load ability is analyzed. The proposed method for calculating the maximum loadability is tested
using a 33-bus IMG and the results are compared with those of other GSA and local sensitivity analysis (LSA) methods.},
  comment  = {used sparse PCE of UQLab.},
  file     = {:Joshi2020 - A Load Management of Islanded Micro Grids with Renewable Energy Generation.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://www.rijse.com/wp-content/uploads/2020/07/A-Load-management-of-Islanded-Micro-grids-with-Renewable-Energy-Generation.pdf},
}

@InProceedings{Bonzanini2020,
  author       = {Angelo D. Bonzanini and Joel A. Paulson and Ali Mesbah},
  booktitle    = {2020 59th {IEEE} Conference on Decision and Control ({CDC})},
  title        = {Safe learning-based model predictive control under state- and input-dependent uncertainty using scenario trees},
  year         = {2020},
  address      = {Jeju Island, South Korea},
  organization = {IEEE},
  abstract     = {The complex and uncertain dynamics of emerging systems pose several unique challenges that need to be overcome in order to synthesize high-performance controllers. A key challenge is that safety is often achieved at the expense of closed-loop performance. This is particularly apparent when the uncertainty description is provided in the form of a bounded set that is estimated offline from limited data. Several recent works suggest replacing this bounded set with a learned state-and input-dependent uncertainty that can represent the fact that the degree of uncertainty in the model may differ throughout the state space. Gaussian process (GP) models are a good candidate for learning such a representation; however, they produce a nonlinear and non-convex description of the uncertainty set that is difficult to incorporate into currently available robust model predictive control (MPC) frameworks. In this work, we present a learning-and scenario-based MPC (L-sMPC) strategy that systematically accounts for feedback in the prediction using a state-and input-dependent scenario tree computed from a GP uncertainty model. To ensure that the closed-loop system evolution remains safe, we also propose a projection-based safety certification scheme that ensures the control inputs keep the system within an appropriately defined invariant set. The advantages of the proposed L-sMPC method in terms of improved performance and an enlarged feasible region are illustrated on benchmark problem.},
  comment      = {used Kriging of UQLab.},
  eventdate    = {2020-12-14/2020-12-18},
  file         = {:Bonzanini2020 - Safe Learning Based Model Predictive Control under State and Input Dependent Uncertainty Using Scenario Trees.pdf:PDF},
  keywords     = {computational science and engineering},
  url          = {https://www.researchgate.net/publication/343473324_Safe_Learning-based_Model_Predictive_Control_under_State-and_Input-dependent_Uncertainty_using_Scenario_Trees},
}

@Article{Totis2020,
  author   = {G. Totis and M. Sortino},
  journal  = {International Journal of Machine Tools and Manufacture},
  title    = {{Polynomial Chaos-Kriging} approaches for an efficient probabilistic chatter prediction in milling},
  year     = {2020},
  volume   = {157},
  abstract = {After more that 60 years of investigation, chatter vibrations in metal cutting are still a major cause for poor surface finish and machine tool damage. In order to avoid undesired machining conditions, chatter prediction algorithms may be applied to draw stability charts that allow a preliminary identification of the safe areas. Nevertheless, the stability boundaries are sensitive to the variations and uncertainties of the dynamic milling model coefficients. Thus, the accuracy and reliability of the obtained predictions can be inadequate for many industrial applications. For solving this problem, robust methods were recently devised that are fast but usually too conservative. On the other side, probabilistic approaches were also developed to estimate the probability of instability for a given combination of cutting parameters, by taking into account the statistical distributions of model coefficients. Probabilistic approaches allow a less conservative, risk-aware selection of stable cutting conditions. Unfortunately, their application is still very limited due to the required large amount of computational power and time. In this work, three novel probabilistic methods based on Polynomial Chaos and Kriging metamodels (PCE, KRI and PCK) were compared to state of the art probabilistic algorithms (MC, MC-SPA, DRM-SPA, RCPM). The numerical analysis and the experimental validation proved that MC-SPA, DRM-SPA, RCPM and PCE are too rough and thus needless for industrial applications. On the contrary, KRI and in some cases also PCK showed an excellent accuracy together with significantly shorter elaboration time than that required by the reference Monte Carlo (MC) technique.},
  comment  = {used PCE, PCK, Kriging of UQLab.},
  doi      = {10.1016/j.ijmachtools.2020.103610},
  file     = {:Totis2020 - Polynomial Chaos Kriging Approaches for an Efficient Probabilistic Chatter Prediction in Milling.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://doi.org/10.1016/j.ijmachtools.2020.103610},
}

@Article{Pichon2020,
  author   = {Lionel Pichon},
  journal  = {Journal of Electromagnetic Waves and Application},
  title    = {Electromagnetic analysis and simulation aspects of wireless power transfer in the domain of inductive power transmission technology},
  year     = {2020},
  number   = {13},
  pages    = {1719--1755},
  volume   = {34},
  abstract = {Wireless power transfer (WPT) technologies have now reached a commercial stage in applications going from consumer electronics, biomedical implants and domestic applications. For electrical cars, WPT offers some new opportunities compared to classical wired charging. Although there exist some commercial cars equipped with wireless charging system, improving design procedures needs to address various domains including power electronics, components and electromagnetics. The magnetic coupling system is a key component of wireless power transfer: the coils and surrounding materials have a great impact on the efficiency of the transfer as well as the level of the stray field near the system. The paper aims to present three essential features regarding the inductive power transfer dedicated to electric vehicles. The first one addresses the topology and efficiency of the coupling system, the second one underlines the human exposure considerations and the third one deals with the impact of uncertainties regarding system parameters.},
  comment  = {used sparse PCE of UQLab.},
  doi      = {10.1080/09205071.2020.1799870},
  file     = {:Pichon2020 - Electromagnetic Analysis and Simulation Aspects of Wireless Power Transfer in the Domain of Inductive Power Transmission Technology.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://doi.org/10.1080/09205071.2020.1799870},
}

@Article{Pawar2020,
  author   = {Hemantkumar Pawar and Ajit P. Chaudhari and N. G. Nirmal},
  journal  = {Resincap Journal of Science and Engineering},
  title    = {Management of autonomous power in interconnected {AC-DC} microgrids and distributed energy resources},
  year     = {2020},
  number   = {8},
  pages    = {1156--1163},
  volume   = {4},
  abstract = {Renewable energy based distributed generators (DGs) play a dominant role in electricity production, with the increase in the global warming. Distributed generation based on wind, solar energy, biomass, mini-hydro along with use of fuel cells and micro turbines will give significant momentum in near future. Advantages like environmental friendliness, expandability and flexibility have made distributed generation, powered by various renewable and nonconventional micro sources, an attractive option for configuring modern electrical grids. A micro grid consists of cluster of loads and distributed generators that operate as a single controllable system. As an integrated energy delivery system micro grid can operate in parallel with or isolated from the main power grid. The micro grid concept introduces the reduction of multiple reverse conversions in an individual AC or DC grid and also facilitates connections to variable renewable AC and DC sources and loads to power systems. The interconnection of DGs to the utility/ grid through power electronic converters has risen concerned about safe operation and protection of equipment‟s. To the customer the micro grid can be designed to meet their special requirements; such as, enhancement of local reliability, reduction of feeder losses, local volt ages support, increased efficiency through use of waste heat, correction of voltage sag or uninterruptible power supply. In the present work the performance of hybrid AC/DC micro grid system is analyzed in the grid tied mode. Here photovoltaic system, wind turbine generator and bat they are used for the development of micro grid. Also control
mechanisms are implemented for the converters to properly co- ordinate the AC sub-grid to DC sub-grid. The result is obtained from the MATLAB/ SIMULINK environment.},
  comment  = {used sparse PCE of UQLab.},
  file     = {:Pawar2020 - Management of Autonomous Power in Interconnected AC DC Microgrids and Distributed Energy Resources.pdf:PDF},
  keywords = {electrical engineering},
  url      = {https://www.rijse.com/paper-category/issue-8-volume-4/},
}

@Article{Kumar2020,
  author   = {P. Kumar and B. Sanderse and K. Boorsma and M. Caboni},
  journal  = {Journal of Physics: Conference Series},
  title    = {Global sensitivity analysis of model uncertainty in aeroelastic wind turbine models},
  year     = {2020},
  volume   = {1618},
  abstract = {A framework is presented for performing global sensitivity analysis of model parameters associated with the Blade Element Momentum (BEM) models. Sobol indices based on adaptive sparse polynomial expansions are used as a measure of global sensitivities. The sensitivity analysis workflow is developed using the uncertainty quantification toolbox UQLab that is integrated with TNO's Aero-Module aeroelastic code. Uncertainties in chord, twist, and lift- and drag-coefficients have been parametrized through the use of NURBS curves. Sensitivity studies are performed on the NM80 wind turbine model from the DanAero project, for a case with 19 uncertainties in both model and geometry. The combination of parametrization and sparse adaptive polynomial chaos yields a new efficient framework for global sensitivity analysis of aeroelastic wind turbine models, paving the way to effective model calibration.},
  comment  = {used sparse PCE and Sobol' analysis of UQLab.},
  doi      = {10.1088/1742-6596/1618/4/042034},
  file     = {:Kumar2020 - Global Sensitivity Analysis of Model Uncertainty in Aeroelastic Wind Turbine Models.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://www.doi.org/10.1088/1742-6596/1618/4/042034},
}

@Article{Persico2020,
  author   = {Giacomo Persico and Andrea G. Sanvito and Vincenzo Dossena},
  journal  = {Journal of Physics: Conference Series},
  title    = {Quantification of the dynamic-stall model uncertainty in the performance prediction of vertical axis wind turbines},
  year     = {2020},
  volume   = {1618},
  abstract = {Low-fidelity predictions for vertical-axis wind turbines are affected by uncertainty due to the complexity of the rotor aerodynamics. In particular, in the most common operating conditions the blades undergo periodic excursions beyond the static stall limit, activating dynamic-stall effects. In this study we show how advanced dynamic-stall models, implemented in the frame of the Blade-Element-Momentum theory, are able to upgrade significantly the prediction of low-fidelity tools, both in deterministic and probabilistic terms. In particular, an uncertainty quantification is performed to investigate the epistemic uncertainty of the Strickland dynamic-stall model, introducing a large variability on the empirical parameters appearing in the formulation. The resulting variability in the power coefficient and torque exchange, compared to corresponding wind-tunnel and high-fidelity CFD values, remains relatively limited and, in the conditions around peak efficiency, it is comparable with the measurement uncertainty of the experiment. As a further relevant conclusion, the model uncertainty does not alter the general outcome of the deterministic model, thus demonstrating the robustness of the DMST predictions obtained in the present study.},
  comment  = {used PCK of UQLab.},
  doi      = {10.1088/1742-6596/1618/5/052071},
  file     = {:Persico2020 - Quantification of the Dynamic Stall Model Uncertainty in the Performance Prediction of Vertical Axis Wind Turbines.pdf:PDF},
  keywords = {mechanical engineering},
  url      = {https://www.doi.org/10.1088/1742-6596/1618/5/052071},
}

@InCollection{Tosin2020,
  author    = {Michel Tosin and Adriano M. A. Côrtes and Americo Cunha},
  booktitle = {Networks in Systems Biology},
  publisher = {Springer},
  title     = {A tutorial on {Sobol’} global sensitivity analysis applied to biological models},
  year      = {2020},
  editor    = {F. A. B. da Silva and N. Carels and M. Trindade dos Santos and F. J. P. Lopes},
  pages     = {93--118},
  series    = {Computational Biology},
  volume    = {32},
  abstract  = {Nowadays, in addition to traditional qualitative methods, quantitative techniques are also a standard tool to describe biological systems behavior. An example is the broad class of mathematical models, based on differential equations, used in ecology, biochemical kinetics, epidemiology, gene regulatory networks, etc. Independent of their simplicity or complexity, all these models have in common (generally unknown a priori) parameters that need to be identified from observations (data) of the real system, usually available on the literature, obtained by specific assays or surveyed by public health offices. Before using this data to calibrate the models, a good practice is to judge the most influential parameters. That can be done with aid of the Sobol’ indices, a variance-based statistical technique for global sensitivity analysis, which measures the individual importance of each parameter, as well as their joint-effect, on the model output (a.k.a. quantity of interest). These variance-based indexes may be computed using Monte Carlo simulation but, depending on the model, this task can be very costly. An alternative approach for this scenario is the use of surrogate models to speed-up the calculations. Using simple biological models, from different areas, we develop a tutorial that illustrates how practitioners can use Sobol’ indices to quantify, in a probabilistic manner, the relevance of the parameters of their models. This tutorial describes a very robust framework to compute Sobol’ indices employing a polynomial chaos surrogate model constructed with the UQLab package.},
  comment   = {used PCE and Sobol'  analysis of UQLab.},
  doi       = {10.1007/978-3-030-51862-2_6},
  file      = {:Tosin2020 - A Tutorial on Sobol’ Global Sensitivity Analysis Applied to Biological Models.pdf:PDF},
  keywords  = {biology},
  url       = {https://doi.org/10.1007/978-3-030-51862-2_6},
}

@Article{Nandagopal2020,
  author    = {Rajaram Attukur Nandagopal and Srikanth Narasimalu and Gin Boay Chai},
  journal   = {Marine Systems {\&} Ocean Technology},
  title     = {Probabilistic structural reliability analysis of a horizontal axis tidal turbine blade by considering the moisture effects on the blade material},
  year      = {2020},
  month     = sep,
  number    = {4},
  pages     = {253--269},
  volume    = {15},
  abstract  = {The use of composite materials in marine structures like the horizontal axis tidal turbine blade introduces inherent uncertainties in the material properties of the blade. Further, the blade material ages because of the absorption of seawater into the material. Hence, whilst performing the static structural analysis of the blade, the probabilistic nature of the material properties, as well as its degradation due to ageing, must be considered. In this study, the probabilistic structural response of a 0.5 m blade was estimated by considering the experimentally determined statistical distributions of the dry and aged material properties. The polynomial chaos expansion (PCE) method was used to build a surrogate model to mimic the static analysis of the blade, which was used in conjunction with the Monte Carlo simulation to estimate the distributions of the structural response of the blade and the probability of failure of the dry and aged blade. Stochastic convergence was used to check the accuracy of the surrogate model built via the PCE method. The study revealed a statistically significant increase in the probability of failure of the blade due to the ageing induced degradation of the blade material. Further, the need for a probabilistic structural analysis was also established.},
  comment   = {used PCE of UQLab.},
  doi       = {10.1007/s40868-020-00088-y},
  file      = {:Nandagopal2020 - Probabilistic Structural Reliability Analysis of a Horizontal Axis Tidal Turbine Blade by Considering the Moisture Effects on the Blade Material.pdf:PDF},
  keywords  = {mechanical engineering},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007/s40868-020-00088-y},
}

@InProceedings{Orcioni2020,
  author    = {Simone Orcioni and Massimo Conti},
  booktitle = {2020 {IEEE} International Symposium on Circuits and Systems ({ISCAS})},
  title     = {Uncertainty quantification of lithium-ion batteries with polynomial chaos},
  year      = {2020},
  address   = {Sevilla, Spain},
  month     = {oct},
  publisher = {{IEEE}},
  abstract  = {Lithium-ion battery pack performance and longevity can be severely affected by cell-to-cell variations. Statistical modeling is an important tool for optimization of performance and safety of battery packs. This work presents uncertainty quantification of Lithium-ion battery performance, in particular sensitivity analysis. This analysis is based on calculation of Sobol' indices from a surrogate model, based on Polynomial Chaos Expansion. Performance is compared with Monte Carlo simulations.},
  comment   = {used sparse PCE of UQLab},
  doi       = {10.1109/iscas45731.2020.9180515},
  eventdate = {2020-10-10/2020-10-21},
  file      = {:Orcioni2020 - Uncertainty Quantification of Lithium Ion Batteries with Polynomial Chaos.pdf:PDF},
  keywords  = {electrical engineering},
  url       = {https://doi.org/10.1109/ISCAS45731.2020.9180515},
}

@Article{Al2020,
  author   = {Resul Al and Chitta Ranjan Behera and Krist V. Gernaey and Gürkan Sin},
  journal  = {Computers {\&} Chemical Engineering},
  title    = {Stochastic simulation-based superstructure optimization framework for process synthesis and design under uncertainty},
  year     = {2020},
  pages    = {107118},
  volume   = {143},
  abstract = {Advances in simulation and optimization technologies coupled with the continued growth in computing power now increasingly pave the way for the development of advanced model-based engineering design frameworks. In this work, we propose an extensive computational framework, which brings together state-of-the-art engineering practices, such as high fidelity process simulation, superstructure-based conceptual design, global sensitivity analysis, Monte Carlo procedures for uncertainty quantification, and a stochastic simulation-based design space optimizer in order to foster decision making under uncertainty. The capabilities of the framework are highlighted in a case study, which addresses the challenges of how to synthesize and design wastewater treatment plant configurations under influent uncertainties. In order to handle multiple stochastic constraints, a black-box solver using a new infill criterion for surrogate-based optimization is also proposed. The results demonstrate the promising potential of the simulation and sampling-based framework for effectively addressing stochastic design problems arising in broader engineering domains.},
  comment  = {used PCE and Sobol'  analysis of UQLab.},
  doi      = {10.1016/j.compchemeng.2020.107118},
  file     = {:Al2020 - Stochastic Simulation Based Superstructure Optimization Framework for Process Synthesis and Design under Uncertainty.pdf:PDF},
  keywords = {chemical engineering},
  url      = {https://doi.org/10.1016/j.compchemeng.2020.107118},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:EMC risk analysis requires various configurations of coupling paths described by important sets of unknown or uncertain parameters. More specifically, values at risk corresponding to extreme values of relevant fields, currents or voltages are often the most important information with regard to a possible EMC risk. Therefore, we aim at estimating extreme quantiles of the relevant field, current or voltage. Controlled stratification accelerates the standard Empirical estimation convergence to sample output extreme values, thus reducing the required number of calls to cost-expensive full-wave simulations. However, controlled stratification requires a simple (i.e. fast calculation time) model with sufficient correlation to the initial model. The main idea in this communication is to use a surrogate model as a simple model. Kriging was previously identified as a surrogate model with relevant properties. In this paper, we investigate the performance of combined kriging and control stratification. We show that this combination outperforms the stand-alone kriging surrogate model for estimating extreme quantiles. On the contrary, the latter performs better to identify less extreme quantiles.\;2\;1\;\;\;\;;
}
