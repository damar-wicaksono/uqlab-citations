% Encoding: UTF-8

@Article{Abdallah2016,
  author   = {I. Abdallah and A. Natarajan and J. D. S{\o}rensen},
  title    = {{I}nfluence of the control system on wind turbine loads during power production in extreme turbulence: {S}tructural reliability},
  journal  = {Renewable Energy},
  pages    = {464-477},
  month    = {March},
  abstract = {\textcopyright  2015 Elsevier Ltd.The wind energy industry is continuously researching better computational models of wind inflow and turbulence to predict extreme loading (the nature of randomness) and their corresponding probability of occurrence. Sophisticated load alleviation control systems are increasingly being designed and deployed to specifically reduce the adverse effects of extreme load events resulting in lighter structures. The main objective herein is to show that despite large uncertainty in the extreme turbulence models, advanced load alleviation control systems yield both a reduction in magnitude and scatter of the extreme loads which in turn translates in a change in the shape of the annual maximum load distribution function resulting in improved structural reliability. Using a probabilistic loads extrapolation approach and the first order reliability method, a large multi-megawatt wind turbine blade and tower structural reliability are assessed when the extreme turbulence model is uncertain. The structural reliability is assessed for the wind turbine when three configurations of an industrial grade load alleviation control system of increasing complexity and performance are used. The load alleviation features include a cyclic pitch, individual pitch, static thrust limiter, condition based thrust limiter and an active tower vibration damper. We show that large uncertainties in the extreme turbulence model can be mitigated and significantly reduced while maintaining an acceptable structural reliability level when advanced load alleviation control systems are used. We end by providing a rational comparison between the long term loads extrapolation method and the environmental contour method for the three control configurations.},
  date     = {2016},
  day      = {01},
  doi      = {10.1016/j.renene.2015.10.044},
  keywords = {civil engineering, used UQLab},
  piis     = {84946196677},
  url      = {http://dx.doi.org/10.1016/j.renene.2015.10.044},
}

@InProceedings{Abraham2016,
  author    = {Simon Abraham and Ghader Ghorbaniasl and Chris Lacor},
  title     = {{A} statistical approach for building sparse polynomial chaos expansions},
  booktitle = {ECCOMAS Congress 2016 - Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering},
  pages     = {6307-6315},
  month     = {January},
  abstract  = {Over the last years, a lot of effort has been made to make existing uncertainty quantification techniques more efficient in high dimensions. An important class of methods relies on the assumption that the polynomial chaos representation of the model response is sparse. This paper contributes to the validation and assessment of an innovative basis selection technique for building sparse polynomial chaos expansions. A regression approach is used for computing the polynomial chaos coefficients. The technique is based on statistical inference theory which provides information about the true regression model from an estimated regression model based on samples. The latter information is used to build iteratively the sparse polynomial chaos expansion. Using the developed methodology, a more robust and efficient basis selection technique is obtained. For validation purpose, the methodology is applied to high dimensional analytical test cases, including the Oakley \& O\textquotesingle Hagan function (d=15) and the Morris function (d=20). The results are compared with those obtained from two state-of-the-art techniques, namely the LARS-based algorithm and compressive sampling. As compared to previous work, more comparisons with the LARS-based method are provided, through the use of UQLab, a MATLAB-based uncertainty quantification framework developed by Sudret and Marelli [1]. It is shown that, with equal settings, the developed methodology results in a more accurate polynomial chaos expansion compared to the aforementioned technique. In addition, a new criterion for building an optimal polynomial chaos expansion is further investigated. The conclusions are in-line with previous findings, i.e. the present criterion always builds a sparser polynomial chaos expansion which is, in addition, at least as accurate as compared to the optimal polynomial chaos expansion obtained from the classical cross validation technique.},
  date      = {2016},
  day       = {01},
  doi       = {10.7712/100016.2259.7412},
  keywords  = {computational science and engineering, used UQLab, benchmark},
  piis      = {84995530441},
  url       = {http://dx.doi.org/10.7712/100016.2259.7412},
}

@Article{Abraham2017,
  author   = {Simon Abraham and Mehrdad Raisee and Ghader Ghorbaniasl and Francesco Contino and Chris Lacor},
  title    = {{A} robust and efficient stepwise regression method for building sparse polynomial chaos expansions},
  journal  = {Journal of Computational Physics},
  pages    = {461-474},
  month    = {March},
  abstract = {\textcopyright  2016 Elsevier Inc.Polynomial Chaos (PC) expansions are widely used in various engineering fields for quantifying uncertainties arising from uncertain parameters. The computational cost of classical PC solution schemes is unaffordable as the number of deterministic simulations to be calculated grows dramatically with the number of stochastic dimension. This considerably restricts the practical use of PC at the industrial level. A common approach to address such problems is to make use of sparse PC expansions. This paper presents a non-intrusive regression-based method for building sparse PC expansions. The most important PC contributions are detected sequentially through an automatic search procedure. The variable selection criterion is based on efficient tools relevant to probabilistic method. Two benchmark analytical functions are used to validate the proposed algorithm. The computational efficiency of the method is then illustrated by a more realistic CFD application, consisting of the non-deterministic flow around a transonic airfoil subject to geometrical uncertainties. To assess the performance of the developed methodology, a detailed comparison is made with the well established LAR-based selection technique. The results show that the developed sparse regression technique is able to identify the most significant PC contributions describing the problem. Moreover, the most important stochastic features are captured at a reduced computational cost compared to the LAR method. The results also demonstrate the superior robustness of the method by repeating the analyses using random experimental designs.},
  date     = {2017},
  day      = {01},
  doi      = {10.1016/j.jcp.2016.12.015},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85006944223},
  url      = {http://dx.doi.org/10.1016/j.jcp.2016.12.015},
}

@InProceedings{Acikgoz2016,
  author    = {Hulusi Acikgoz and Ravi Kumar Arya and Raj Mittra},
  title     = {{S}tatistical analysis of 3{D}-printed flat {G}RI{N} lenses},
  booktitle = {2016 IEEE Antennas and Propagation Society International Symposium, APSURSI 2016 - Proceedings},
  pages     = {473-474},
  month     = {October},
  abstract  = {\textcopyright  2016 IEEE.This paper presents the statistical analysis of a 3D printed flat lens by using the Polynomial Chaos Expansion (PCE) analysis technique. The flat lens is fabricated using the 3D printing technology and is based on the grading-index (GRIN) approach. It is composed of several concentric rings with graded relative permittivity, made of a single material with different air holes-host material volume ratio. We show that the hole size can have significant effect on the performance of the lens, especially on the focal distance. PCE analysis enables us also to determine the impact of each individual ring on the performance of the lens.},
  date      = {2016},
  day       = {25},
  doi       = {10.1109/APS.2016.7695945},
  keywords  = {electrical engineering, used UQLab},
  piis      = {84997525285},
  url       = {http://dx.doi.org/10.1109/APS.2016.7695945},
}

@InCollection{Al2018,
  author    = {Resul Al and Chitta Ranjan Behera and Alexandr Zubov and G{\"u}rkan Sin},
  title     = {{S}ystematic framework development for the construction of surrogate models for wastewater treatment plants},
  booktitle = {Computer Aided Chemical Engineering},
  pages     = {1909-1914},
  month     = {January},
  abstract  = {Surrogate modeling (also referred to as metamodeling) has attracted increased attention from researchers in various fields of engineering due to its use in computationally expensive engineering tasks such as Monte Carlo based global sensitivity analysis and process design optimization. However, the applications of surrogate models in the field of wastewater treatment modeling have not been extensively explored in the literature. In this work we present a systematic methodology for construction of powerful surrogate models to be used for the global sensitivity analysis of Benchmark Simulation Model 1 (BSM1) plant. A quasi-random design of experiments technique, Sobol sampling, is employed to generate an experimental design, which is further used to build surrogate models. A class of advanced metamodeling algorithms such as sparse polynomial chaos expansion (PCE) using least angle regression, Kriging interpolation, polynomial chaos Kriging (PCK), radial basis function (RBF) interpolation, multivariate adaptive regression splines (MARS), and a multilayer perceptron type feedforward neural network (ANN) are applied. The generalization error of the developed models has been estimated using holdout cross-validation with coefficient of determination (R2) and root mean squared error (RMSE) being used for evaluation of model predictive accuracy and model selection, respectively. The framework was further investigated for its suitability in a Monte Carlo based global sensitivity analysis using Sobol’ method. The results obtained suggest that by following the framework, ANN and Kriging type surrogate models can effectively be constructed and used to estimate Sobol’ sensitivity indices of WWTP design parameters.},
  date      = {2018},
  day       = {01},
  doi       = {10.1016/B978-0-444-64241-7.50313-X},
  keywords  = {chemical engineering},
  piis      = {85050584125},
  url       = {http://dx.doi.org/10.1016/B978-0-444-64241-7.50313-X},
}

@InProceedings{Barbi2018,
  author    = {M. Barbi and H. M. Torun and M. Swaminathan and I. S. Stievano and F. G. Canavero and P. Besnier},
  title     = {{U}ncertainty quantification of {S}iP based integrated voltage regulator},
  booktitle = {2018 IEEE 22nd Workshop on Signal and Power Integrity, SPI 2018 - Proceedings},
  pages     = {1-4},
  month     = {June},
  abstract  = {\textcopyright  2018 IEEE. This paper deals with the uncertainty quantification applied to the analysis of Integrated Voltage Regulator (IVR) efficiency. It presents a meta-model based on a sparse polynomial chaos technique, aiming at estimating statistical quantities of a response with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained are validated against MC simulation.},
  date      = {2018},
  day       = {29},
  doi       = {10.1109/SaPIW.2018.8401677},
  keywords  = {electrical engineering, used UQLab},
  piis      = {85050460012},
  url       = {http://dx.doi.org/10.1109/SaPIW.2018.8401677},
}

@Article{Bdour2016,
  author   = {Tarek Bdour and Alain Reineix},
  title    = {{G}lobal {S}ensitivity {A}nalysis and {U}ncertainty {Q}uantification of {R}adiated {S}usceptibility in {P}CB {U}sing {N}onintrusive {P}olynomial {C}haos {E}xpansions},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  pages    = {939-942},
  month    = {June},
  abstract = {\textcopyright  2016 IEEE. In this paper, we introduce a workflow that uses uncertainty quantification (UQ) and sensitivity analysis (SA) in conjunction with a metamodeling technique called polynomial chaos expansion (PCE) to investigate the stochastic terminal response of a printed circuit board (PCB) due to an external plane wave excitation. The objective of PCE use is to reduce the computational burden of commonly used Monte Carlo (MC) approach for calculating statistical moments and Sobol\textquotesingle  sensitivity indices in the presence of uncertain geometrical and electrical parameters of the investigated PCB. The statistical results computed by PCE has been shown to be very accurate compared to MC simulation results.},
  date     = {2016},
  day      = {01},
  doi      = {10.1109/TEMC.2016.2535266},
  keywords = {electrical engineering, used UQLab},
  piis     = {84979468030},
  url      = {http://dx.doi.org/10.1109/TEMC.2016.2535266},
}

@InProceedings{Bilicz2016,
  author    = {S{\'a}ndor Bilicz and Szabolcs Gyim{\'o}thy and J{\'o}zsef P{\'a}v{\'o} and P{\'e}ter Horv{\'a}th and K{\'a}roly Mar{\'a}k},
  title     = {{U}ncertainty quantification of wireless power transfer systems},
  booktitle = {2016 IEEE Wireless Power Transfer Conference, WPTC 2016},
  month     = {June},
  abstract  = {\textcopyright  2016 IEEE.In this paper, the uncertainty of electric properties of wireless power transfer systems due to uncertain geometric design parameters is analyzed. An electromagnetic simulation tool, based on an integral formulation, is coupled with a stochastic method that quantitatively determines the contribution of each uncertain design variable to the output uncertainty in terms of Sobol\textquotesingle  indices. A generalized polynomial chaos expansion - being a state-of-art surrogate modeling method - is used to reduce the computational cost involved by these stochastic simulations. The performance of the proposed method is illustrated via the analysis of a resonant wireless power transfer chain.},
  date      = {2016},
  day       = {23},
  doi       = {10.1109/WPT.2016.7498861},
  keywords  = {electrical engineering, used UQLab},
  piis      = {84979574638},
  url       = {http://dx.doi.org/10.1109/WPT.2016.7498861},
}

@Article{Borgonovo2017,
  author   = {E. Borgonovo and X. Lu and E. Plischke and O. Rakovec and M. C. Hill},
  title    = {{M}aking the most out of a hydrological model data set: {S}ensitivity analyses to open the model black-box},
  journal  = {Water Resources Research},
  pages    = {7933-7950},
  month    = {September},
  abstract = {\textcopyright  2017. American Geophysical Union. All Rights Reserved.In this work, we investigate methods for gaining greater insight from hydrological model runs conducted for uncertainty quantification and model differentiation. We frame the sensitivity analysis questions in terms of the main purposes of sensitivity analysis: parameter prioritization, trend identification, and interaction quantification. For parameter prioritization, we consider variance-based sensitivity measures, sensitivity indices based on the L1-norm, the Kuiper metric, and the sensitivity indices of the DELSA methods. For trend identification, we investigate insights derived from graphing the one-way ANOVA sensitivity functions, the recently introduced CUSUNORO plots, and derivative scatterplots. For interaction quantification, we consider information delivered by variance-based sensitivity indices. We rely on the so-called given-data principle, in which results from a set of model runs are used to perform a defined set of analyses. One avoids using specific designs for each insight, thus controlling the computational burden. The methodology is applied to a hydrological model of a river in Belgium simulated using the well-established Framework for Understanding Structural Errors (FUSE) on five alternative configurations. The findings show that the integration of the chosen methods provides insights unavailable in most other analyses.},
  date     = {2017},
  day      = {01},
  doi      = {10.1002/2017WR020767},
  keywords = {hydrology},
  piis     = {85030105143},
  url      = {http://dx.doi.org/10.1002/2017WR020767},
}

@InProceedings{Capellari2016,
  author    = {Giovanni Capellari and Eleni Chatzi and Stefano Mariani},
  title     = {{A}n optimal sensor placement method for {S}HM based on {B}ayesian experimental design and {P}olynomial {C}haos {E}xpansion},
  booktitle = {ECCOMAS Congress 2016 - Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering},
  pages     = {6272-6282},
  month     = {January},
  abstract  = {We present an optimal sensor placement methodology for structural health monitoring (SHM) purposes, relying on a Bayesian experimental design approach. The unknown structural properties, e.g. the residual strength and stiffness, are inferred from data collected through a network of sensors, whose architecture, i.e., type and position may largely affect the accuracy of the monitoring system. In tackling this issue, an optimal network configuration is herein sought by maximizing the expected information gain between prior and posterior probability distributions of the parameters to be estimated. Since the objective function linked to the network topology cannot be analytically computed, a numerical approximation is provided by means of a Monte Carlo analysis, wherein each realization is obtained via finite element modeling. Since the computational burden linked to this procedure often grows infeasible, a Polynomial Chaos Expansion (PCE) approach is adopted for accelerating the computation of the forward problem. The analysis expands over joint samples covering both structural state and design variables, i.e., sensor locations. Via increase of the number of deployed sensors in the network, the optimization procedure soon turns computationally costly due to the curse of dimensionality. To this end, a stochastic optimization method is adopted for accelerating the convergence of the optimization process and thereby the damage detection capability of the SHM system. The proposed method is applied to thin flexible structures, and the resulting optimal sensor configuration is shown. The effects of the number of training samples, the polynomial degree of the approximation expansion and the optimization settings are also discussed.},
  date      = {2016},
  day       = {01},
  doi       = {10.7712/100016.2257.6762},
  keywords  = {sensors engineering, used UQLab},
  piis      = {84995471069},
  url       = {http://dx.doi.org/10.7712/100016.2257.6762},
}

@Article{Chen2017,
  author   = {Cheng Chen and Weijie Xu and Tong Guo and Kai Chen},
  title    = {{A}nalysis of actuator delay and its effect on uncertainty quantification for real-time hybrid simulation},
  journal  = {Earthquake Engineering and Engineering Vibration},
  pages    = {713-725},
  month    = {October},
  abstract = {\textcopyright  2017, Institute of Engineering Mechanics, China Earthquake Administration and Springer-Verlag GmbH Germany. Uncertainties in structure properties can result in different responses in hybrid simulations. Quantification of the effect of these uncertainties would enable researchers to estimate the variances of structural responses observed from experiments. This poses challenges for real-time hybrid simulation (RTHS) due to the existence of actuator delay. Polynomial chaos expansion (PCE) projects the model outputs on a basis of orthogonal stochastic polynomials to account for influences of model uncertainties. In this paper, PCE is utilized to evaluate effect of actuator delay on the maximum displacement from real-time hybrid simulation of a single degree of freedom (SDOF) structure when accounting for uncertainties in structural properties. The PCE is first applied for RTHS without delay to determine the order of PCE, the number of sample points as well as the method for coefficients calculation. The PCE is then applied to RTHS with actuator delay. The mean, variance and Sobol indices are compared and discussed to evaluate the effects of actuator delay on uncertainty quantification for RTHS. Results show that the mean and the variance of the maximum displacement increase linearly and exponentially with respect to actuator delay, respectively. Sensitivity analysis through Sobol indices also indicates the influence of the single random variable decreases while the coupling effect increases with the increase of actuator delay.},
  date     = {2017},
  day      = {01},
  doi      = {10.1007/s11803-017-0409-6},
  keywords = {civil engineering, used UQLab},
  piis     = {85033482190},
  url      = {http://dx.doi.org/10.1007/s11803-017-0409-6},
}

@Article{Cheng2018,
  author   = {Kai Cheng and Zhenzhou Lu},
  title    = {{S}parse polynomial chaos expansion based on {D}-M{O}RP{H} regression},
  journal  = {Applied Mathematics and Computation},
  pages    = {17-30},
  month    = {April},
  abstract = {\textcopyright  2017 Elsevier Inc. Polynomial chaos expansion (PCE) is widely used by engineers and modelers in various engineering fields for uncertainty analysis. The computational cost of full PCE is unaffordable for the \textquotedblleft curse of dimensionality\textquotedblright  of the expansion coefficients. In this paper, a new method for developing sparse PCE is proposed based on the diffeomorphic modulation under observable response preserving homotopy (D-MORPH) algorithm. D-MORPH is a regression technique, it can construct the full PCE models with model evaluations much less than the unknown coefficients. This technique determines the unknown coefficients by minimizing the least-squared error and an objective function. For the purpose of developing sparse PCE, an iterative reweighted algorithm is proposed to construct the objective function. As a result, the objective in D-MORPH regression is converted to minimize the \mathscr{l}1 norm of PCE coefficients, and the sparse PCE is established after the proposed algorithm converges to the optimal value. To validate the performance of the developed methodology, several benchmark examples are investigated. The accuracy and efficiency are compared to the well-established least angle regression (LAR) sparse PCE, and results show that the developed method is superior to the LAR-based sparse PCE in terms of efficiency and accuracy.},
  date     = {2018},
  day      = {15},
  doi      = {10.1016/j.amc.2017.11.044},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85037524488},
  url      = {http://dx.doi.org/10.1016/j.amc.2017.11.044},
}

@Article{Chiaramello2017,
  author   = {E. Chiaramello and S. Fiocchi and P. Ravazzani and M. Parazzini},
  title    = {{S}tochastic {D}osimetry for the {A}ssessment of {C}hildren {E}xposure to {U}niform 50 {H}z {M}agnetic {F}ield with {U}ncertain {O}rientation},
  journal  = {BioMed Research International},
  month    = {January},
  abstract = {\textcopyright  2017 E. Chiaramello et al. This study focused on the evaluation of the exposure of children aging from five to fourteen years to 50 Hz homogenous magnetic field uncertain orientation using stochastic dosimetry. Surrogate models allowed assessing how the variation of the orientation of the magnetic field influenced the induced electric field in each tissue of the central nervous system (CNS) and in the peripheral nervous system (PNS) of children. Results showed that the electric field induced in CNS and PNS tissues of children were within the ICNIRP basic restrictions for general public and that no significant difference was found in the level of exposure of children of different ages when considering 10000 possible orientations of the magnetic field. A "mean stochastic model," useful to estimate the level of exposure in each tissue of a representative child in the range of age from five to fourteen years, was developed. In conclusion, this study was useful to deepen knowledge about the ELF-MF exposure, including the evaluation of variable and uncertain conditions, thus representing a step towards a more realistic characterization of the exposure to EMF.},
  date     = {2017},
  day      = {01},
  doi      = {10.1155/2017/4672124},
  keywords = {biomedical science, used UQLab},
  piis     = {85042085356},
  url      = {http://dx.doi.org/10.1155/2017/4672124},
}

@Article{Colone2018,
  author   = {Lorenzo Colone and Anand Natarajan and Nikolay Dimitrov},
  title    = {{I}mpact of turbulence induced loads and wave kinematic models on fatigue reliability estimates of offshore wind turbine monopiles},
  journal  = {Ocean Engineering},
  pages    = {295-309},
  month    = {May},
  abstract = {The cost of offshore wind turbine substructures has a significant impact on competitiveness of the wind energy market and is affected by conservative safety margins adopted in the design phase. This implies that an accurate design load prediction, especially of those resulting in fatigue damage accumulation, may help achieve more cost-effective solutions. In this article, the impact of turbulence and wave loads on fatigue reliability of pile foundations is investigated for a 5-MW offshore wind turbine. Loads obtained by varying turbulence percentiles are compared with those obtained from the full joint probability distribution of wind speed and turbulence through Monte Carlo (MC) simulations, and from the equivalent turbulence level currently adopted by IEC standards. The analyses demonstrate that a lower equivalent turbulence percentile leads to a more realistic and less conservative estimation of fatigue loads. Subsequently, the research focuses on studying the effects of uncertain marine environments on the fatigue load distribution, showing that the latter is insensitive to the random variability of the hydrodynamic coefficients. With respect to the wave kinematic model, a comparison between nonlinear and linear waves clearly suggests that hydrodynamic forces depend significantly on the kinematic model adopted and the operational conditions of the turbine. Furthermore, a term is derived to correct the error introduced by Wheeler stretching at finite water depths. The respective model uncertainties that originate from the nonlinear irregular wave model and Wheeler correction are quantified and employed in a reliability analysis. In a case study, the results are finally compared in terms of estimated probability of failure, with the aim to quantify the influence of environmental models on monopile reliability.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.oceaneng.2018.02.045},
  keywords = {ocean engineering, used UQLab},
  piis     = {85042905073},
  url      = {http://dx.doi.org/10.1016/j.oceaneng.2018.02.045},
}

@InProceedings{Dalisay2017,
  author    = {Jon Dewitt E Dalisay and Edwin N. Quiros},
  title     = {{U}ncertainty quantification of autoignition kinetics using sparse polynomial chaos},
  booktitle = {11th Asia-Pacific Conference on Combustion, ASPACC 2017},
  month     = {January},
  abstract  = {\textcopyright  2017 Combustion Institute. All rights reserved. Quantification of prediction uncertainties is requisite for the development of truly predictive combustion models. Parameter uncertainties, such as those associated with reaction rate coefficients, give rise to combustion quantities of interest, such as ignition delay, that are uncertain. In this work, we assessed the applicability of polynomial chaos expansions based on least angle regression (LAR PCE) [1] for the uncertainty propagation (UP) and global sensitivity analysis (SA) of autoignition kinetics. The UP applicability assessment was done on atmospheric, stoichiometric constant volume 0-D autoignition of methane over initial temperatures of 1000-1800 K, and the global SA applicability assessment was done on the same pressure and equivalence ratio conditions but only at an initial temperature of 1000 K. The chemical kinetic mechanism GRI-Mech 3.0 [2] was used for all simulations and direct Monte Carlo results served as references for comparison. We demonstrated the effectiveness of LAR PCE due to its ability to construct accurate sparse PCEs. Further, we carried out UP on a comprehensive range of operating conditions. Given our findings, we anticipate that LAR PCE will be used in the future for the UP and global SA of very large combustion kinetic models involving complex fuels.},
  date      = {2017},
  day       = {01},
  piis      = {85046487994},
}

@InProceedings{Du2017,
  author    = {Jinxin Du and Christophe Roblin},
  title     = {{S}tatistical modeling of the reflection coefficient of deformable antennas},
  booktitle = {2017 11th European Conference on Antennas and Propagation, EUCAP 2017},
  pages     = {1928-1932},
  month     = {May},
  abstract  = {\textcopyright  2017 Euraap. A modeling methodology is proposed for characterizing the reflection coefficient S11(f) of narrow band antennas undergoing random disturbances. Firstly, identification techniques are used to get a parsimonious representation of the S11; then the Polynomial Chaos Expansion (PCE) method is used to characterize quantitatively the influence of random disturbances on the compressed S11. The derived S11 model can be used as efficient surrogate for statistical analysis of antennas\textquotesingle  frequency behavior. We have applied the proposed methodology to two narrow band antennas - a deformable dipole and a textile patch - in order to demonstrate its performance. Models with good accuracy have been derived for both cases.},
  date      = {2017},
  day       = {15},
  doi       = {10.23919/EuCAP.2017.7928373},
  keywords  = {electrical engineering, used UQLab},
  piis      = {85020204287},
  url       = {http://dx.doi.org/10.23919/EuCAP.2017.7928373},
}

@Article{Du2017a,
  author   = {Jinxin Du and Christophe Roblin},
  title    = {{S}tatistical {M}odeling of {D}isturbed {A}ntennas {B}ased on the {P}olynomial {C}haos {E}xpansion},
  journal  = {IEEE Antennas and Wireless Propagation Letters},
  pages    = {1843-1846},
  month    = {January},
  abstract = {\textcopyright  2016 IEEE. A new methodology of statistical modeling of the far field (FF) radiated by antennas undergoing random disturbances is presented. First, the radiated FF is transformed into a parsimonious form using the spherical modes expansion method (SMEM); then, a surrogate model relating the parsimonious field with the input random parameters is constructed using the polynomial chaos expansion method (PCEM). The combination of the SMEM and PCEM allows developing a compact and precise model with a minimized experimental design cost. The obtained model is computationally costless for generating statistical samples of disturbed antennas easily usable as surrogate models in various types of analyses. In order to demonstrate its performance, the proposed methodology is validated with a deformable canonical antenna-A dipole undergoing three independent random deformations (stretching, bending, and torsion), deriving a compact and precise surrogate model.},
  date     = {2017},
  day      = {01},
  doi      = {10.1109/LAWP.2016.2609739},
  keywords = {electrical engineering, used UQLab},
  piis     = {85024483492},
  url      = {http://dx.doi.org/10.1109/LAWP.2016.2609739},
}

@Article{Dutta2018,
  author   = {Subhrajit Dutta and Siddhartha Ghosh and Mandar M. Inamdar},
  title    = {{O}ptimisation of tensile membrane structures under uncertain wind loads using {P}CE and kriging based metamodels},
  journal  = {Structural and Multidisciplinary Optimization},
  pages    = {1149-1161},
  month    = {March},
  abstract = {\textcopyright  2017, Springer-Verlag GmbH Germany.Tensile membrane structures (TMS) are light-weight flexible structures that are designed to span long distances with structural efficiency. The stability of a TMS is jeopardised under heavy wind forces due to its inherent flexibility and inability to carry out-of-plane moment and shear. A stable TMS under uncertain wind loads (without any tearing failure) can only be achieved by a proper choice of the initial prestress. In this work, a double-loop reliability-based design optimisation (RBDO) of TMS under uncertain wind load is proposed. Using a sequential polynomial chaos expansion (PCE) and kriging based metamodel, this RBDO reduces the cost of inner-loop reliability analysis involving an intensive finite element solver. The proposed general approach is applied to the RBDO of two benchmark TMS and its computational efficiency is demonstrated through these case studies. The method developed here is suggested for RBDO of large and complex engineering systems requiring costly numerical solution.},
  date     = {2018},
  day      = {01},
  doi      = {10.1007/s00158-017-1802-5},
  keywords = {civil engineering, used UQLab},
  piis     = {85029009303},
  url      = {http://dx.doi.org/10.1007/s00158-017-1802-5},
}

@Article{Erten2016,
  author   = {Esra Erten and Juan M. Lopez-Sanchez and Onur Yuzugullu and Irena Hajnsek},
  title    = {{R}etrieval of agricultural crop height from space: {A} comparison of {SAR} techniques},
  journal  = {Remote Sensing of Environment},
  pages    = {130-144},
  month    = {December},
  abstract = {\textcopyright  2016 Elsevier Inc.This paper deals with the retrieval of agricultural crop height from space by using multipolarization Synthetic Aperture Radar (SAR) images. Coherent and incoherent crop height estimation methods are discussed for the first time with a unique TanDEM-X dataset acquired over rice cultivation areas. Indeed, with its polarimetric and interferometric capabilities, the TanDEM-X mission enables the tracking of crop height through interferometric SAR (InSAR), polarimetric interferometric SAR (PolInSAR) and the inversion of radiative transfer-based backscattering model. The paper evaluates the three aforementioned techniques simultaneously with a data set acquired in September 2014 and 2015 over rice fields in Turkey during their reproductive stage. The assessment of the absolute height accuracy and the limitations of the approaches are provided. In-situ measurements conducted in the same cultivation periods are used for validation purposes. The PolInSAR and morphological backscattering model results showed better performance with low RMSEs (12 and 13 cm) compared to the differential InSAR result having RMSE of 18 cm. The spatial baseline, i.e. the distance between satellites, is a key parameter for coherent methods such as InSAR and PolInSAR. Its effect on the absolute height accuracy is discussed using TanDEM-X pairs separated by a baseline of 101.7m and 932m. Although the InSAR based approach is demonstrated to provide sufficient crop height accuracy, the availability of a precise vegetation-free digital elevation model and a structurally dense crop are basic requirements for achieving high accuracy. The PolInSAR approach provides reliable crop height estimation if the spatial baseline is large enough for the inversion. The impact of increasing spatial baseline on the absolute accuracy of the crop height estimation is evident for both methods. However, PolInSAR is more cost-efficient, e.g. there is no need for phase unwrapping and any external vegetation free surface elevation data. Instead, the usage of radiative transfer based backscattering models provides not only crop height but also other biophysical properties of the crops with consistent accuracy. The efficient retrieval of crop height with backscattering model is achieved by metamodelling, which makes the computational cost of backscattering inversion comparable to the ones of the coherent methods. However, effectiveness depends on not only the backscattering model, but also the integration of agronomic crop growth rules. Motivated by these results, a combination of backscattering and PolInSAR inversion models would provide a successful method of future precision farming studies.},
  date     = {2016},
  day      = {15},
  doi      = {10.1016/j.rse.2016.10.007},
  keywords = {sensors engineering, used UQLab},
  piis     = {84991593765},
  url      = {http://dx.doi.org/10.1016/j.rse.2016.10.007},
}

@InProceedings{Erten2016a,
  author    = {Esra Erten and Onur Yuzugullu and Juan M. Lopez-Sanchez and Irena Hajnsek},
  title     = {{SAR} algorithms for crop height estimation: {T}he paddy-rice case study},
  booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
  pages     = {7117-7120},
  month     = {November},
  abstract  = {\textcopyright  2016 IEEE.This paper presents a study of the sensibility of the incoherent (electromagnetic backscattering model) and coherent (DInSAR and PolInSAR inversion) crop height estimation methods of SAR imaging. The methods were compared for paddy-rice crop height monitoring with a TanDEM-X dataset. For this, rice-cultivated agricultural fields located in Northern Turkey were selected. Intensive ground data collection during the cultivation period in 2015 was carried out. The accuracy analysis showed that the requirement of external (vegetation-free) DEM in DInSAR-based crop height estimation decreases its performance compared to the PolInSAR and backscattering inversion methods.},
  date      = {2016},
  day       = {01},
  doi       = {10.1109/IGARSS.2016.7730857},
  keywords  = {sensors engineering, used UQLab},
  piis      = {85007425666},
  url       = {http://dx.doi.org/10.1109/IGARSS.2016.7730857},
}

@Article{Fengjie2018,
  author   = {Tan Fengjie and Tom Lahmer},
  title    = {{S}hape optimization based design of arch-type dams under uncertainties},
  journal  = {Engineering Optimization},
  pages    = {1470-1482},
  month    = {September},
  abstract = {Comparing existing design methodologies for arch-type dams, model-based shape optimization can effectively reduce construction costs and leverage the properties of construction materials. To apply means of shape optimization, suitable variables need to be chosen to formulate the objective function, which is here the volume of the arch dam. A genetic algorithm is adopted as the optimization method, which allows a global search. The reliability index is considered as the main constraint. Its computation is realized by adaptive Kriging Monte Carlo simulation, which visibly increases the analysis efficiency compared with traditional Monte Carlo simulations. Constraints, such as the reliability index and further with respect to the geometry, are taken into consideration by a penalty formulation. By means of this approach, a reliability-based design can be found which ensures both the safety and serviceability of a newly designed arch-type dam.},
  date     = {2018},
  day      = {02},
  doi      = {10.1080/0305215X.2017.1409348},
  keywords = {civil engineering, used UQLab},
  piis     = {85038396163},
  url      = {http://dx.doi.org/10.1080/0305215X.2017.1409348},
}

@InProceedings{Gaspar2016,
  author    = {B. Gaspar and C. Guedes Soares and E. Bahmyari and M. Reza Khedmati},
  title     = {{A}pplication of polynomial chaos expansions in stochastic analysis of plate elements under lateral pressure},
  booktitle = {Proceedings of 3rd International Conference on Maritime Technology and Engineering, MARTECH 2016},
  pages     = {471-480},
  month     = {January},
  abstract  = {\textcopyright  2016 Taylor \& Francis Group, London. An application of the polynomial chaos expansion technique in the stochastic analysis of plate elements under lateral pressure is presented. An adaptive non-intrusive technique based on regression is adopted to define sparse polynomial chaos expansion representations. The analysis is performed considering as response quantity of interest the maximum deflection of a steel rectangular plate element under lateral pressure representative of a ship bottom plate element. The material modulus of elasticity is considered to be a spatially varying property represented by a random field. The plate numerical model is based on the classical theory of thin plates, which is solved numerically using the element free Galerkin method. The accuracy and efficiency of the adaptive non-intrusive sparse polynomial chaos expansion technique adopted is demonstrated in the paper for different correlation lengths of the random field.},
  date      = {2016},
  day       = {01},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016740809},
}

@InProceedings{Gaspar2016a,
  author    = {B. Gaspar and A. P. Teixeira and C. Guedes Soares},
  title     = {{S}ensitivity analysis of the {I}AC{S}-C{S}R buckling strength requirements for stiffened panels},
  booktitle = {Proceedings of 3rd International Conference on Maritime Technology and Engineering, MARTECH 2016},
  pages     = {459-470},
  month     = {January},
  abstract  = {\textcopyright  2016 Taylor \& Francis Group, London. Asensitivity analysis of the IACS-CSR buckling strength requirements for stiffened panels under uniaxial compression is presented. The buckling strength requirements considered are defined on the basis of semi-empirical design formulations that account explicitly for three failure modes of the stiffened panel: uniaxial buckling of the plating between stiffeners, column buckling of stiffeners with attached plating and lateral-torsional buckling or tripping of stiffeners. A sample of deck stiffened panels obtained from a representative sample of double hull oil tanker designs is considered in the sensitivity analysis. Different local and global sensitivity analysis methods are used to compute sensitivity indices for the different stiffened panel designs and failure modes. The most important variables with respect to the contribution to the critical buckling stress variability are identified and the different local and global sensitivity indices compared.},
  date      = {2016},
  day       = {01},
  keywords  = {ocean engineering, used UQLab},
  piis      = {85016821176},
}

@Article{Hamdi2017,
  author   = {Hamidreza Hamdi and Ivo Couckuyt and Mario Costa Sousa and Tom Dhaene},
  title    = {{G}aussian {P}rocesses for history-matching: application to an unconventional gas reservoir},
  journal  = {Computational Geosciences},
  pages    = {267-287},
  month    = {April},
  abstract = {\textcopyright  2017, Springer International Publishing Switzerland.The process of reservoir history-matching is a costly task. Many available history-matching algorithms either fail to perform such a task or they require a large number of simulation runs. To overcome such struggles, we apply the Gaussian Process (GP) modeling technique to approximate the costly objective functions and to expedite finding the global optima. A GP model is a proxy, which is employed to model the input-output relationships by assuming a multi-Gaussian distribution on the output values. An infill criterion is used in conjunction with a GP model to help sequentially add the samples with potentially lower outputs. The IC fault model is used to compare the efficiency of GP-based optimization method with other typical optimization methods for minimizing the objective function. In this paper, we present the applicability of using a GP modeling approach for reservoir history-matching problems, which is exemplified by numerical analysis of production data from a horizontal multi-stage fractured tight gas condensate well. The results for the case that is studied here show a quick convergence to the lowest objective values in less than 100 simulations for this 20-dimensional problem. This amounts to an almost 10 times faster performance compared to the Differential Evolution (DE) algorithm that is also known to be a powerful optimization technique. The sensitivities are conducted to explain the performance of the GP-based optimization technique with various correlation functions.},
  date     = {2017},
  day      = {01},
  doi      = {10.1007/s10596-016-9611-2},
  keywords = {geoscience, used UQLab},
  piis     = {85009932344},
  url      = {http://dx.doi.org/10.1007/s10596-016-9611-2},
}

@Article{Hariri-Ardebili2018,
  author   = {Mohammad Amin Hariri-Ardebili and Farhad Pourkamali-Anaraki},
  title    = {{S}implified reliability analysis of multi hazard risk in gravity dams via machine learning techniques},
  journal  = {Archives of Civil and Mechanical Engineering},
  pages    = {592-610},
  month    = {February},
  abstract = {\textcopyright  2017 Politechnika Wroc{\l}awska Deterministic analysis does not provide a comprehensive model for concrete dam response under multi-hazard risk. Thus, the use of probabilistic approach is usually recommended which is problematic due to high computational demand. This paper presents a simplified reliability analysis framework for gravity dams subjected to flooding, earthquakes, and aging. A group of time-variant degradation models are proposed for different random variables. Response of the dam is presented by explicit limit state functions. The probability of failure is directly computed by either classical Monte Carlo simulation or the refined importance sampling technique. Next, three machine learning techniques (i.e., K-nearest neighbor, support vector machine, and naive Bayes classifier) are adopted for binary classification of the structural results. These methods are then demonstrated in terms of accuracy, applicability and computational time for prediction of the failure probability. Results are then generalized for different dam classes (based on the height-to-width ratio), various water levels, earthquake intensity, degradation rate, and cross-correlation between the random variables. Finally, a sigmoid-type function is proposed for analytical calculation of the failure probability for different classes of gravity dams. This function is then specialized for the hydrological hazard and the failure surface is presented as a direct function of the dam\textquotesingle s height and width.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.acme.2017.09.003},
  keywords = {civil engineering, used UQLab},
  piis     = {85033218669},
  url      = {http://dx.doi.org/10.1016/j.acme.2017.09.003},
}

@InProceedings{Hashemian2017,
  author    = {Raoufehsadat Hashemian and Niklas Carlsson and Diwakar Krishnamurthy and Martin Arlitt},
  title     = {{I}ris: {I}te{R}ative and {I}ntelligent {E}xperiment {S}election},
  booktitle = {ICPE 2017 - Proceedings of the 2017 ACM/SPEC International Conference on Performance Engineering},
  pages     = {143-154},
  month     = {April},
  abstract  = {\textcopyright  2017 ACM. Benchmarking is a widely-used technique to quantify the performance of software systems. However, the design and implementation of a benchmarking study can face several challenges. In particular, the time required to perform a benchmarking study can quickly spiral out of control, owing to the number of distinct variables to systematically examine. In this paper, we propose IRIS, an IteRative and Intelligent Experiment Selection methodology, to maximize the information gain while minimizing the duration of the benchmarking process. IRIS selects the region to place the next experiment point based on the variability of both dependent, i.e., response, and independent variables in that region. It aims to identify a performance function that minimizes the response variable prediction error for a constant and limited experimentation budget. We evaluate IRIS for a wide selection of experimental, simulated and synthetic systems with one, two and three independent variables. Considering a limited experimentation budget, the results show IRIS is able to reduce the performance function prediction error up to 4:3 times compared to equal distance experiment point selection. Moreover, we show that the error reduction can further improve through system-specific parameter tuning. Analysis of the error distributions obtained with IRIS reveals that the technique is particularly effective in regions where the response variable is sensitive to changes in the independent variables.},
  date      = {2017},
  day       = {17},
  doi       = {10.1145/3030207.3030225},
  keywords  = {computational science and engineering, used UQLab},
  piis      = {85019034357},
  url       = {http://dx.doi.org/10.1145/3030207.3030225},
}

@Article{Kaintura2017,
  author   = {A. Kaintura and D. Spina and I. Couckuyt and L. Knockaert and W. Bogaerts and T. Dhaene},
  title    = {{A} Kriging and {S}tochastic {C}ollocation ensemble for uncertainty quantification in engineering applications},
  journal  = {Engineering with Computers},
  pages    = {935-949},
  month    = {October},
  abstract = {\textcopyright  2017, Springer-Verlag London.We propose a new surrogate modeling approach by combining two non-intrusive techniques: Kriging and Stochastic Collocation. The proposed method relies on building a sufficiently accurate Stochastic Collocation model which acts as a basis to construct a Kriging model on the residuals, to combine the accuracy and efficiency of Stochastic Collocation methods in describing stochastic quantities with the flexibility and modeling power of Kriging-based approaches. We investigate and compare performance of the proposed approach with state-of-art techniques over benchmark problems and practical engineering examples on various experimental designs.},
  date     = {2017},
  day      = {01},
  doi      = {10.1007/s00366-017-0507-0},
  keywords = {computational science and engineering, used UQLab},
  piis     = {85015702883},
  url      = {http://dx.doi.org/10.1007/s00366-017-0507-0},
}

@InProceedings{Larbi2017,
  author    = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besniert},
  title     = {{C}rosstalk analysis of printed circuits with many uncertain parameters using sparse polynomial chaos metamodels},
  booktitle = {2017 International Symposium on Electromagnetic Compatibility - EMC EUROPE 2017, EMC Europe 2017},
  month     = {November},
  abstract  = {\textcopyright  2017 IEEE. This paper presents a metamodel based on the sparse polynomial chaos approach, well adapted to high-dimensional uncertainty quantification problems, applied for the analysis of crosstalk in printed circuit board microstrip traces. It enables to estimate, with a low computational cost compared to Monte Carlo (MC) simulation, statistical quantities and provides a sensitivity analysis of the crosstalk effects considering numerous uncertain variables. The approach is validated against MC simulation and shows a good efficiency and accuracy.},
  date      = {2017},
  day       = {02},
  doi       = {10.1109/EMCEurope.2017.8094623},
  keywords  = {electrical engineering, used UQLab},
  piis      = {85040624369},
  url       = {http://dx.doi.org/10.1109/EMCEurope.2017.8094623},
}

@InProceedings{Larbi2017a,
  author    = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title     = {{A}nalysis of a printed circuit board with many uncertain variables by sparse polynomial chaos},
  booktitle = {2017 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization for RF, Microwave, and Terahertz Applications, NEMO 2017},
  pages     = {323-325},
  month     = {June},
  abstract  = {\textcopyright  2017 IEEE. This communication deals with the uncertainty quantification in high dimensional problems. It introduces a metamodel based on the sparse polynomial chaos for the analysis of a printed circuit board, depending on many uncertain variables. This metamodel allows to estimate statistical quantities of an output with a relative low computational cost compared to Monte Carlo (MC) simulation. Results obtained have been validated by comparison with MC simulation.},
  date      = {2017},
  day       = {29},
  doi       = {10.1109/NEMO.2017.7964274},
  keywords  = {electrical engineering, used UQLab},
  piis      = {85028511104},
  url       = {http://dx.doi.org/10.1109/NEMO.2017.7964274},
}

@Article{Larbi2018,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {{V}ariability {I}mpact of {M}any {D}esign {P}arameters: {T}he {C}ase of a {R}ealistic {E}lectronic {L}ink},
  journal  = {IEEE Transactions on Electromagnetic Compatibility},
  pages    = {34-41},
  month    = {February},
  abstract = {\textcopyright  1964-2012 IEEE. In this paper, we adopt the so-called sparse polynomial chaos metamodel for the uncertainty quantification in the framework of high-dimensional problems. This metamodel is used to model a realistic electronic bus structure with a large number of uncertain input parameters such as those related to microstrip line geometries. It aims at estimating quantities of interest, such as statistical moments, probability density functions, and provides sensitivity analysis of a response. It drastically reduces the model computational cost with regard to brute force Monte Carlo (MC) simulation. The method presents a good performance and is validated in comparison with MC simulation.},
  date     = {2018},
  day      = {01},
  doi      = {10.1109/TEMC.2017.2727961},
  keywords = {electrical engineering, used UQLab},
  piis     = {85028942396},
  url      = {http://dx.doi.org/10.1109/TEMC.2017.2727961},
}

@Article{Larbi2018a,
  author   = {Mourad Larbi and Igor S. Stievano and Flavio G. Canavero and Philippe Besnier},
  title    = {{I}dentification of main factors of uncertainty in a microstrip line network},
  journal  = {Progress in Electromagnetics Research},
  pages    = {61-72},
  month    = {January},
  abstract = {\textcopyright  2018, Electromagnetics Academy. All rights reserved. This paper deals with uncertainty propagation applied to the analysis of crosstalk in printed circuit board microstrip traces. Complex interconnection networks generally are affected by many uncertain parameters and their point-to-point transfer functions are computationally expensive, thus making Monte-Carlo analyses rather inefficient. To overcome this situation, a metamodel is highly desirable. This paper presents a sparse and accelerated polynomial chaos approach, which proves to be well adapted for high-dimensional uncertainty quantification and well suited for the sensitivity analysis of crosstalk effects. We highlight the significant advantage of the advocated approach for the design of microstrip line networks of complex topology. In fact, we demonstrate how a small number of system simulations can help to quantify the statistics of the output variability and identify a reduced set of high-impact parameters.},
  date     = {2018},
  day      = {01},
  keywords = {electrical engineering, used UQLab},
  piis     = {85049190395},
}

@InProceedings{Li2017,
  author    = {Meng Li and Yinghong Wen and Jinbao Zhang and Dan Zhang},
  title     = {{A}n {E}MC safety assessment model to analyze complex system in high speed railways},
  booktitle = {Proceedings of the 2017 19th International Conference on Electromagnetics in Advanced Applications, ICEAA 2017},
  pages     = {626-629},
  month     = {October},
  abstract  = {\textcopyright  2017 IEEE.This paper presents a safety assessment model with regard to electromagnetic compatibility (EMC) fault suitable for estimating the EMC failure probability (EMC FP) of the complex system facing uncertain electromagnetic environment. First, the model is successfully applied to decompose the complex system into different independent subsystem and equipment. Then the practical EMI scenario is modeled as different typical EMC problem, such as the Plane Wave Coupling and Crosstalk respectively. After that the fault tree analysis (FTA) is proposed to use in combination with electromagnetic topology (EMT) calculating the EMC FP of the system. A focus is put on the practical case of the Desktop Management Interface (DMI) system. In this context, a system-level EMC safety assessment model has been applied to solve EMC FP by uncertainty analysis in high speed railways with the complex electromagnetic environment.},
  date      = {2017},
  day       = {11},
  doi       = {10.1109/ICEAA.2017.8065324},
  keywords  = {electrical engineering, used UQLab},
  piis      = {85035133738},
  url       = {http://dx.doi.org/10.1109/ICEAA.2017.8065324},
}

@Article{Mara2017,
  author   = {Thierry A. Mara and Benjamin Belfort and Vincent Fontaine and Anis Younes},
  title    = {{A}ddressing factors fixing setting from given data: {A} comparison of different methods},
  journal  = {Environmental Modelling and Software},
  pages    = {29-38},
  month    = {January},
  abstract = {{\"\i}\textquestiondown \textonehalf  2016 This paper deals with global sensitivity analysis of computer model output. Given an independent input sample and associated model output vector with possibly the vector of output derivatives with respect to the input variables, we show that it is possible to evaluate the following global sensitivity measures: (i) the Sobol’ indices, (ii) the Borgonovo\textquotesingle s density-based sensitivity measure, and (iii) the derivative-based global sensitivity measure of Sobol’ and Kucherenko. We compare the efficiency of the different methods to address factors fixing setting, an important issue in global sensitivity analysis. First, global sensitivity analysis of the Ishigami function is performed with the different methods. Then, they are applied to two different responses of a soil drainage model. The results show that the polynomial chaos expansion for estimating Sobol’ indices is the most efficient approach.},
  date     = {2017},
  day      = {01},
  doi      = {10.1016/j.envsoft.2016.10.004},
  keywords = {computational science and engineering},
  piis     = {84994027682},
  url      = {http://dx.doi.org/10.1016/j.envsoft.2016.10.004},
}

@InProceedings{Mentani2016,
  author    = {Alessio Mentani and Laura Govoni and Guido Gottardi and St{\'e}phane Lambert and Franck Bourrier and David Toe},
  title     = {{A} New {A}pproach to {E}valuate the {E}ffectiveness of {R}ockfall {B}arriers},
  booktitle = {Procedia Engineering},
  pages     = {398-403},
  month     = {January},
  abstract  = {\textcopyright  2016 The Authors. The paper addresses the response of a semi-rigid rockfall protection barrier using numerical models. The study show a large dependence of the barrier response to the impact conditions. The block size and impact position, rather than the velocity direction and magnitude induce different modes of failure of the fence, which in turn result in different values of failure energy. As a result, the barrier capacity cannot be established in a deterministic way. The effectiveness of structures as such can be more successfully evaluated through a reliability probabilistic approach. Results can be used to create a meta-model of the barrier response which can be incorporated into rockfall simulation models, enabling a reliable and comprehensive design of rockfall mitigation interventions performed with this type of structure.},
  date      = {2016},
  day       = {01},
  doi       = {10.1016/j.proeng.2016.08.462},
  keywords  = {geomechanics, used UQLab},
  piis      = {84988353699},
  url       = {http://dx.doi.org/10.1016/j.proeng.2016.08.462},
}

@Article{Murcia2018,
  author   = {Juan Pablo Murcia and Pierre-Elouan R{\'e}thor{\'e} and Nikolay Dimitrov and Anand Natarajan and John Dalsgaard S{\o}rensen and Peter Graf and Taeseong Kim},
  title    = {{U}ncertainty propagation through an aeroelastic wind turbine model using polynomial surrogates},
  journal  = {Renewable Energy},
  pages    = {910-922},
  month    = {April},
  abstract = {\textcopyright  2017 Polynomial surrogates are used to characterize the energy production and lifetime equivalent fatigue loads for different components of the DTU 10 MW reference wind turbine under realistic atmospheric conditions. The variability caused by different turbulent inflow fields are captured by creating independent surrogates for the mean and standard deviation of each output with respect to the inflow realizations. A global sensitivity analysis shows that the turbulent inflow realization has a bigger impact on the total distribution of equivalent fatigue loads than the shear coefficient or yaw miss-alignment. The methodology presented extends the deterministic power and thrust coefficient curves to uncertainty models and adds new variables like damage equivalent fatigue loads in different components of the turbine. These surrogate models can then be implemented inside other work-flows such as: estimation of the uncertainty in annual energy production due to wind resource variability and/or robust wind power plant layout optimization. It can be concluded that it is possible to capture the global behavior of a modern wind turbine and its uncertainty under realistic inflow conditions using polynomial response surfaces. The surrogates are a way to obtain power and load estimation under site specific characteristics without sharing the proprietary aeroelastic design.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.renene.2017.07.070},
  keywords = {energy engineering},
  piis     = {85032993803},
  url      = {http://dx.doi.org/10.1016/j.renene.2017.07.070},
}

@InProceedings{Mylonas2017,
  author    = {Charilaos Mylonas and Bemetz Valentin and Eleni Chatzi},
  title     = {{M}utliscale surrogate modeling and uncertainty quantification for periodic composite structures},
  booktitle = {UNCECOMP 2017 - Proceedings of the 2nd International Conference on Uncertainty Quantification in Computational Sciences and Engineering},
  pages     = {406-418},
  month     = {January},
  abstract  = {\textcopyright  2017 The Authors. Published by Eccomas Proceedia. Computational modeling of the structural behavior of continuous fiber composite materials often takes into account the periodicity of the underlying micro-structure. A well established method dealing with the structural behavior of periodic micro-structures is the socalled Asymptotic Expansion Homogenization (AEH). By considering a periodic perturbation of the material displacement, scale bridging functions, also referred to as elastic correctors, can be derived in order to connect the strains at the level of the macro-structure with microstructural strains. For complicated inhomogeneous micro-structures, the derivation of such functions is usually performed by the numerical solution of a PDE problem -Typically with the Finite Element Method. Moreover, when dealing with uncertain micro-structural geometry and material parameters, there is considerable uncertainty introduced in the actual stresses experienced by the materials. Due to the high computational cost of computing the elastic correctors, the choice of a pure Monte-Carlo approach for dealing with the inevitable material and geometric uncertainties is clearly computationally intractable. This problem is even more pronounced when the effect of damage in the micro-scale is considered, where re-evaluation of the micro-structural representative volume element is necessary for every occurring damage. The novelty in this paper is that a non-intrusive surrogate modeling approach is employed with the purpose of directly bridging the macro-scale behavior of the structure with the material behavior in the micro-scale, therefore reducing the number of costly evaluations of corrector functions, allowing for future developments on the incorporation of fatigue or static damage in the analysis of composite structural components.},
  date      = {2017},
  day       = {01},
  doi       = {10.7712/120217.5379.16904},
  keywords  = {civil engineering, used UQLab},
  piis      = {85043484343},
  url       = {http://dx.doi.org/10.7712/120217.5379.16904},
}

@InProceedings{Mylonas2017a,
  author    = {Charilaos Mylonas and Imad Abdallah and Eleni Chatzi},
  title     = {{S}urrogate modelling for fatigue damage of wind-turbine blades using polynomial chaos expansions and non-negative matrix factorization},
  booktitle = {IABSE Conference, Vancouver 2017: Engineering the Future - Report},
  pages     = {809-816},
  month     = {January},
  abstract  = {\textcopyright  2018 Ingenta. A computational approach for the estimation of fatigue degradation of composite wind turbine blades by means of time domain aero-servo-elastic simulations is proposed. Wind turbine blades are subjected throughout their lifetime to highly stochastic loading. Fatigue damage of the composite reinforcement of the wind turbine blades has been identified early on in the wind turbine design practice as a factor driving design. A simple fatigue accumulation model is utilized for the spar cap reinforcement of a wind-turbine blade. Non-Negative Matrix Factorization (NMF) for the damage accumulation random field is used for dimensionality reduction. An approximate computationally efficient model, relying on Polynomial Chaos Expansion (PCE) of the damage state with respect to probabilistically modelled mean wind and turbulence intensity is derived. The framework is exemplified in a case-study of a 1.5MW wind turbine.},
  date      = {2017},
  day       = {01},
  keywords  = {civil engineering, used UQLab},
  piis      = {85050026817},
}

@Article{Ni2017,
  author   = {Fei Ni and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {{B}asis-{A}daptive {S}parse {P}olynomial {C}haos {E}xpansion for {P}robabilistic {P}ower {F}low},
  journal  = {IEEE Transactions on Power Systems},
  pages    = {694-704},
  month    = {January},
  abstract = {\textcopyright  1969-2012 IEEE. This paper introduces the basis-adaptive sparse polynomial chaos (BASPC) expansion to perform the probabilistic power flow (PPF) analysis in power systems. The proposed method takes advantage of three state-of-the-art uncertainty quantification methodologies reasonably: the hyperbolic scheme to truncate the infinite polynomial chaos (PC) series; the least angle regression (LARS) technique to select the optimal degree of each univariate PC series; and the Copula to deal with nonlinear correlations among random input variables. Consequently, the proposed method brings appealing features to PPF, including the ability to handle the large-scale uncertainty sources; to tackle the nonlinear correlation among the random inputs; to analytically calculate representative statistics of the desired outputs; and to dramatically alleviate the computational burden as of traditional methods. The accuracy and efficiency of the proposed method are verified through either quantitative indicators or graphical results of PPF on both the IEEE European Low Voltage Test Feeder and the IEEE 123 Node Test Feeder, in the presence of more than 100 correlated uncertain input variables.},
  date     = {2017},
  day      = {01},
  doi      = {10.1109/TPWRS.2016.2558622},
  keywords = {electrical engineering, used UQLab},
  piis     = {85008659770},
  url      = {http://dx.doi.org/10.1109/TPWRS.2016.2558622},
}

@Article{Ni2018,
  author   = {Fei Ni and Michiel Nijhuis and Phuong H. Nguyen and Joseph F. G. Cobben},
  title    = {{V}ariance-{B}ased {G}lobal {S}ensitivity {A}nalysis for {P}ower {S}ystems},
  journal  = {IEEE Transactions on Power Systems},
  pages    = {1670-1682},
  month    = {March},
  abstract = {\textcopyright  1969-2012 IEEE. Knowledge of the impact of uncertain inputs is valuable, especially in power systems with large amounts of stochastic renewable generations. A global sensitivity analysis (GSA) can determine the impact of input uncertainties on the output quantity of interest in a certain physical or mathematical model. The GSA has not been widely employed in power systems due to the prohibitively computational burden. In this paper, it is demonstrated that, via the implementation of a basis-Adaptive sparse polynomial chaos expansion, a GSA can be applied to the power system with numerous uncertain inputs. The performance of the proposed method is tested on both the IEEE 13-bus test feeder and the IEEE 123-node test system, in presence of a large amount of independent or correlated uncertain inputs. The possible application of a GSA on the basis of the basis-Adaptive sparse polynomial chaos expansion in power systems are discussed in terms of various sensitivities. The findings cannot only be used to rank the most influential input uncertainties with respect to a specific output, such as variances of the nodal power, but also to identify the most sensitive or robust electrical variables such as the bus voltage with respect to input uncertainties.},
  date     = {2018},
  day      = {01},
  doi      = {10.1109/TPWRS.2017.2719046},
  keywords = {electrical engineering, used UQLab},
  piis     = {85023758823},
  url      = {http://dx.doi.org/10.1109/TPWRS.2017.2719046},
}

@InProceedings{Palar2018,
  author    = {Pramudita Satria Palar and Koji Shimoyama},
  title     = {{P}olynomial-chaos-kriging-assisted efficient global optimization},
  booktitle = {2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings},
  pages     = {1-8},
  month     = {February},
  abstract  = {\textcopyright  2017 IEEE. In this paper, we explore the use of the recently proposed polynomial chaos-Kriging (PCK) surrogate model to assist a single-objective efficient global optimization (EGO) framework in order to solve expensive optimization problems. PCK is a form of universal Kriging (UK) that employs orthogonal polynomials and least-angle-regression (LARS) algorithm to select the proper set of polynomial basis. The use of LARS within the PCK algorithm eliminates the need for the manual selection of UK\textquotesingle s trend function. Investigation on the capability of PCK-EGO is performed on five synthetic and one aerodynamic test problems. In light of the results, we observe that PCK-EGO performs in a similar way to standard EGO in cases with no clear polynomial-like trend. However, PCK-EGO shows a notable faster convergence in problems where the objective function exhibits a landscape trend that can be captured by polynomials. Application to the subsonic wing problem further demonstrates that PCK-EGO is more efficient than EGO in a real-world aerodynamic optimization problem.},
  date      = {2018},
  day       = {02},
  doi       = {10.1109/SSCI.2017.8280831},
  keywords  = {computational science and engineering, optimization, used UQLab},
  piis      = {85046164050},
  url       = {http://dx.doi.org/10.1109/SSCI.2017.8280831},
}

@InCollection{Palar2018a,
  author    = {Pramudita Satria Palar and Koji Shimoyama},
  title     = {{E}nsemble of kriging with multiple kernel functions for engineering design optimization},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages     = {211-222},
  month     = {January},
  abstract  = {\textcopyright  2018, Springer International Publishing AG, part of Springer Nature. We introduce the ensemble of Kriging with multiple kernel functions guided by cross-validation error for creating a robust and accurate surrogate model to handle engineering design problems. By using the ensemble of Kriging models, the resulting ensemble model preserves the uncertainty structure of Kriging, thus, can be further exploited for Bayesian optimization. The objective of this paper is to develop a Kriging methodology that eliminates the needs for manual kernel selection which might not be optimal for a specific application. Kriging models with three kernel functions, that is, Gaussian, Mat{\'e}rn-3/2, and Mat{\'e}rn-5/2 are combined through a global and a local ensemble technique where their approximation quality are investigated on a set of aerodynamic problems. Results show that the ensemble approaches are more robust in terms of accuracy and able to perform similarly to the best performing individual kernel function or avoiding misspecification of kernel.},
  date      = {2018},
  day       = {01},
  doi       = {10.1007/978-3-319-91641-5\_18},
  keywords  = {computational science and engineering, used UQLab},
  piis      = {85047468928},
  url       = {http://dx.doi.org/10.1007/978-3-319-91641-5\_18},
}

@Article{Perko2016,
  author   = {Zolt{\'a}n Perk{\'o} and Sebastian R Van Der Voort and Steven Van De Water and Charlotte M H Hartman and Mischa Hoogeman and Danny Lathouwers},
  title    = {{F}ast and accurate sensitivity analysis of {I}MP{T} treatment plans using {P}olynomial {C}haos {E}xpansion},
  journal  = {Physics in Medicine and Biology},
  pages    = {4646-4664},
  month    = {May},
  abstract = {{\"\i}\textquestiondown \textonehalf  2016 Institute of Physics and Engineering in Medicine. The highly conformal planned dose distribution achievable in intensity modulated proton therapy (IMPT) can severely be compromised by uncertainties in patient setup and proton range. While several robust optimization approaches have been presented to address this issue, appropriate methods to accurately estimate the robustness of treatment plans are still lacking. To fill this gap we present Polynomial Chaos Expansion (PCE) techniques which are easily applicable and create a meta-model of the dose engine by approximating the dose in every voxel with multidimensional polynomials. This Polynomial Chaos (PC) model can be built in an automated fashion relatively cheaply and subsequently it can be used to perform comprehensive robustness analysis. We adapted PC to provide among others the expected dose, the dose variance, accurate probability distribution of dose-volume histogram (DVH) metrics (e.g. minimum tumor or maximum organ dose), exact bandwidths of DVHs, and to separate the effects of random and systematic errors. We present the outcome of our verification experiments based on 6 head-and-neck (HN) patients, and exemplify the usefulness of PCE by comparing a robust and a non-robust treatment plan for a selected HN case. The results suggest that PCE is highly valuable for both research and clinical applications.},
  date     = {2016},
  day      = {26},
  doi      = {10.1088/0031-9155/61/12/4646},
  keywords = {biomedical science, used UQLab},
  piis     = {84975041431},
  url      = {http://dx.doi.org/10.1088/0031-9155/61/12/4646},
}

@InProceedings{Reis2018,
  author    = {T. Reis and D. Calderon and P. Sartor and J. E. Cooper and J. Cheeseman},
  title     = {{D}evelopment of a wing weight convergence simulation including uncertainty modelling and sensitivity analysis},
  booktitle = {AIAA Non-Deterministic Approaches Conference, 2018},
  month     = {January},
  abstract  = {\textcopyright  2018, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved. Weight is a crucial part in the design of an aircraft, having a major influence on its performance. Therefore, being able to track the weight throughout the entire design process, including assessment of uncertainty and risk, is essential. This paper presents a new methodology for modelling uncertainty and performing sensitivity studies on aircraft weight estimation. A process has been developed that emulates the weight convergence corridor for a representative commercial jet aircraft. It combines a traditional wing-box sizing method for structural weight with a variety of alternative methods for nonstructural weight. The objective function for the sizing routine is to weight minimisation. The alternative methods mimic the different phases of design in the aircraft development cycle. Maturity of design translates to the status of the information available, which translates to accuracy in the weight estimation method in use. The unique feature of this process is the incorporation of uncertainty which is achieved by modelling the uncertain parameters based on different probability distribution functions (PDFs). Design features and aircraft components are interrelated and therefore an underlying dependency grid is generated. Combining the PDFs on the grid propagates the uncertainty towards an ultimate distribution of the total weight. This paper investigates the primary uses of the process developed for wing weight estimation. It investigates the sensitivities of the process, identifies its predominant uncertainty sources and quantifies their impact on the aircraft weight.},
  date      = {2018},
  day       = {01},
  doi       = {10.2514/6.2018-2165},
  keywords  = {aeronautical engineering},
  piis      = {85044382453},
  url       = {http://dx.doi.org/10.2514/6.2018-2165},
}

@Article{Sanctis2016,
  author   = {Gianluca De Sanctis and Mario Fontana},
  title    = {{R}isk-based optimisation of fire safety egress provisions based on the {L}QI acceptance criterion},
  journal  = {Reliability Engineering and System Safety},
  pages    = {339-350},
  month    = {August},
  abstract = {\textcopyright  2016 Elsevier Ltd. All rights reserved.Life safety is the primary objective of fire safety design and is mainly ensured by an appropriate design of the means of egress, which reduce the risk to life. An unlimited reduction of this risk is not desirable since it would lead to an immense investment of resources. The Life Quality Index (LQI) acceptance criterion is a societal indicator that is used to judge the efficiency of safety measures to reduce the risk to life and can be used to optimise the investments into life saving measures. The risk is assessed by a probabilistic engineering approach that considers the fire development and the individual-based evacuation process as well as the associated uncertainties. Advanced uncertainty propagation methods are applied in order to face the difficulties by using computational expensive models and by using individual-based models. Sensitivity measures are applied to reveal the contribution of the uncertainties on the estimation of the risk. The approach is applied for a risk-based optimisation of the minimal required door width for retail buildings.},
  date     = {2016},
  day      = {01},
  doi      = {10.1016/j.ress.2016.04.001},
  keywords = {civil engineering, used UQLab},
  piis     = {84964397298},
  url      = {http://dx.doi.org/10.1016/j.ress.2016.04.001},
}

@InProceedings{Sawicka2016,
  author    = {K. Sawicka and G. B.M. Heuvelink},
  title     = {\textquotesingle spup\textquotesingle - {A}n {R} package for uncertainty propagation in spatial environmental modelling},
  booktitle = {Proceedings of Spatial Accuracy 2016},
  pages     = {275-282},
  month     = {January},
  abstract  = {Computer models are crucial tools in engineering and environmental sciences for simulating the behaviour of complex systems. While many models are deterministic, the uncertainty in their predictions needs to be estimated before they are used for decision support. Advances in uncertainty analysis have been paralleled by a growing number of software tools, but none has gained recognition for universal applicability, including case studies with spatial models and spatial model inputs. We develop an R package that facilitates uncertainty propagation analysis in spatial environmental modelling. The \textquotesingle spup\textquotesingle  package includes functions for uncertainty model specification, propagation of uncertainty using Monte Carlo (MC) techniques, and uncertainty visualization functions. Uncertain variables are represented as objects which uncertainty is described by probability distributions. Spatial auto-correlation within a variable and crosscorrelation between variables is also accommodated for. The package has implemented the MC approach with efficient sampling algorithms, i.e. stratified random sampling and Latin hypercube sampling. The MC realizations may be used as an input to the environmental models called from R, or externally. Selected static and interactive visualization methods that are understandable by nonstatisticians can be used to visualize uncertainty about the measured input, model parameters and output of the uncertainty propagation.},
  date      = {2016},
  day       = {01},
  keywords  = {environmental engineering},
  piis      = {84991310059},
}

@InCollection{Schenkendorf2017,
  author    = {Ren{\'e} Schenkendorf and Xiangzhong Xie and Ulrike Krewer},
  title     = {{A}n {E}fficient {P}olynomial {C}haos {E}xpansion {S}trategy for {A}ctive {F}ault {I}dentification of {C}hemical {P}rocesses},
  booktitle = {Computer Aided Chemical Engineering},
  pages     = {1675-1680},
  month     = {October},
  abstract  = {To gain profit from complex chemical processes, it is essential to ensure its proper operation, i.e. to avoid costly unexpected downtimes of underlying processing units. This paper explores a highly efficient active fault detection and isolation (FDI) framework, which facilitates the discriminability of a set of analysed model candidates including the reference model (nominal behaviour) as well as pre-defined failure models (faulty behaviour). Practically, an auxiliary, model-discriminating input is derived by solving a dynamic optimization problem. While using a model-based approach, the active FDI implementation has to be robustified against the inherent model parameter uncertainties. To this end, a non-intrusive polynomial chaos expansion (PCE) is used to address these uncertainties. To guarantee a computationally feasible performance, the original PCE setting has been considerably improved. Here, the basic idea is to render the design variables (auxiliary inputs) into random variables as well. Thus, the derived PCE results are not only sensitive to the model parameters but also to the design variables. To lower the computational burden further, a least angle regression strategy is applied utilizing the sparsity property of the PCE approach. The overall effectiveness of this One-Short Sparse Polynomial Chaos Expansion (OS2-PCE) concept for FDI is illustrated conceptually by analysing a tubular plug flow reactor.},
  date      = {2017},
  day       = {01},
  doi       = {10.1016/B978-0-444-63965-3.50281-6},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041401594},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50281-6},
}

@InProceedings{Terraz2017,
  author    = {Th{\'e}ophile Terraz and Alejandro Ribes and Yvan Fournier and Bertrand Iooss and Bruno Raffin},
  title     = {{M}elissa: {L}arge scale in transit sensitivity analysis avoiding intermediate files},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2017},
  month     = {November},
  abstract  = {\textcopyright  2017 Association for Computing Machinery.Global sensitivity analysis is an important step for analyzing and validating numerical simulations. One classical approach consists in computing statistics on the outputs from well-chosen multiple simulation runs. Simulation results are stored to disk and statistics are computed postmortem. Even if supercomputers enable to run large studies, scientists are constrained to run low resolution simulations with a limited number of probes to keep the amount of intermediate storage manageable. In this paper we propose a file avoiding, adaptive, fault tolerant and elastic framework that enables high resolution global sensitivity analysis at large scale. Our approach combines iterative statistics and in transit processing to compute Sobol\textquotesingle  indices without any intermediate storage. Statistics are updated on-the-fly as soon as the in transit parallel server receives results from one of the running simulations. For one experiment, we computed the Sobol\textquotesingle  indices on 10M hexahedra and 100 timesteps, running 8000 parallel simulations executed in 1h27 on up to 28672 cores, avoiding 48TB of file storage.},
  date      = {2017},
  day       = {12},
  doi       = {10.1145/3126908.3126922},
  keywords  = {computational science and engineering},
  piis      = {85040167418},
  url       = {http://dx.doi.org/10.1145/3126908.3126922},
}

@Article{Toe2017,
  author   = {David Toe and Franck Bourrier and Ignacio Olmedo and Jean-Matthieu Monnet and Fr{\'e}d{\'e}ric Berger},
  title    = {{A}nalysis of the effect of trees on block propagation using a {D}EM model: implications for rockfall modelling},
  journal  = {Landslides},
  pages    = {1603-1614},
  month    = {October},
  abstract = {\textcopyright  2017, Springer-Verlag Berlin Heidelberg. The objective of this research was to use numerical models based on mechanical approaches to improve the integration of the protective role of forests against rockfall into block propagation models. A model based on the discrete element method (DEM) was developed to take into account the complex mechanical processes involved during the impact of a block on a tree. This modelling approach requires the definition of many input parameters and cannot be directly integrated into block propagation models. A global sensitivity analysis identified the leading parameters of the block kinematics after impact (i.e. block energy reduction, trajectory changes, and rotational velocity): the impact velocity, the tree diameter, and the impact point horizontal location (i.e. eccentricity). Comparisons with the previous experimental and numerical studies of block impacts on trees demonstrated the applicability of the DEM model and showed some of the limitations of earlier approaches. Our sensitivity analysis highlights the significant influence of the impact velocity on the reduction of the block’s kinetic energy. Previous approaches usually also focus on parameters such as impact height, impact vertical incidence, and tree species, whose importance is only minor according to the present results. This suggests that the integration of forest effects into block propagation models could be both improved and simplified. The DEM model can also be used as an alternative to classical approaches for the integration of forest effects by directly coupling it with block propagation models. This direct coupling only requires the additional definition of the location and the diameter of each tree. Indeed, the input parameters related to the mechanical properties of the stem and the block/stem interaction in the DEM model can be set to average values because they are not leading parameters. The other input parameters are already defined or calculated in the block propagation model.},
  date     = {2017},
  day      = {01},
  doi      = {10.1007/s10346-017-0799-6},
  keywords = {geomechanics, used UQLab},
  piis     = {85015610979},
  url      = {http://dx.doi.org/10.1007/s10346-017-0799-6},
}

@Article{Toe2018,
  author   = {David Toe and Alessio Mentani and Laura Govoni and Franck Bourrier and Guido Gottardi and St{\'e}phane Lambert},
  title    = {{I}ntroducing {M}eta-models for a {M}ore {E}fficient {H}azard {M}itigation {S}trategy with {R}ockfall {P}rotection {B}arriers},
  journal  = {Rock Mechanics and Rock Engineering},
  pages    = {1097-1109},
  month    = {April},
  abstract = {\textcopyright  2018, Springer-Verlag GmbH Austria, part of Springer Nature. The paper presents a new approach to assess the effecctiveness of rockfall protection barriers, accounting for the wide variety of impact conditions observed on natural sites. This approach makes use of meta-models, considering a widely used rockfall barrier type and was developed from on FE simulation results. Six input parameters relevant to the block impact conditions have been considered. Two meta-models were developed concerning the barrier capability either of stopping the block or in reducing its kinetic energy. The outcome of the parameters range on the meta-model accuracy has been also investigated. The results of the study reveal that the meta-models are effective in reproducing with accuracy the response of the barrier to any impact conditions, providing a formidable tool to support the design of these structures. Furthermore, allowing to accommodate the effects of the impact conditions on the prediction of the block\textendash barrier interaction, the approach can be successfully used in combination with rockfall trajectory simulation tools to improve rockfall quantitative hazard assessment and optimise rockfall mitigation strategies.},
  date     = {2018},
  day      = {01},
  doi      = {10.1007/s00603-017-1394-9},
  keywords = {geomechanics, used UQLab},
  piis     = {85040370552},
  url      = {http://dx.doi.org/10.1007/s00603-017-1394-9},
}

@Article{Torres-Matallana2018,
  author   = {Jairo Arturo Torres-Matallana and Ulrich Leopold and Gerard B.M. Heuvelink},
  title    = {{spup}: {A}n {R}-package for spatio-temporal uncertainty propagation across multiple scales with examples in urbanwater modelling},
  journal  = {Water (Switzerland)},
  month    = {June},
  abstract = {\textcopyright  2018 by the authors. Integrated environmental modelling requires coupling sub-models at different spatial and temporal scales, thus accounting for change of support procedures (aggregation and disaggregation). We introduce the R-package spatio-temporal Uncertainty Propagation across multiple scales, stUPscales, which constitutes a contribution to state-of-the-art open source tools that support uncertainty propagation analysis in temporal and spatio-temporal domains. We illustrate the tool with an uncertainty propagation example in environmental modelling, specifically in the urban water domain. The functionalities of the class setup and the methods and functions MC.setup, MC.sim, MC.analysis and Agg.t are explained, which are used for setting up, running and analysing Monte Carlo uncertainty propagation simulations, and for spatio-temporal aggregation. We also show how the package can be used to model and predict variables that vary in space and time by using a spatio-temporal variogram model and space-time ordinary kriging. stUPscales takes uncertainty characterisation and propagation a step further by including temporal and spatio-temporal auto- and cross-correlation, resulting in more realistic (spatio-)temporal series of environmental variables. Due to its modularity, the package allows the implementation of additional methods and functions for spatio-temporal disaggregation of model inputs and outputs, when linking models across multiple space-time scales.},
  date     = {2018},
  day      = {23},
  doi      = {10.3390/w10070837},
  keywords = {environmental engineering},
  piis     = {85048591822},
  url      = {http://dx.doi.org/10.3390/w10070837},
}

@Article{Turati2018,
  author   = {Pietro Turati and Antonio Cammi and Stefano Lorenzi and Nicola Pedroni and Enrico Zio},
  title    = {{A}daptive simulation for failure identification in the {A}dvanced {L}ead {F}ast {R}eactor {E}uropean {D}emonstrator},
  journal  = {Progress in Nuclear Energy},
  pages    = {176-190},
  month    = {March},
  abstract = {\textcopyright  2017 Elsevier LtdThe identification undesired or abnormal states of a nuclear power plant is of primary importance for defining accident prevention and mitigation actions. To this aim, computational models and simulators are frequently employed, as they allow to study the system response to different operational conditions. For complex systems like the nuclear power plants, this is in general challenging because the simulation tools are i) high-dimensional; ii) black-box; iii) dynamic and iv) computationally demanding. In this paper, an adaptive simulation framework recently proposed by some of the authors is tailored for the analysis of accident scenarios involving the control system of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED). The results confirm that the adaptive simulation framework proposed is effective in identifying critical regions of operation with a limited number of calls to the computationally expensive model. The time of occurrence and magnitude of the failures of the components of the control system are identified as key factors to characterize the critical regions. In particular, it is shown that the order of occurrence of the components’ failures strongly affects the evolution of the accident scenarios.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.pnucene.2017.11.013},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85037379518},
  url      = {http://dx.doi.org/10.1016/j.pnucene.2017.11.013},
}

@Article{Wang2016,
  author   = {Chen Wang and Qingyun Duan and Charles H. Tong and Zhenhua Di and Wei Gong},
  title    = {{A} G{U}I platform for uncertainty quantification of complex dynamical models},
  journal  = {Environmental Modelling and Software},
  pages    = {1-12},
  month    = {February},
  abstract = {\textcopyright  2015 The Authors.Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
  date     = {2016},
  day      = {01},
  doi      = {10.1016/j.envsoft.2015.11.004},
  piis     = {84949194235},
  url      = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
}

@Article{Wu2018,
  author   = {Xu Wu and Tomasz Kozlowski and Hadi Meidani and Koroush Shirvan},
  title    = {{I}nverse uncertainty quantification using the modular {B}ayesian approach based on {G}aussian process, {P}art 1: {T}heory},
  journal  = {Nuclear Engineering and Design},
  pages    = {339-355},
  month    = {August},
  abstract = {In nuclear reactor system design and safety analysis, the Best Estimate plus Uncertainty (BEPU) methodology requires that computer model output uncertainties must be quantified in order to prove that the investigated design stays within acceptance criteria. \textquotedblleft Expert opinion\textquotedblright  and \textquotedblleft user self-evaluation\textquotedblright  have been widely used to specify computer model input uncertainties in previous uncertainty, sensitivity and validation studies. Inverse Uncertainty Quantification (UQ) is the process to inversely quantify input uncertainties based on experimental data in order to more precisely quantify such ad-hoc specifications of the input uncertainty information. In this paper, we used Bayesian analysis to establish the inverse UQ formulation, with systematic and rigorously derived metamodels constructed by Gaussian Process (GP). Due to incomplete or inaccurate underlying physics, as well as numerical approximation errors, computer models always have discrepancy/bias in representing the realities, which can cause over-fitting if neglected in the inverse UQ process. The model discrepancy term is accounted for in our formulation through the \textquotedblleft model updating equation\textquotedblright . We provided a detailed introduction and comparison of the full and modular Bayesian approaches for inverse UQ, as well as pointed out their limitations when extrapolated to the validation/prediction domain. Finally, we proposed an improved modular Bayesian approach that can avoid extrapolating the model discrepancy that is learnt from the inverse UQ domain to the validation/prediction domain.},
  date     = {2018},
  day      = {15},
  doi      = {10.1016/j.nucengdes.2018.06.004},
  keywords = {nuclear engineering},
  piis     = {85048576082},
  url      = {http://dx.doi.org/10.1016/j.nucengdes.2018.06.004},
}

@Article{Wu2018a,
  author   = {Xu Wu and Tomasz Kozlowski and Hadi Meidani},
  title    = {{K}riging-based inverse uncertainty quantification of nuclear fuel performance code {B}IS{O}N fission gas release model using time series measurement data},
  journal  = {Reliability Engineering and System Safety},
  pages    = {422-436},
  month    = {January},
  abstract = {\textcopyright  2017 Elsevier LtdIn nuclear reactor fuel performance simulation, fission gas release (FGR) and swelling involve treatment of several complicated and interrelated physical processes, which inevitably depend on uncertain input parameters. However, the uncertainties associated with these input parameters are only known by \textquotedblleft expert judgment\textquotedblright . In this paper, inverse Uncertainty Quantification (UQ) under the Bayesian framework is applied to BISON code FGR model based on Ris{\o}-AN3 time series experimental data. Inverse UQ seeks statistical descriptions of the uncertain input parameters that are consistent with the available measurement data. It always captures the uncertainties in its estimates rather than merely determining the best-fit values. Kriging metamodel is applied to greatly reduce the computational cost during Markov Chain Monte Carlo sampling. We performed a dimension reduction for the FGR time series data using Principal Component Analysis. We also projected the original FGR time series measurement data onto the PC subspace as \textquotedblleft transformed experiment data\textquotedblright . A forward uncertainty propagation based on the posterior distributions shows that the agreement between BISON simulation and Ris{\o}-AN3 time series measurement data is greatly improved. The posterior distributions for the uncertain input factors can be used to replace the expert specifications for future uncertainty/sensitivity analysis.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.ress.2017.09.029},
  keywords = {nuclear engineering, used UQLab},
  piis     = {85030669828},
  url      = {http://dx.doi.org/10.1016/j.ress.2017.09.029},
}

@InCollection{Xie2017,
  author    = {Xiangzhong Xie and Ren{\'e} Schenkendorf and Ulrike Krewer},
  title     = {{R}obust {D}esign of {C}hemical {P}rocesses {B}ased on a {O}ne-{S}hot {S}parse {P}olynomial {C}haos {E}xpansion {C}oncept},
  booktitle = {Computer Aided Chemical Engineering},
  pages     = {613-618},
  month     = {October},
  abstract  = {The application of robust model-based design concepts for complex chemical processes is limited due to the repeated cpu-intensive uncertainty quantification step for any new tested process design configuration. Therefore, an efficient One-Shot Sparse Polynomial Chaos Expansion (OS2-PCE) based process design framework is introduced in this work. The key idea is to define the process design variables as uncertain quantities as well and, in consequence, they become an integral part of the robust optimization routine. Moreover, by utilizing the sparsity feature of the PCE approach, the implementation of a least angle regression (LAR) concept leads to a significant reduction in computational costs. The overall performance of the novel OS2-PCE approach is illustrated by a robust process design study of a jacketed tubular reactor. In comparison to state-of-the-art concepts, the proposed framework shows promising results in terms of efficiency and robustness.},
  date      = {2017},
  day       = {01},
  doi       = {10.1016/B978-0-444-63965-3.50104-5},
  keywords  = {chemical engineering, used UQLab},
  piis      = {85041412536},
  url       = {http://dx.doi.org/10.1016/B978-0-444-63965-3.50104-5},
}

@Article{Xie2018,
  author   = {Xiangzhong Xie and R{\"u}diger Ohs and Antje Spie{\ss} and Ulrike Krewer and Ren{\'e} Schenkendorf},
  title    = {{M}oment-{I}ndependent {S}ensitivity {A}nalysis of {E}nzyme-{C}atalyzed {R}eactions with {C}orrelated {M}odel {P}arameters},
  journal  = {IFAC-PapersOnLine},
  pages    = {753-758},
  month    = {January},
  abstract = {\textcopyright  2018 The dynamic models used for biological and chemical process analysis and design usually include a significant number of uncertain model parameters. Sensitivity analysis is frequently applied to provide quantitative information regarding the influence of the parameters, as well as their uncertainties, on the model output. Various techniques are available in the literature to calculate parameter sensitivities based on local derivatives or changes in dedicated statistical moments of the model output. However, these methods may lead to an inevitable loss of information for a proper sensitivity analysis and are not directly available for problems with correlated model parameters. In this work, we demonstrate the use of a moment-independent sensitivity analysis concept in the presence and absence of parameter correlations and investigate the correlation effect in more detail. Moment-independent sensitivity analysis calculates parameter sensitivities based on changes in the entire probability density distribution of the model output and is formulated independently of whether the parameters are correlated or not. Technically, a single-loop Monte Carlo simulation method in combination with polynomial chaos expansion is implemented to reduce the computational cost significantly. A sampling procedure derived from Gaussian copula formalism is used to generate sample points for arbitrarily correlated uncertain parameters. The proposed concept is demonstrated with a case study of an enzyme-catalyzed reaction network. We observe evident differences in the parameter sensitivities for cases with independent and correlated model parameters.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.ifacol.2018.04.004},
  keywords = {chemical engineering, used UQLab},
  piis     = {85046655059},
  url      = {http://dx.doi.org/10.1016/j.ifacol.2018.04.004},
}

@InProceedings{Yousefian2017,
  author    = {Sajjad Yousefian and Gilles Bourque and Rory F.D. Monaghan},
  title     = {{R}eview of hybrid emissions prediction tools and uncertainty quantification methods for gas turbine combustion systems},
  booktitle = {Proceedings of the ASME Turbo Expo},
  month     = {January},
  abstract  = {\textcopyright  2017 ASME.There is a need for fast and reliable emissions prediction tools in the design, development and performance analysis of gas turbine combustion systems to predict emissions such as NOx, CO. Hybrid emissions prediction tools are defined as modelling approaches that (1) use computational fluid dynamics (CFD) or component modelling methods to generate flow field information, and (2) integrate them with detailed chemical kinetic modelling of emissions using chemical reactor network (CRN) techniques. This paper presents a review and comparison of hybrid emissions prediction tools and uncertainty quantification (UQ) methods for gas turbine combustion systems. In the first part of this study, CRN solvers are compared on the bases of some selected attributes which facilitate flexibility of network modelling, implementation of large chemical kinetic mechanisms and automatic construction of CRN. The second part of this study deals with UQ, which is becoming an important aspect of the development and use of computational tools in gas turbine combustion chamber design and analysis. Therefore, the use of UQ technique as part of the generalized modelling approach is important to develop a UQenabled hybrid emissions prediction tool. UQ techniques are compared on the bases of the number of evaluations and corresponding computational cost to achieve desired accuracy levels and their ability to treat deterministic models for emissions prediction as black boxes that do not require modifications. Recommendations for the development of UQenabled emissions prediction tools are made.},
  date      = {2017},
  day       = {01},
  doi       = {10.1115/GT2017-64271},
  keywords  = {mechanical engineering, review article},
  piis      = {85030695160},
  url       = {http://dx.doi.org/10.1115/GT2017-64271},
}

@Article{Yuzugullu2017,
  author   = {Onur Yuzugullu and Esra Erten and Irena Hajnsek},
  title    = {{E}stimation of {R}ice {C}rop {H}eight from {X}-and {C}-Band {P}ol{S}AR by {M}etamodel-{B}ased {O}ptimization},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  pages    = {194-204},
  month    = {January},
  abstract = {\textcopyright  2008-2012 IEEE. Rice crops are important in global food economy and are monitored by precise agricultural methods, in which crop morphology in high spatial resolution becomes the point of interest. Synthetic aperture radar (SAR) technology is being used for such agricultural purposes. Using polarimetric SAR (PolSAR) data, plant morphology dependent electromagnetic scattering models can be used to approximate the backscattering behaviors of the crops. However, the inversion of such models for the morphology estimation is complex, ill-posed, and computationally expensive. Here, a metamodel-based probabilistic inversion algorithm is proposed to invert the morphology-based scattering model for the crop biophysical parameter mainly focusing on the crop height estimation. The accuracy of the proposed approach is tested with ground measured biophysical parameters on rice fields in two different bands (X and C) and several channel combinations. Results show that in C-band the combination of the HH and VV channels has the highest overall accuracy through the crop growth cycle. Finally, the proposed metamodel-based probabilistic biophysical parameter retrieval algorithm allows estimation of rice crop height using PolSAR data with high accuracy and low computation cost. This research provides a new perspective on the use of PolSAR data in modern precise agriculture studies.},
  date     = {2017},
  day      = {01},
  doi      = {10.1109/JSTARS.2016.2575362},
  keywords = {sensors engineering, used UQLab},
  piis     = {84976511966},
  url      = {http://dx.doi.org/10.1109/JSTARS.2016.2575362},
}

@Article{Zega2018,
  author   = {Valentina Zega and Attilio Frangi and Andrea Guercilena and Gabriele Gattere},
  title    = {{A}nalysis of frequency stability and thermoelastic effects for slotted tuning fork {M}EM{S} resonators},
  journal  = {Sensors (Switzerland)},
  month    = {July},
  abstract = {\textcopyright  2018 by the authors. Licensee MDPI, Basel, Switzerland. MicroElectroMechanical Systems (MEMS) resonators are attracting increasing interest because of their smaller size and better integrability as opposed to their quartz counterparts. However, thermal drift of the natural frequency of silicon structures is one of the main issues that has hindered the development of MEMS resonators. Extensive investigations have addressed both the fabrication process (e.g., introducing heavy doping of the silicon) and the mechanical design (e.g., exploiting proper orientation of the device, slots, nonlinearities). In this work, starting from experimental data published in the literature, we show that a careful design can help reduce the thermal drift even when slots are inserted in the devices in order to decrease thermoelastic losses. A custom numerical code able to predict the dynamic behavior of MEMS resonators for different materials, orientations and doping levels is coupled with an evolutionary optimization algorithm and the possibility to find an optimal mechanical design is demonstrated on a tuning-fork resonator.},
  date     = {2018},
  day      = {04},
  doi      = {10.3390/s18072157},
  keywords = {sensors engineering},
  piis     = {85049736453},
  url      = {http://dx.doi.org/10.3390/s18072157},
}

@Article{Vohra2018,
  author   = {Manav Vohra and Ali Yousefzadi Nobakht and Seungha Shin and Sankaran Mahadevan},
  title    = {Uncertainty quantification in non-equilibrium molecular dynamics simulations of thermal transport},
  journal  = {International Journal of Heat and Mass Transfer},
  pages    = {297-307},
  month    = {December},
  abstract = {Bulk thermal conductivity estimates based on predictions from non-equilibrium molecular dynamics (NEMD) using the so-called direct method are known to be severely under-predicted since finite simulation length-scales are unable to mimic bulk transport. Moreover, subjecting the system to a temperature gradient by means of thermostatting tends to impact phonon transport adversely. Additionally, NEMD predictions are tightly coupled with the choice of the inter-atomic potential and the underlying values associated with its parameters. In the case of silicon (Si), nominal estimates of the Stillinger-Weber (SW) potential parameters are largely based on a constrained regression approach aimed at agreement with experimental data while ensuring structural stability. However, this approach has its shortcomings and it may not be ideal to use the same set of parameters to study a wide variety of Si-based systems subjected to different thermodynamic conditions. In this study, NEMD simulations are performed on a Si bar to investigate the impact of bar-length, and the applied temperature gradient on the discrepancy between predictions and the available measurement for bulk thermal conductivity at 300 K by constructing statistical response surfaces at different temperatures. The approach helps quantify the discrepancy, observed to be largely dependent on the system-size, with minimal computational effort. A computationally efficient approach based on derivative-based sensitivity measures to construct a reduced-order polynomial chaos surrogate for NEMD predictions is also presented. The surrogate is used to perform parametric sensitivity analysis, forward propagation of the uncertainty, and calibration of the important SW potential parameters in a Bayesian setting. It is found that only two (out of seven) parameters contribute significantly to the uncertainty in bulk thermal conductivity estimates for Si.},
  date     = {2018},
  day      = {01},
  doi      = {10.1016/j.ijheatmasstransfer.2018.07.073},
  keywords = {mechanical engineering, used UQLab},
  piis     = {85050528394},
  url      = {http://dx.doi.org/10.1016/j.ijheatmasstransfer.2018.07.073},
}

@Comment{jabref-meta: databaseType:bibtex;}
